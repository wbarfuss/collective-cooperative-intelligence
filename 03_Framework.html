<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.544">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Supplementary Information for Collective Cooperative Intelligence - 3&nbsp; Framework</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./04_MultiStability.html" rel="next">
<link href="./02_LiteratureBackground.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./03_Framework.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Framework</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Supplementary Information for Collective Cooperative Intelligence</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/wbarfuss/collective-cooperative-intelligence" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./Supplementary-Information-for-Collective-Cooperative-Intelligence.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02_LiteratureBackground.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Literature background</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03_Framework.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Framework</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04_MultiStability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Multi-stability</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05_AbruptTransitions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Abrupt transitions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06_Hysteresis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Hysteresis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07_DynamicRegmies.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Dynamic regimes</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08_SimulationScripts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Simulation Scripts</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09_References.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">References</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#multi-agent-environment-interface-maei" id="toc-multi-agent-environment-interface-maei" class="nav-link active" data-scroll-target="#multi-agent-environment-interface-maei">Multi-agent environment interface (MAEi)</a></li>
  <li><a href="#ecological-tipping-environment" id="toc-ecological-tipping-environment" class="nav-link" data-scroll-target="#ecological-tipping-environment">Ecological Tipping Environment</a></li>
  <li><a href="#sec-RL" id="toc-sec-RL" class="nav-link" data-scroll-target="#sec-RL">Reinforcement learning</a>
  <ul class="collapse">
  <li><a href="#gain" id="toc-gain" class="nav-link" data-scroll-target="#gain">Gain</a></li>
  <li><a href="#value-functions" id="toc-value-functions" class="nav-link" data-scroll-target="#value-functions">Value functions</a></li>
  <li><a href="#strategy-function" id="toc-strategy-function" class="nav-link" data-scroll-target="#strategy-function">Strategy function</a></li>
  <li><a href="#sec-RL-learning" id="toc-sec-RL-learning" class="nav-link" data-scroll-target="#sec-RL-learning">Learning</a></li>
  </ul></li>
  <li><a href="#collective-reinforcement-learning-dynamics-crld" id="toc-collective-reinforcement-learning-dynamics-crld" class="nav-link" data-scroll-target="#collective-reinforcement-learning-dynamics-crld">Collective Reinforcement Learning Dynamics (CRLD)</a>
  <ul class="collapse">
  <li><a href="#motivation" id="toc-motivation" class="nav-link" data-scroll-target="#motivation">Motivation</a></li>
  <li><a href="#derivation" id="toc-derivation" class="nav-link" data-scroll-target="#derivation">Derivation</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-framework" class="quarto-section-identifier"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Framework</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="multi-agent-environment-interface-maei" class="level2">
<h2 class="anchored" data-anchor-id="multi-agent-environment-interface-maei">Multi-agent environment interface (MAEi)</h2>
<div id="fig-MAEi" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-MAEi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./graphics/MAEi_full.png" class="img-fluid figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-MAEi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.1: Multi-Agent Environment Interface (MAEi)
</figcaption>
</figure>
</div>
<p>The basis for the learning dynamics is the multi-agent environment interface (MAEi) (<a href="#fig-MAEi" class="quarto-xref">Figure&nbsp;<span>3.1</span></a>), which itself is based in its most basic form on the formal framework of stochastic games, also known as Markov games <span class="citation" data-cites="Littman1994">(<a href="09_References.html#ref-Littman1994" role="doc-biblioref">Littman, 1994</a>)</span>, which consist of the elements <span class="math inline">\(\langle N, \mathcal S, \boldsymbol{\mathcal A}, T, \boldsymbol{R} \rangle\)</span>.</p>
<p>In an MAEi, <span class="math inline">\(N \in \mathbb N\)</span> agents reside in an environment of <span class="math inline">\(Z \in \mathbb N\)</span> states <span class="math inline">\(\mathcal S=(S_1, \dots, S_Z)\)</span>. In each state <span class="math inline">\(s\)</span>, each agent <span class="math inline">\(i \in \{1,\dots, N\}\)</span> has a maximum of <span class="math inline">\(M \in \mathbb N\)</span> available actions <span class="math inline">\(\mathcal A^i = (A^i_1,\dots,A^i_M)\)</span> to choose from. <span class="math inline">\(\boldsymbol{\mathcal A} = \bigotimes_i \mathcal A^i\)</span> is the joint-action set where <span class="math inline">\(\bigotimes_i\)</span> denotes the cartesian product over the sets indexed by <span class="math inline">\(i\)</span>. Agents choose their actions simultaneously. A joint action is denoted by <span class="math inline">\(\boldsymbol a = (a^1, \dots, a^N) \in \boldsymbol{\mathcal A}\)</span>. With <span class="math inline">\(\boldsymbol a^{-i} = (a^1,\dots,\)</span> <span class="math inline">\(a^{i-1},\)</span> <span class="math inline">\(a^{i+1},\)</span> <span class="math inline">\(\dots,\)</span> <span class="math inline">\(a^N)\)</span> we denote the joint action except agent <span class="math inline">\(i\)</span>’s, and we write the joint action in which agent <span class="math inline">\(i\)</span> chooses <span class="math inline">\(a^i\)</span> and all other agents choose <span class="math inline">\(\boldsymbol a^{-i}\)</span> as <span class="math inline">\(a^i\boldsymbol a^{-i}\)</span>. We chose an equal number of actions for all states and agents out of notational convenience.</p>
<p>The transition function <span class="math inline">\(T: \mathcal S \times \boldsymbol{\mathcal A} \times \mathcal S \rightarrow [0, 1]\)</span> determines the probabilistic state change. <span class="math inline">\(T(s, \boldsymbol a, s')\)</span> is the transition probability from current state <span class="math inline">\(s\)</span> to next state <span class="math inline">\(s'\)</span> under joint action <span class="math inline">\(\boldsymbol a\)</span>. Throughout this work, we restrict ourselves to ergodic environments without absorbing states.</p>
<p>The reward function <span class="math inline">\(\boldsymbol R: \mathcal S \times \boldsymbol{\mathcal A} \times \mathcal S \rightarrow \mathbb{R}^N\)</span> maps the triple of current state <span class="math inline">\(s\)</span>, joint action <span class="math inline">\(\boldsymbol a\)</span> and next state <span class="math inline">\(s'\)</span> to an immediate reward scalar for each agent. <span class="math inline">\(R^i(s,\boldsymbol a,s')\)</span> is the reward agent <span class="math inline">\(i\)</span> receives. Note that the reward function is often defined as depending only on the current state and joint action, <span class="math inline">\(R^i(s, \boldsymbol a)\)</span>. Our formulation maps onto this variant by averaging out the transition probabilities towards the next state according to <span class="math inline">\(R^i(s, \boldsymbol a) = \sum_{s'} T(s, \boldsymbol a, s') R^i(s, \boldsymbol a, s')\)</span>.</p>
<p>In principle, agents could condition their probabilities of choosing action on the entire history of past play. However, doing so is not only cognitively demanding. It also requires that agents observe all other agents’ actions. Therefore, we focus our analysis on simple, so-called Markov strategies, with which agents choose their actions based only on the current state: <span class="math inline">\(X^i : \mathcal S^i \times \mathcal A^i \rightarrow [0,1]\)</span>. <span class="math inline">\(X^i(s, a^i)\)</span> is the probability that agent <span class="math inline">\(i\)</span> chooses action <span class="math inline">\(a^i\)</span> given the environment is in state <span class="math inline">\(s\)</span>. We denote the joint strategy by <span class="math inline">\(\boldsymbol X = \boldsymbol X(s, \boldsymbol a) = \bigotimes_i X^i(s, a^i) : {\mathcal S} \times \mathbf{\mathcal A} \rightarrow [0,1]^N\)</span>.</p>
</section>
<section id="ecological-tipping-environment" class="level2">
<h2 class="anchored" data-anchor-id="ecological-tipping-environment">Ecological Tipping Environment</h2>
<p>We illustrate an application of the multi-agent environment interface by specifying a concrete environment that allows studying the prospects of collective action under environmental tipping elements <span class="citation" data-cites="BarfussEtAl2020">(<a href="09_References.html#ref-BarfussEtAl2020" role="doc-biblioref">Barfuss et al., 2020</a>)</span>.</p>
<div id="fig-EcoPG" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-EcoPG-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="graphics/EnvEcoPG.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:40.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-EcoPG-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.2: Ecological Tipping Environment
</figcaption>
</figure>
</div>
<p>It is available in the Python package via:</p>
<div id="cell-10" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyCRLD.Environments.EcologicalPublicGood <span class="im">import</span> EcologicalPublicGood <span class="im">as</span> EPG</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> EPG(N<span class="op">=</span><span class="dv">2</span>, f<span class="op">=</span><span class="fl">1.2</span>, c<span class="op">=</span><span class="dv">5</span>, m<span class="op">=-</span><span class="dv">5</span>, qc<span class="op">=</span><span class="fl">0.2</span>, qr<span class="op">=</span><span class="fl">0.01</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The environmental state set consists of two states, a prosperous and a degraded one, <span class="math inline">\(\mathcal S = \{\mathsf g, \mathsf p\}\)</span>.</p>
<div id="cell-12" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>env.Sset</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="2">
<pre><code>['g', 'p']</code></pre>
</div>
</div>
<p>In each state <span class="math inline">\(s \in \mathcal S\)</span>, each agent <span class="math inline">\(i \in \{1, \dots, N\}\)</span> can choose from their action set between either cooperation or defection, <span class="math inline">\(\mathcal A^i=\{\mathsf c, \mathsf d\}\)</span>.</p>
<div id="cell-14" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>env.Aset</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>[['c', 'd'], ['c', 'd']]</code></pre>
</div>
</div>
<p>We denote the number of cooperating (defecting) agents by <span class="math inline">\(N_{\mathsf c}\)</span> (<span class="math inline">\(N_{\mathsf d} = N - N_{\mathsf c}\)</span>).</p>
<p>A collapse from the prosperous state to the degraded state occurs with transition probability, <span class="math display">\[
T(\mathsf p, \boldsymbol a, \mathsf g) = \frac{N_{\mathsf d}}{N} q_{c},
\]</span> with <span class="math inline">\(q_c \in [0, 1]\)</span> being the collapse leverage parameter, indicating how much impact a defecting agent exerts on the environment. Thus, the environment remains within the prosperous state with probability, <span class="math inline">\(T(\mathsf p, \boldsymbol a, \mathsf p) = 1 - T(\mathsf p, \boldsymbol a, \mathsf g)\)</span>.</p>
<p>In the degraded state, we set the recovery to occur with probability, <span class="math display">\[
T(\mathsf g, \boldsymbol a, \mathsf p) = q_{r},
\]</span> independent of the agents’ actions. The parameter <span class="math inline">\(q_r\)</span> sets the recovery probability, and the probability that the environment remains degraded is, thus, <span class="math inline">\(T(\mathsf g, \boldsymbol a, \mathsf g) = 1 - T(\mathsf g, \boldsymbol a, \mathsf p)\)</span>.</p>
<div id="cell-17" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>env.T.<span class="bu">round</span>(<span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>array([[[[0.99, 0.01],
         [0.99, 0.01]],

        [[0.99, 0.01],
         [0.99, 0.01]]],


       [[[0.  , 1.  ],
         [0.1 , 0.9 ]],

        [[0.1 , 0.9 ],
         [0.2 , 0.8 ]]]])</code></pre>
</div>
</div>
<p>Rewards in the prosperous state follow the standard public good game, <span class="math display">\[
R^i(\mathsf p, a^i{\boldsymbol a}^{-i}, \mathsf p) =
\left\{
\begin{array}{ll}
fc {N_c \over N} - c&amp; \text{if } a^i = \mathsf c \\
fc {N_c \over N} &amp; \text{if } a^i = \mathsf d
\end{array}
\right.
\]</span> where <span class="math inline">\(c\)</span> denotes the cost of cooperation and <span class="math inline">\(f\)</span>, the cooperation synergy factor.</p>
<div id="cell-19" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>env.R[<span class="dv">0</span>, <span class="dv">1</span>, :, :, <span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>array([[ 1., -2.],
       [ 3.,  0.]])</code></pre>
</div>
</div>
<div id="cell-20" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>env.R[<span class="dv">1</span>, <span class="dv">1</span>, :, :, <span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>array([[ 1.,  3.],
       [-2.,  0.]])</code></pre>
</div>
</div>
<p>However, when a state transition involves the degraded state, <span class="math inline">\(\mathsf g\)</span>, the agents receive an environmental collapse impact, <span class="math inline">\(m &lt; 0\)</span>, <span class="math display">\[
R^i(\mathsf p, {\boldsymbol a}, \mathsf g) = R^i(\mathsf g, {\boldsymbol a}, \mathsf p) = R^i(\mathsf g, {\boldsymbol a}, \mathsf g) = m, \quad \text{for all } \boldsymbol a, i.
\]</span></p>
<p>For illustration purposes, we set the model’s parameters as <span class="math inline">\(N=2, f=1.2, c=5, m=-5, q_c=0.2\)</span>, and <span class="math inline">\(qr=0.01\)</span>:</p>
<div id="cell-23" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> EPG(N<span class="op">=</span><span class="dv">2</span>, f<span class="op">=</span><span class="fl">1.2</span>, c<span class="op">=</span><span class="dv">5</span>, m<span class="op">=-</span><span class="dv">5</span>, qc<span class="op">=</span><span class="fl">0.2</span>, qr<span class="op">=</span><span class="fl">0.01</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="sec-RL" class="level2">
<h2 class="anchored" data-anchor-id="sec-RL">Reinforcement learning</h2>
<p>Learning helps agents adjust their behavior to changes in their environment, both from other agents and external factors. This is essential when the future is unpredictable, unknown, and complex, and thus, detailed pre-planning is doomed to failure.</p>
<p>In particular, reinforcement learning is a trial-and-error method of mapping situations to actions to maximize a numerical reward signal <span class="citation" data-cites="SuttonBarto2018">(<a href="09_References.html#ref-SuttonBarto2018" role="doc-biblioref">Sutton &amp; Barto, 2018</a>)</span>. When rewards are a delayed consequence of current actions, so-called temporal-difference or reward-prediction learning has been particularly influential <span class="citation" data-cites="Sutton1988">(<a href="09_References.html#ref-Sutton1988" role="doc-biblioref">Sutton, 1988</a>)</span>. This type of learning summarizes the difference between value estimates from past and present experiences into a reward-prediction error, which is then used to adapt the current behavior to gain more rewards over time. There also exist remarkable similarities between computational reinforcement learning and the results of neuroscientific experiments <span class="citation" data-cites="DayanNiv2008">(<a href="09_References.html#ref-DayanNiv2008" role="doc-biblioref">Dayan &amp; Niv, 2008</a>)</span>. Dopamine conveys reward-prediction errors to brain structures where learning and decision-making occur <span class="citation" data-cites="SchultzEtAl1997">(<a href="09_References.html#ref-SchultzEtAl1997" role="doc-biblioref">Schultz et al., 1997</a>)</span>. This dopamine reward-prediction error signal constitutes a potential neuronal substrate for the essential economic decision quantity of <em>utility</em> <span class="citation" data-cites="SchultzEtAl2017">(<a href="09_References.html#ref-SchultzEtAl2017" role="doc-biblioref">Schultz et al., 2017</a>)</span>.</p>
<p>In the following, we present the essential elements of the reinforcement learning update.</p>
<section id="gain" class="level3">
<h3 class="anchored" data-anchor-id="gain">Gain</h3>
<p>We assume that at each time step <span class="math inline">\(t\)</span>, each agent <span class="math inline">\(i\)</span> strives to maximize its exponentially discounted sum of future rewards,</p>
<p><span id="eq-gain"><span class="math display">\[
G^i_t = \mathsf N^i \sum_{k=0}^\infty (\gamma^i)^k r^i_{t+k},
\tag{3.1}\]</span></span></p>
<p>where <span class="math inline">\(r^i(t+k)\)</span> is the reward agent <span class="math inline">\(i\)</span> receives at time step <span class="math inline">\(t+k\)</span>, and <span class="math inline">\(\gamma^i \in [0,1)\)</span> is the discount factor of agent <span class="math inline">\(i.\)</span> The <em>discount factor</em> regulates how much an agent cares for future rewards, where <span class="math inline">\(\gamma^i\)</span> close to <span class="math inline">\(1\)</span> means that it cares for the future almost as much for the present and <span class="math inline">\(\gamma^i\)</span> close to <span class="math inline">\(0\)</span> means that it cares almost only for immediate rewards. <span class="math inline">\(\mathsf N^i\)</span> denotes a normalization constant. It is either <span class="math inline">\(1\)</span>, or <span class="math inline">\((1-\gamma^i).\)</span> While machine learning researchers often use <span class="math inline">\(\mathsf N^i=1\)</span>, the pre-factor <span class="math inline">\(N^i=(1-\gamma^i)\)</span> has the advantage of normalizing the gains, <span class="math inline">\(G^i(t)\)</span>, to be on the same numerical scale as the rewards.</p>
</section>
<section id="value-functions" class="level3">
<h3 class="anchored" data-anchor-id="value-functions">Value functions</h3>
<p>Given a joint strategy <span class="math inline">\(\boldsymbol X\)</span>, we define the <em>state values</em>, <span class="math inline">\(V^i_{\boldsymbol X}(s)\)</span>, as the expected gain, <span class="math inline">\(G^i(t)\)</span>, when starting in state <span class="math inline">\(s\)</span> and then following the joint strategy, <span class="math inline">\(\boldsymbol X\)</span>,</p>
<p><span id="eq-state-values"><span class="math display">\[
V^i_{\boldsymbol X}(s) = \mathbb E_{\boldsymbol X} \big[G^i_t | s_t = s \big].
\tag{3.2}\]</span></span></p>
<p>Analogously, we define the <em>state-action values</em>, <span class="math inline">\(Q^i_{\boldsymbol X}(s, a)\)</span>, as the expected gain, <span class="math inline">\(G^i(t)\)</span>, when starting in state <span class="math inline">\(s\)</span>, executing action <span class="math inline">\(a\)</span>, and then following the joint strategy, <span class="math inline">\(\boldsymbol X\)</span>,</p>
<p><span id="eq-state-action-values"><span class="math display">\[
Q^i_{\boldsymbol X}(s, a) = \mathbb E_{\boldsymbol X} \big[G^i_t | s_t = s, a^i_t=a \big].
\tag{3.3}\]</span></span></p>
<p>From <a href="#eq-gain" class="quarto-xref">Equation&nbsp;<span>3.1</span></a> and <a href="#eq-state-values" class="quarto-xref">Equation&nbsp;<span>3.2</span></a>, we can obtain the famous Bellman equation as follows, denoting the next state as <span class="math inline">\(s'\)</span>,</p>
<p><span class="math display">\[\begin{align}
V^i_{\boldsymbol X}(s) &amp;= \mathbb E_{\boldsymbol X} \big[G^i_t | s_t = s \big] \\
&amp;= \mathbb E_{\boldsymbol X} \Big[\mathsf N^i \sum_{k=0}^\infty (\gamma^i)^k r^i_{t+k} | s_t = s \Big] \\
&amp;= \mathbb E_{\boldsymbol X} \Big[\mathsf N^i r^i_t + \mathsf N^i \gamma^i \sum_{k=0}^\infty (\gamma^i)^{k} r^i_{t+1+k} | s_t = s \Big] \\
&amp;= \mathbb E_{\boldsymbol X} \Big[\mathsf N^i r^i_t + \gamma^i V^i_{\boldsymbol X}(s') | s_t = s\Big] \\
&amp;= \mathbb E_{\boldsymbol X} \Big[\mathsf N^i R^i(s,\boldsymbol a, s') + \gamma^i V^i_{\boldsymbol X}(s') | s_t = s\Big].
\end{align}\]</span></p>
<p>Analogously, we can write for the state-action values, <span id="eq-state-action-bellman"><span class="math display">\[
Q^i_{\boldsymbol X}(s, a) = \mathbb E_{\boldsymbol X} \Big[\mathsf N^i R^i(s,\boldsymbol a, s') + \gamma^i V^i_{\boldsymbol X}(s') | s_t = s, a^i_t=a\Big].
\tag{3.4}\]</span></span></p>
<p>Thus, the value function can be expressed via a recursive relationship. The value of a state equals the discounted value of the next state (<span class="math inline">\(\gamma^i V^i_{\boldsymbol X}(s')\)</span>) plus the reward the agent receives along the way, properly normalized (<span class="math inline">\(N^i R^i(s,\boldsymbol a, s')\)</span>). This recursion will come in useful for learning (see <a href="#sec-RL-learning" class="quarto-xref"><span>Section 3.3.4</span></a>).</p>
</section>
<section id="strategy-function" class="level3">
<h3 class="anchored" data-anchor-id="strategy-function">Strategy function</h3>
<p>In general, reinforcement learning agents do not know the true state and state-action values, <span class="math inline">\(V^i_{\boldsymbol X}(s)\)</span>, and <span class="math inline">\(Q^i_{\boldsymbol X}(s, a)\)</span>. Instead, they hold variable beliefs about the quality of each available action in each state <span class="math inline">\(Q^i_t(s, a)\)</span>. The higher an agent believes an action brings value, the more likely it will choose it. We parameterize the agents’ behavior according to the soft-max strategy function,</p>
<p><span id="eq-softmaxstrategy"><span class="math display">\[
X^i_t(s, a) = \frac{e^{\beta^i Q^i_t(s,a)}}{\sum_b e^{\beta^i Q^i_t(s,b)}},
\tag{3.5}\]</span></span></p>
<p>where the <em>intensity-of-choice</em> parameters, <span class="math inline">\(\beta^i \in \mathbb R^+\)</span>, regulate the exploration-exploitation trade-off. For high <span class="math inline">\(\beta^i\)</span>, agents exploit their learned knowledge about the environment, leaning toward actions with high estimated state-action values. For low <span class="math inline">\(\beta^i\)</span>, agents are more likely to deviate from these high-value actions to explore the environment further with the chance of finding actions that eventually lead to even higher values. This soft-max strategy function can be motivated by the maximum-entropy principle <span class="citation" data-cites="JaynesBretthorst2003">(<a href="09_References.html#ref-JaynesBretthorst2003" role="doc-biblioref">Jaynes &amp; Bretthorst, 2003</a>)</span>, stating that the current strategy of an agent should follow a distribution that maximizes entropy subject to current beliefs about the qualities <span class="math inline">\(Q_t^i(s, a)\)</span> <span class="citation" data-cites="Wolpert2006 WolpertEtAl2012">(<a href="09_References.html#ref-Wolpert2006" role="doc-biblioref">Wolpert, 2006</a>; <a href="09_References.html#ref-WolpertEtAl2012" role="doc-biblioref">Wolpert et al., 2012</a>)</span>.</p>
</section>
<section id="sec-RL-learning" class="level3">
<h3 class="anchored" data-anchor-id="sec-RL-learning">Learning</h3>
<p>Learning means updating the quality estimates, <span class="math inline">\(Q^i_t(s,a)\)</span>, with the current reward-prediciton error, <span class="math inline">\(\delta^i_t(s, a)\)</span>, after selection action <span class="math inline">\(a_t\)</span> in state <span class="math inline">\(s_t\)</span> according to</p>
<p><span id="eq-qualityupdate"><span class="math display">\[
Q^i_{t+1}(s_t, a_t) = Q^i_{t}(s_t, a_t) + \alpha^i \delta^i_t(s_t, a_t),
\tag{3.6}\]</span></span></p>
<p>where <span class="math inline">\(\alpha^i \in (0,1)\)</span> is the learning rate of agent <span class="math inline">\(i\)</span>, which regulates how much new information the agent uses for the update. The reward-prediction error, <span class="math inline">\(\delta^i_t(s_t, a_t)\)</span>, equals the difference of the new quality estimate, <span class="math inline">\(\mathsf N^i r^i_t + \gamma^i \mathcal Q_n^i(s_{t+1})\)</span>, and the current quality estimate, <span class="math inline">\(\mathcal Q_c^i(s_{t})\)</span>,</p>
<p><span id="eq-rewardpredictionerror"><span class="math display">\[
\delta^i_t(s_t, a_t) = \mathsf N^i r^i_t + \gamma^i \mathcal{Q}^i_n(s_{t+1}, a_{t+1}) - \mathcal Q^i_c(s_{t}, a_{t}),
\tag{3.7}\]</span></span></p>
<p>where the <span class="math inline">\(\mathcal{Q}_n^i\)</span> represents the quality estimate of the <em>next</em> state and <span class="math inline">\(\mathcal{Q}_c^i\)</span> represents the quality estimate of the <em>current</em> state. Depending on how we choose, <span class="math inline">\(\mathcal{Q}_n^i\)</span>, and <span class="math inline">\(\mathcal{Q}_c^i\)</span>, we recover various well-known temporal-difference reinforcement learning update schemes <span class="citation" data-cites="BarfussEtAl2019">(<a href="09_References.html#ref-BarfussEtAl2019" role="doc-biblioref">Barfuss et al., 2019</a>)</span>.</p>
<section id="variants" class="level4">
<h4 class="anchored" data-anchor-id="variants">Variants</h4>
<p>For example, if <span class="math inline">\(\mathcal{Q}_n^i = \mathcal{Q}_c^i = Q^i_t\)</span>, we obtain the so called SARSA update,</p>
<p><span class="math display">\[\delta^i_t(s_t, a_t) = \mathsf N^i r^i_t + \gamma^i Q^i_t(s_{t+1}, a_{t+1}) - Q^i_t(s_{t}, a_{t}).\]</span></p>
<p>If <span class="math inline">\(\mathcal{Q}_n^i = \max_b Q^i_t(s_{t+1}, b)\)</span>, and <span class="math inline">\(\mathcal{Q}_c^i = Q^i_t\)</span>, we obtain the famous Q-learning update,</p>
<p><span class="math display">\[\delta^i_t(s_t, a_t) = \mathsf N^i r^i_t + \gamma^i \max_b Q^i_t(s_{t+1}, b) - Q^i_t(s_{t}, a_{t}).\]</span></p>
<p>And if <span class="math inline">\(\mathcal{Q}_n^i = \mathcal{Q}_c^i = V^i_t\)</span> is a separate state-value estimate, we obtain an actor-critic update,</p>
<p><span class="math display">\[\delta^i_t(s_t, a_t) = \mathsf N^i r^i_t + \gamma^i V^i_t(s_{t+1}) - V^i_t(s_{t}).\]</span></p>
</section>
</section>
</section>
<section id="collective-reinforcement-learning-dynamics-crld" class="level2">
<h2 class="anchored" data-anchor-id="collective-reinforcement-learning-dynamics-crld">Collective Reinforcement Learning Dynamics (CRLD)</h2>
<section id="motivation" class="level3">
<h3 class="anchored" data-anchor-id="motivation">Motivation</h3>
<p>In <a href="#sec-RL" class="quarto-xref"><span>Section 3.3</span></a>, we saw how to derive temporal-difference reward-prediction reinforcement learning from first principles. Agents strive to improve their discounted sum of future rewards (<a href="#eq-gain" class="quarto-xref">Equation&nbsp;<span>3.1</span></a>) while acting according to the maximum entropy principle (<a href="#eq-softmaxstrategy" class="quarto-xref">Equation&nbsp;<span>3.5</span></a>). However, using these standard reinforcement algorithms directly for modeling comes also with some <em>challenges</em>:</p>
<ul>
<li>First of all, the learning is highly <em>stochastic</em>, since, in general, all agents strategies <span class="math inline">\(X^i(s,a)\)</span>, and the environments transition function <span class="math inline">\(T(s, \boldsymbol a, s')\)</span> are probability distributions.</li>
<li>This stochasticity can make it sometimes <em>hard to explain</em>, why a phenomenon occurred in a simulation.</li>
<li>Reinforcement learning is also very <em>sample-inefficient</em>, meaning it can take the agents a long time to learn something.</li>
<li>Thus, learning simulations are <em>computationally intense</em>, since one requires many simulations to make sense of the stochasticity, of which each takes a long time to address the sample inefficiency.</li>
</ul>
<p>How can we address these challenges? In <a href="#sec-RL-learning" class="quarto-xref"><span>Section 3.3.4</span></a>, we saw that we could express different reward-prediction learning variants by formulating different reward-prediction errors, <span class="math inline">\(\boldsymbol \delta\)</span>. The essential idea of the collective reinforcement learning dynamics approach is to replace the individual sample realizations of the reward-prediction error with its strategy average plus a small error term,</p>
<p><span class="math display">\[\boldsymbol \delta \leftarrow \bar{\boldsymbol\delta} + \boldsymbol\epsilon.\]</span></p>
<p>Thus, collective reinforcement learning dynamics describe how agents with access to (a good approximation of) the strategy-average reward-prediction error would learn. There are at least three interpretations to motivate how the agents can obtain the strategy averages:</p>
<ul>
<li>The agents are batch learners. They store experiences (state observations, rewards, actions, next state observations) inside a memory batch and replay these experiences to make the learning more stable. In the limit of an infinite memory batch, the error term vanishes, <span class="math inline">\(\boldsymbol\epsilon \rightarrow 0\)</span> <span class="citation" data-cites="Barfuss2020">(<a href="09_References.html#ref-Barfuss2020" role="doc-biblioref">Barfuss, 2020</a>)</span>.</li>
<li>The agents learn on two different time scales. On one time scale, the agents interact with the environment, collecting experiences and integrating them to improve their quality estimates while keeping their strategies fixed. On the other time scale, they use the accumulated experiences to adapt their strategy. In the limit of a complete time scale separation, having infinite experiences between two strategy updates, the error term vanishes, <span class="math inline">\(\boldsymbol\epsilon \rightarrow 0\)</span> <span class="citation" data-cites="Barfuss2022">(<a href="09_References.html#ref-Barfuss2022" role="doc-biblioref">Barfuss, 2022</a>)</span>.</li>
<li>The agents have a model of how the environment works, including how the other agents behave currently, but not how the other agents learn. This model can be used to stabilize learning. In the limit of a perfect model (and sufficient cognitive resources), the error term vanishes, <span class="math inline">\(\boldsymbol\epsilon \rightarrow 0\)</span>.</li>
</ul>
<p>In the following, we focus on the idealized case of a vanishing error term, <span class="math inline">\(\boldsymbol\epsilon \rightarrow 0\)</span>.</p>
</section>
<section id="derivation" class="level3">
<h3 class="anchored" data-anchor-id="derivation">Derivation</h3>
<p>We start by combining <a href="#eq-softmaxstrategy" class="quarto-xref">Equation&nbsp;<span>3.5</span></a> and <a href="#eq-qualityupdate" class="quarto-xref">Equation&nbsp;<span>3.6</span></a> to obtain the joint strategy update,</p>
<p><span id="eq-jointstrategyupdate"><span class="math display">\[
X^i_{t+1}(s, a) = \frac{X^i_t(s, a) \exp \left({\alpha^i\beta^i \bar\delta^i(s, a)} \right)}{\sum_b X^i_t(s, b) \exp \left({\alpha^i\beta^i \bar\delta^i(s, b)} \right)},
\tag{3.8}\]</span></span></p>
<p>where we have also replaced the sample reward-prediction error, <span class="math inline">\(\delta^i_t(s, a)\)</span>, with its strategy average, <span class="math inline">\(\bar\delta^i(s, a)\)</span>. Thus, in the remainder, we can focus on obtaining the strategy-average reward-prediction error, <span class="math inline">\(\bar\delta^i(s, a)=\delta^i_{\boldsymbol{X}_t}(s, a)\)</span>. We equip a symbol with a straight bar on top to denote the averaging with the current joint policy <span class="math inline">\(\boldsymbol{X}_t\)</span>. From <a href="#eq-rewardpredictionerror" class="quarto-xref">Equation&nbsp;<span>3.7</span></a>, we see that we need to construct the strategy-average reward, the strategy-average value of the next state, and the strategy-average value of the current state.</p>
<p><a href="#eq-jointstrategyupdate" class="quarto-xref">Equation&nbsp;<span>3.8</span></a> suggests summarizing the product of the learning rate <span class="math inline">\(\alpha^i\)</span> and the intensity-of-choice <span class="math inline">\(\beta^i\)</span> into an effective learning rate <span class="math inline">\(\eta^i\)</span>. If we restate the denominator by <span class="math inline">\(\bar{\mathfrak Z}^i(s) = \sum_b X^i_t(s, b) \exp \left({\alpha^i\beta^i \bar\delta^i(s, b)} \right)\)</span>, we recover exactly the form used in the main text,</p>
<p><span class="math display">\[
X^i_{t+1}(s, a) = \frac{1}{\bar{\mathfrak{Z}}^i(s)} X^i_t(s, a) \exp\big(\eta^i \cdot \bar \delta^i(s, a) \big).
\]</span></p>
<section id="rewards" class="level4">
<h4 class="anchored" data-anchor-id="rewards">Rewards</h4>
<p>The strategy-average version of the current reward is obtained by considering each agent <span class="math inline">\(i\)</span> taking action <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span> when all other agents <span class="math inline">\(j\)</span> act according to their strategy <span class="math inline">\(X^j(s, a^j)\)</span>, causing the environment to transition to the next state <span class="math inline">\(s'\)</span> with probability <span class="math inline">\(T(s, \boldsymbol a, s')\)</span>, during which agent <span class="math inline">\(i\)</span> receives reward <span class="math inline">\(R^i(s, \boldsymbol a, s')\)</span>. Mathematically, we write,</p>
<p><span class="math display">\[
\bar R^i(s, a) = \sum_{a^j} \sum_{s'}  \prod_{j\neq i} X^j(s, a^j) T(s, \boldsymbol a, s') R^i(s, \mathbf a, s').
\]</span></p>
</section>
<section id="next-values" class="level4">
<h4 class="anchored" data-anchor-id="next-values">Next values</h4>
<p>The strategy average of the following state value is likewise computed by averaging over all actions of the other agents and following states.</p>
<p>We start with the simplest learning variant, <em>actor-critic learning</em>. For each agent <span class="math inline">\(i\)</span>, state <span class="math inline">\(s\)</span>, and action <span class="math inline">\(a\)</span>, all other agents <span class="math inline">\(j\neq i\)</span> choose their action <span class="math inline">\(a^j\)</span> with probability <span class="math inline">\(X^j(s, a^j)\)</span>. Consequently, the environment transitions to the next state <span class="math inline">\(s'\)</span> with probability <span class="math inline">\(T(s, \boldsymbol a, s')\)</span>. At <span class="math inline">\(s'\)</span>, the agent estimates the quality of the next state to be of <span class="math inline">\(\bar V^i(s')\)</span>. Mathematically, we write,</p>
<p><span class="math display">\[
{}^{n}\!{\bar Q}^i(s, a) = \sum_{a^j} \sum_{s'} \prod_{j \neq i} X^j(s, a^j) T(s, \boldsymbol a, s') \bar V^i(s').
\]</span></p>
<p>We obtain the strategy-average value estimate of the following state precisely as the state values of the following state, <span class="math inline">\(\bar V^i(s') = V^i_{\boldsymbol X}(s')\)</span>, as defined in <a href="#eq-state-values" class="quarto-xref">Equation&nbsp;<span>3.2</span></a>. We compute them by writing the Bellman equation, <span class="math display">\[
{\bar V}^i(s) = \mathsf N^i {\bar R^i}(s) + \gamma^i {\bar T}(s, s') {\bar V}^i(s'),
\]</span> in matrix form, <span class="math display">\[
\boldsymbol{\bar V}^i = \mathsf N^i \boldsymbol{\bar R^i} + \gamma^i \boldsymbol{\bar T} \boldsymbol{\bar V}^i,
\]</span> which allows us to bring all state value variables on one site through a matrix inversion, <span class="math display">\[
\boldsymbol{\bar V}^i = N^i \left( \mathbb{1}_{Z} - \gamma^i \boldsymbol{\bar T} \right)^{-1} \boldsymbol{\bar R^i}.
\]</span></p>
<p>Here, <span class="math inline">\(\bar R^i(s)\)</span> is the strategy-average reward value agent <span class="math inline">\(i\)</span> receives in state <span class="math inline">\(s\)</span>. They are computed by averaging over all agents’ strategies, <span class="math inline">\(X^j(s, a^j)\)</span>, and the state transition <span class="math inline">\(T(s, \boldsymbol a, s')\)</span>, <span class="math display">\[
\bar R^i(s) = \sum_{a^j} \sum_{s'} \prod_{j} X^j(s, a^j) T(s, \boldsymbol a, s') R^i(s, \boldsymbol a, s').
\]</span></p>
<p>And <span class="math inline">\(\bar T(s, s')\)</span> are the strategy-average transition probabilities. They are computed by averaging over all agents’ strategies, <span class="math inline">\(X^j(s, a^j)\)</span>, <span class="math display">\[
\bar T(s, s') = \sum_{a^j} \prod_{j} X^j(s, a^j) T(s, \boldsymbol a, s').
\]</span></p>
<p>Last, <span class="math inline">\(\mathbb{1}_Z\)</span>, is the <span class="math inline">\(Z\)</span>-by-<span class="math inline">\(Z\)</span> identity matrix.</p>
<p>For <em>SARSA learning</em>, the strategy average of the following state value reads,</p>
<p><span class="math display">\[
{}^{n}\!{\bar Q}^i(s, a) = \sum_{a^j} \sum_{s'} \prod_{j \neq i} X^j(s, a^j) T(s, \boldsymbol a, s') \sum_{a^i} X^i(s', a^i) \bar Q^i(s', a^i),
\]</span></p>
<p>where we replace <span class="math inline">\(Q^i_t(s_{t+1}, a_{t+1})\)</span> by the strategy-average next-state next-action value <span class="math inline">\(\sum_{a^i} X^i(s', a^i) \bar Q^i(s', a^i)\)</span>.</p>
<p>Here, the strategy-average state-action values, <span class="math inline">\(\bar Q^i(s, a) = Q^i_{\boldsymbol X}(s, a)\)</span>, are exaclty the state-action values defined in <a href="#eq-state-action-values" class="quarto-xref">Equation&nbsp;<span>3.3</span></a>. We compute them exactly as <a href="#eq-state-action-values" class="quarto-xref">Equation&nbsp;<span>3.3</span></a> prescribes,</p>
<p><span class="math display">\[
\bar Q^i(s, a) = \mathsf N^i \bar R^i(s, a) + \gamma^i \sum_{s'} \bar T^i(s, a, s') \bar V^i(s'),
\]</span></p>
<p>where <span class="math inline">\(\bar T^i(s, a, s')\)</span> is the strategy-average transition model from the perspective of agent <span class="math inline">\(i\)</span>. It can be computed by averaging out all other agents’ strategies from the transition tensor, <span class="math display">\[
\bar T^i(s, a, s') = \sum_{a^j} \prod_{j \neq i} X^j(s, a^j) T(s, \mathbf a, s').
\]</span></p>
<p>However, it is easy to show that <span class="math inline">\(\sum_{a^i} X^i(s', a^i) \bar Q^i(s', a^i) = \bar V^i(s')\)</span>, and thus, the strategy-average next-state values of SARSA and actor-critic learning are indeed identical.</p>
</section>
<section id="current-values" class="level4">
<h4 class="anchored" data-anchor-id="current-values">Current values</h4>
<p>The strategy-average of the current state value in the reward-prediction error of <em>actor-critic learning</em>, <span class="math inline">\(\bar V^i(s)\)</span>, is - for each agent <span class="math inline">\(i\)</span> and state <span class="math inline">\(s\)</span> - a constant in actions. Thus, they do not affect the joint strategy update (<a href="#eq-jointstrategyupdate" class="quarto-xref">Equation&nbsp;<span>3.8</span></a>).</p>
<p>The state-action value of the current state, <span class="math inline">\(Q^i_t(s_t, a_t)\)</span>, in <em>SARSA learning</em> becomes, <span class="math inline">\(\frac{1}{\beta^i} \ln X^i(s, a)\)</span>, in the strategy-average reward-prediction error and can be seen as a regularization term. We can derive it by inverting <a href="#eq-softmaxstrategy" class="quarto-xref">Equation&nbsp;<span>3.5</span></a>, <span class="math display">\[
Q^i_t(s, a) = \frac{1}{\beta^i} \ln X^i_t(s, a) + \frac{1}{\beta^i} \ln\Big(\sum_b e^{\beta^i Q^i_t(s, b)} \Big),
\]</span> and realizing that the dynamics induced by <a href="#eq-jointstrategyupdate" class="quarto-xref">Equation&nbsp;<span>3.8</span></a> are invariant under additive transformations, which are constant in actions.</p>
</section>
<section id="reward-prediction-error" class="level4">
<h4 class="anchored" data-anchor-id="reward-prediction-error">Reward-prediction error</h4>
<p>Together, the strategy-average reward-prediction error for <em>actor-critic learning</em> reads, <span class="math display">\[
\bar \delta^i(s, a) = \mathsf N^i \bar R^i(s, a) + \gamma^i \cdot {}^n\! \bar Q^i(s, a) = \bar Q^i(s, a),
\]</span> and the strategy-average <em>actor-critic learning</em> dynamics, thus, <span class="math display">\[
X^i_{t+1}(s, a) = \frac{X^i_t(s, a) \exp\big( \alpha^i\beta^i \bar Q^i(s, a)\big)}{\sum_b X^i_t(s, b) \exp\big( \alpha^i\beta^i \bar Q^i(s, b)\big)}.
\]</span> With <span class="math inline">\(\alpha^i\beta^i \bar Q^i(s, a)\)</span> being the <em>fitness</em> of agent <span class="math inline">\(i\)</span>’s action <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span>, these dynamics are exactly equivalent to the alternative replicator dynamics in discrete time <span class="citation" data-cites="HofbauerSigmund2003">(<a href="09_References.html#ref-HofbauerSigmund2003" role="doc-biblioref">Hofbauer &amp; Sigmund, 2003</a>)</span>.</p>
<p>For <em>SARSA learning</em>, the strategy-average reward-prediction error reads, <span class="math display">\[
\bar \delta^i(s, a) = \mathsf N^i \bar R^i(s, a) + \gamma^i \cdot {}^n\! \bar Q^i(s, a) - \frac{1}{\beta^i} \ln X^i(s, a) = \bar Q^i(s, a) - \frac{1}{\beta^i} \ln X^i(s, a) ,
\]</span> and the strategy-average <em>SARSA learning</em> dynamics, thus, <span class="math display">\[
X^i_{t+1}(s, a) = \frac{X^i_t(s, a) \exp\big( \alpha^i \big( \beta^i \bar Q^i(s, a) - \ln X^i(s, a) \big)\big)}{\sum_b X^i_t(s, b) \exp\big( \alpha^i \big(\beta^i \bar Q^i(s, b) - \ln X^i(s, b) \big)\big)}.
\]</span></p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list" style="display: none">
<div id="ref-Barfuss2020" class="csl-entry" role="listitem">
Barfuss, W. (2020). Reinforcement <span>Learning Dynamics</span> in the <span>Infinite Memory Limit</span>. <em>Proceedings of the 19th <span>International Conference</span> on <span>Autonomous Agents</span> and <span>MultiAgent Systems</span></em>, 1768–1770.
</div>
<div id="ref-Barfuss2022" class="csl-entry" role="listitem">
Barfuss, W. (2022). Dynamical systems as a level of cognitive analysis of multi-agent learning. <em>Neural Computing and Applications</em>, <em>34</em>(3), 1653–1671. <a href="https://doi.org/10.1007/s00521-021-06117-0">https://doi.org/10.1007/s00521-021-06117-0</a>
</div>
<div id="ref-BarfussEtAl2019" class="csl-entry" role="listitem">
Barfuss, W., Donges, J. F., &amp; Kurths, J. (2019). Deterministic limit of temporal difference reinforcement learning for stochastic games. <em>Physical Review E</em>, <em>99</em>(4), 043305. <a href="https://doi.org/10.1103/PhysRevE.99.043305">https://doi.org/10.1103/PhysRevE.99.043305</a>
</div>
<div id="ref-BarfussEtAl2020" class="csl-entry" role="listitem">
Barfuss, W., Donges, J. F., Vasconcelos, V. V., Kurths, J., &amp; Levin, S. A. (2020). Caring for the future can turn tragedy into comedy for long-term collective action under risk of collapse. <em>Proceedings of the National Academy of Sciences</em>, <em>117</em>(23), 12915–12922. <a href="https://doi.org/10.1073/pnas.1916545117">https://doi.org/10.1073/pnas.1916545117</a>
</div>
<div id="ref-DayanNiv2008" class="csl-entry" role="listitem">
Dayan, P., &amp; Niv, Y. (2008). Reinforcement learning: <span>The Good</span>, <span>The Bad</span> and <span>The Ugly</span>. <em>Current Opinion in Neurobiology</em>, <em>18</em>(2), 185–196. <a href="https://doi.org/10.1016/j.conb.2008.08.003">https://doi.org/10.1016/j.conb.2008.08.003</a>
</div>
<div id="ref-HofbauerSigmund2003" class="csl-entry" role="listitem">
Hofbauer, J., &amp; Sigmund, K. (2003). Evolutionary game dynamics. <em>Bulletin of the American Mathematical Society</em>, <em>40</em>(4), 479–519. <a href="https://doi.org/10.1090/S0273-0979-03-00988-1">https://doi.org/10.1090/S0273-0979-03-00988-1</a>
</div>
<div id="ref-JaynesBretthorst2003" class="csl-entry" role="listitem">
Jaynes, E. T., &amp; Bretthorst, G. L. (2003). <em>Probability theory: The logic of science</em>. <span>Cambridge University Press</span>. <a href="http://www5.unitn.it/Biblioteca/it/Web/LibriElettroniciDettaglio/50847">http://www5.unitn.it/Biblioteca/it/Web/LibriElettroniciDettaglio/50847</a>
</div>
<div id="ref-Littman1994" class="csl-entry" role="listitem">
Littman, M. L. (1994). Markov games as a framework for multi-agent reinforcement learning. In W. W. Cohen &amp; H. Hirsh (Eds.), <em>Machine <span>Learning Proceedings</span> 1994</em> (pp. 157–163). <span>Morgan Kaufmann</span>. <a href="https://doi.org/10.1016/B978-1-55860-335-6.50027-1">https://doi.org/10.1016/B978-1-55860-335-6.50027-1</a>
</div>
<div id="ref-SchultzEtAl1997" class="csl-entry" role="listitem">
Schultz, W., Dayan, P., &amp; Montague, P. R. (1997). A <span>Neural Substrate</span> of <span>Prediction</span> and <span>Reward</span>. <em>Science</em>, <em>275</em>(5306), 1593–1599. <a href="https://doi.org/10.1126/science.275.5306.1593">https://doi.org/10.1126/science.275.5306.1593</a>
</div>
<div id="ref-SchultzEtAl2017" class="csl-entry" role="listitem">
Schultz, W., Stauffer, W. R., &amp; Lak, A. (2017). The phasic dopamine signal maturing: From reward via behavioural activation to formal economic utility. <em>Current Opinion in Neurobiology</em>, <em>43</em>, 139–148. <a href="https://doi.org/10.1016/j.conb.2017.03.013">https://doi.org/10.1016/j.conb.2017.03.013</a>
</div>
<div id="ref-Sutton1988" class="csl-entry" role="listitem">
Sutton, R. S. (1988). Learning to predict by the methods of temporal differences. <em>Machine Learning</em>, <em>3</em>(1), 9–44. <a href="https://doi.org/10.1007/BF00115009">https://doi.org/10.1007/BF00115009</a>
</div>
<div id="ref-SuttonBarto2018" class="csl-entry" role="listitem">
Sutton, R. S., &amp; Barto, A. G. (2018). <em>Reinforcement learning: An introduction</em> (Second edition). <span>The MIT Press</span>.
</div>
<div id="ref-Wolpert2006" class="csl-entry" role="listitem">
Wolpert, D. H. (2006). Information <span>Theory</span> - <span>The Bridge Connecting Bounded Rational Game Theory</span> and <span>Statistical Physics</span>. In D. Braha, A. A. Minai, &amp; Y. Bar-Yam (Eds.), <em>Complex <span>Engineered Systems</span>: <span>Science Meets Technology</span></em> (pp. 262–290). <span>Springer</span>. <a href="https://doi.org/10.1007/3-540-32834-3_12">https://doi.org/10.1007/3-540-32834-3_12</a>
</div>
<div id="ref-WolpertEtAl2012" class="csl-entry" role="listitem">
Wolpert, D. H., Harré, M., Olbrich, E., Bertschinger, N., &amp; Jost, J. (2012). Hysteresis effects of changing the parameters of noncooperative games. <em>Physical Review E</em>, <em>85</em>(3), 036102. <a href="https://doi.org/10.1103/PhysRevE.85.036102">https://doi.org/10.1103/PhysRevE.85.036102</a>
</div>
</div>
</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./02_LiteratureBackground.html" class="pagination-link  aria-label=" &lt;span="" background&lt;="" span&gt;"="">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Literature background</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./04_MultiStability.html" class="pagination-link" aria-label="<span class='chapter-number'>4</span>&nbsp; <span class='chapter-title'>Multi-stability</span>">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Multi-stability</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>