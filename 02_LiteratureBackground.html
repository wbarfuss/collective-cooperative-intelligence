<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.544">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Supplementary Information for Collective Cooperative Intelligence - 2&nbsp; Literature background</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./03_Framework.html" rel="next">
<link href="./index.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./02_LiteratureBackground.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Literature background</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Supplementary Information for Collective Cooperative Intelligence</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/wbarfuss/collective-cooperative-intelligence" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./Supplementary-Information-for-Collective-Cooperative-Intelligence.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02_LiteratureBackground.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Literature background</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03_Framework.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Framework</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04_MultiStability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Multi-stability</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05_AbruptTransitions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Abrupt transitions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06_Hysteresis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Hysteresis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07_DynamicRegmies.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Dynamic regimes</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08_SimulationScripts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Simulation Scripts</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09_References.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">References</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#complex-systems-science-css" id="toc-complex-systems-science-css" class="nav-link active" data-scroll-target="#complex-systems-science-css">Complex Systems Science (CSS)</a></li>
  <li><a href="#multi-agent-reinforcement-learning-marl" id="toc-multi-agent-reinforcement-learning-marl" class="nav-link" data-scroll-target="#multi-agent-reinforcement-learning-marl">Multi-Agent Reinforcement Learning (MARL)</a></li>
  <li><a href="#examplary-works-on-the-learning-dynamics-of-cooperation" id="toc-examplary-works-on-the-learning-dynamics-of-cooperation" class="nav-link" data-scroll-target="#examplary-works-on-the-learning-dynamics-of-cooperation">Examplary works on the learning dynamics of cooperation</a></li>
  <li><a href="#sec-note" id="toc-sec-note" class="nav-link" data-scroll-target="#sec-note">On cooperation and social dilemmas</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-background" class="quarto-section-identifier"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Literature background</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="complex-systems-science-css" class="level2">
<h2 class="anchored" data-anchor-id="complex-systems-science-css">Complex Systems Science (CSS)</h2>
<p>Complex systems are generally out of equilibrium with many interacting components, feedback, and couplings between components and levels <span class="citation" data-cites="levin_complex_2002">(e.g., <a href="09_References.html#ref-levin_complex_2002" role="doc-biblioref">Levin, 2002</a>)</span>. Emergent, collective behavior at the macro level is surprising and hard to predict. An extensive repertoire of strategies and interaction types at the component level makes for multiple emergent patterns and functions at the macroscale–the state of the art to date largely treats each pattern independently. Consequently, there is little understanding of the overall degree of collectivity <span class="citation" data-cites="daniels_introduction_2021">(<a href="09_References.html#ref-daniels_introduction_2021" role="doc-biblioref">Daniels et al., 2021</a>)</span>. Power law and heavy-tailed distributions can lead to consequential ‘black swan’ and second and third-order effects <span class="citation" data-cites="de_marzo_quantifying_2022">(<a href="09_References.html#ref-de_marzo_quantifying_2022" role="doc-biblioref">De Marzo et al., 2022</a>)</span>. Some complex systems sit near a critical point at which small perturbations can cause a phase transition or reconfiguration because of long-range correlations <span class="citation" data-cites="mora_are_2011">(<a href="09_References.html#ref-mora_are_2011" role="doc-biblioref">Mora &amp; Bialek, 2011</a>)</span>, including in finite, relatively small systems like many animal societies and human groups <span class="citation" data-cites="daniels_control_2017">(<a href="09_References.html#ref-daniels_control_2017" role="doc-biblioref">Daniels et al., 2017</a>)</span>.</p>
<p>CSS methods are diverse. Those most relevant to understanding emergence of cooperative collectives include nonlinear dynamics to study temporal oscillations and couplings in time such as how individuals synchronize their activities <span class="citation" data-cites="sarfati_self-organization_2021">(<a href="09_References.html#ref-sarfati_self-organization_2021" role="doc-biblioref">Sarfati et al., 2021</a>)</span>, stochastic differential equations to study how, for example, noise influences transitions between disordered and ordered states <span class="citation" data-cites="jhawar_noise-induced_2020">(<a href="09_References.html#ref-jhawar_noise-induced_2020" role="doc-biblioref">Jhawar et al., 2020</a>)</span>, approaches from statistical mechanics to study collective behavior in space, such as how swarms <span class="citation" data-cites="buhl_disorder_2006">(<a href="09_References.html#ref-buhl_disorder_2006" role="doc-biblioref">Buhl et al., 2006</a>)</span> and flocks choose trajectories <span class="citation" data-cites="bialek_statistical_2012">(<a href="09_References.html#ref-bialek_statistical_2012" role="doc-biblioref">Bialek et al., 2012</a>)</span>, game theory <span class="citation" data-cites="HofbauerSigmund1998 Nowak2006a">(<a href="09_References.html#ref-HofbauerSigmund1998" role="doc-biblioref">Hofbauer &amp; Sigmund, 1998</a>; <a href="09_References.html#ref-Nowak2006a" role="doc-biblioref">Nowak, 2006</a>)</span>, network theory <span class="citation" data-cites="newman_structure_2003">(<a href="09_References.html#ref-newman_structure_2003" role="doc-biblioref">Newman, 2003</a>)</span> to quantify interaction structure and how for example individuals make decisions under the influence of others in uncertain environments <span class="citation" data-cites="kleshnina_effect_2023">(<a href="09_References.html#ref-kleshnina_effect_2023" role="doc-biblioref">Kleshnina et al., 2023</a>)</span>, cellular automata <span class="citation" data-cites="wolframcellular1994">(<a href="09_References.html#ref-wolframcellular1994" role="doc-biblioref">Wolfram, 1994</a>)</span> to gain insight from toy models about the relationship between rule complexity and pattern formation, agent based modeling <span class="citation" data-cites="epstein_growing_1996">(<a href="09_References.html#ref-epstein_growing_1996" role="doc-biblioref">Epstein &amp; Axtell, 1996</a>)</span>, the physics of information to identify the mechanisms supporting information processing and quantify their efficiency with the goal of understanding how energy and information processing interact to shape collective effects <span class="citation" data-cites="kempes_thermodynamic_2017">(<a href="09_References.html#ref-kempes_thermodynamic_2017" role="doc-biblioref">Kempes et al., 2017</a>)</span>, and information theory to identify and quantify the contribution of higher order interactions to macroscale effects <span class="citation" data-cites="tekin_measuring_2017 tekin_general_2018 rosas_quantifying_2019">(<a href="09_References.html#ref-rosas_quantifying_2019" role="doc-biblioref">Rosas et al., 2019</a>; <a href="09_References.html#ref-tekin_measuring_2017" role="doc-biblioref">Tekin et al., 2017</a>, <a href="09_References.html#ref-tekin_general_2018" role="doc-biblioref">2018</a>)</span>, quantify overall degree of collectivity <span class="citation" data-cites="daniels_quantifying_2016">(<a href="09_References.html#ref-daniels_quantifying_2016" role="doc-biblioref">Daniels et al., 2016</a>)</span>, and to build unifying frameworks leveraging ideas from predictive coding <span class="citation" data-cites="rao_predictive_1999 darriba_predictions_2018 friston_does_2018">(<a href="09_References.html#ref-darriba_predictions_2018" role="doc-biblioref">Darriba &amp; Waszak, 2018</a>; <a href="09_References.html#ref-friston_does_2018" role="doc-biblioref">Friston, 2018</a>; <a href="09_References.html#ref-rao_predictive_1999" role="doc-biblioref">Rao &amp; Ballard, 1999</a>)</span>, active inference and the free-energy principle <span class="citation" data-cites="buckley_free_2017">(<a href="09_References.html#ref-buckley_free_2017" role="doc-biblioref">Buckley et al., 2017</a>)</span>, and the information theory of individuality <span class="citation" data-cites="krakauer_information_2020">(<a href="09_References.html#ref-krakauer_information_2020" role="doc-biblioref">D. Krakauer et al., 2020</a>)</span> for formalizing the role of uncertainty reduction (also called surprise minimization <span class="citation" data-cites="heins_collective_2023">(<a href="09_References.html#ref-heins_collective_2023" role="doc-biblioref">Heins et al., 2023</a>)</span>) in micro-macro relationships and entity formation and evolution.</p>
<p>We expect information theoretic approaches emphasizing uncertainty reduction will be particularly productive for informing the development of a ‘strategic statistical mechanics’ that brings together powerful probabilistic approaches from statistical physics and information theory for deriving micro-macro maps with logical principles from theoretical computer science and the study of inference to capture robust and optimal design of how strategies interact in social circuits to support cooperation at scale. For example, a recent paper on surprise minimization <span class="citation" data-cites="heins_collective_2023">(<a href="09_References.html#ref-heins_collective_2023" role="doc-biblioref">Heins et al., 2023</a>)</span> makes substantial progress in this direction. The authors develop a modeling framework to capture spatial collective behavior with inference-capable agents. The agents can estimate hidden causes of their sensations and adapt their position in space to minimize surprise (prediction error). The authors then study the relationship between individual inference and the emergence of collective states like cohesion and milling. Next steps include 1) exploring the pros and cons of active inference vs.&nbsp;MARL for encoding cognition into agents, 2) combining this approach with inductive game theory (probabilistic strategies are empirically grounded and extracted from time series) <span class="citation" data-cites="dedeo2010inductive krakauer_intelligent_2010">(<a href="09_References.html#ref-dedeo2010inductive" role="doc-biblioref">DeDeo et al., 2010</a>; <a href="09_References.html#ref-krakauer_intelligent_2010" role="doc-biblioref">D. C. Krakauer et al., 2010</a>)</span>, and 3) studying how collective states and their transitions are computed from the social circuits <span class="citation" data-cites="dedeo2010inductive brush_conflicts_2018 ramos-fernandez_collective_2020">(<a href="09_References.html#ref-brush_conflicts_2018" role="doc-biblioref">Brush et al., 2018</a>; <a href="09_References.html#ref-dedeo2010inductive" role="doc-biblioref">DeDeo et al., 2010</a>; <a href="09_References.html#ref-ramos-fernandez_collective_2020" role="doc-biblioref">Ramos-Fernandez et al., 2020</a>)</span> that form as strategies are updated and consolidate into slow variables, which reduce social uncertainty and permit accelerated rates of adaptation <span class="citation" data-cites="flack_coarse_graining_2017">(<a href="09_References.html#ref-flack_coarse_graining_2017" role="doc-biblioref">Flack, 2017</a>)</span>. With these extensions, it should be possible to begin to deduce the organizational and algorithmic principles underlying the emergence of micro-macro maps in collective information processing systems through feed-forward effects and downward causation. These principles will likely inform the conditions under which collective, cooperative intelligence emerges at scale.</p>
<p>It is worth noting that in the models developed in the surprise minimization, inductive game theory, and some of the collective computation work described above, individuals are tracked, making these models, in a sense, agent-based models. However, these approaches distinguish themselves by incorporating agents that model the world in a restricted but cognitively principled manner or by parameterizing the models using probabilistic strategies obtained directly from data and by leveraging the rigor of powerful probabilistic modeling frameworks in statistical physics or dynamical systems traditions. In the more conventional agent-based modeling community, there have also been attempts to develop a more rigorous axiomatic approach, e.g., based on symmetries and bifurcation theory <span class="citation" data-cites="ParkEtAl2021 FranciEtAl2022">(<a href="09_References.html#ref-FranciEtAl2022" role="doc-biblioref">Franci et al., 2022</a>; <a href="09_References.html#ref-ParkEtAl2021" role="doc-biblioref">Park et al., 2021</a>)</span> and game theory and control <span class="citation" data-cites="MardenShamma2018">(<a href="09_References.html#ref-MardenShamma2018" role="doc-biblioref">Marden &amp; Shamma, 2018</a>)</span>.</p>
<p>Work within the game theory and cultural evolution CSS sub-communities has made strides in understanding the social and cultural dynamics resulting from interacting boundedly rational agents with a finite computational budget. This work focuses on social and cultural learning mechanisms that allow agents to improve their behavior over time <span class="citation" data-cites="HollandMiller1991 Arthur1994 Arthur2014">(<a href="09_References.html#ref-Arthur1994" role="doc-biblioref">Arthur, 1994</a>, <a href="09_References.html#ref-Arthur2014" role="doc-biblioref">2014</a>; <a href="09_References.html#ref-HollandMiller1991" role="doc-biblioref">Holland &amp; Miller, 1991</a>)</span>. Game theoretic models in this tradition aim to explain emerging cooperation from simple yet plausible mechanisms. For example, the famous strategy tit-for-tat, which merely reciprocates what the opponent did in the previous turn, is surprisingly successful against much more complicated strategies <span class="citation" data-cites="AxelrodHamilton1981">(<a href="09_References.html#ref-AxelrodHamilton1981" role="doc-biblioref">Axelrod &amp; Hamilton, 1981</a>)</span>. Its success can be attributed to its ability to control payoffs, ensuring that it receives the same score as the opponent, regardless of the complexity of the opponent’s strategy. The “zero-determinant” strategies later discovered by <span class="citation" data-cites="press:PNAS:2012">Press &amp; Dyson (<a href="09_References.html#ref-press:PNAS:2012" role="doc-biblioref">2012</a>)</span> provide a vast generalization of this phenomenon, allowing for extortion and generosity, in addition to more equitable relationships like that of tit-for-tat. These strategies have also encouraged a more geometric view of behavior in CSS <span class="citation" data-cites="hilbe:NHB:2018">(<a href="09_References.html#ref-hilbe:NHB:2018" role="doc-biblioref">Hilbe, Chatterjee, et al., 2018</a>)</span>, moving away from purely mechanistic descriptions.</p>
<p>An unsatisfactory facet of many of these game theoretic and cultural and social evolution models is that “cooperation” is based on an atomic action with the property that more cooperation translates to better social welfare. This interchangeability is likely part of the reason for the widespread focus on mechanisms for increasing the level of cooperation within a system. However, even for the most basic model of a conflict of interest, the repeated Prisoner’s Dilemma, it can be the case that high levels of “cooperation” are suboptimal for individuals and the collective, e.g., when agents are better off alternating “cooperation” and “defection” over time (relative to always cooperating) <span class="citation" data-cites="McAvoyEtAl2022">(<a href="09_References.html#ref-McAvoyEtAl2022" role="doc-biblioref">McAvoy et al., 2022</a>)</span>. Along these lines, nonlinearities produce counter-intuitive or hard-to-predict dynamics, meaning that it is essential to consider not only the level of cooperation but also the specific collective states or social outcomes that emerge from alternative strategic configurations and game structures. With their simplifying assumptions, these approaches are suitable for gaining insight into null expectations for baseline conditions but are more limited in utility when tackling cooperation at scale in complex environments composed of cognitively complex, error-prone agents <span class="citation" data-cites="McNamara2013">(<a href="09_References.html#ref-McNamara2013" role="doc-biblioref">McNamara, 2013</a>)</span>.</p>
<p>Humans routinely deviate from the behavior predicted by the economic model of <em>Homo economicus</em> <span class="citation" data-cites="Camerer2011">(<a href="09_References.html#ref-Camerer2011" role="doc-biblioref">Camerer, 2011</a>)</span>. Yet, they are also more sophisticated than assumed in many simple evolutionary game theory models. They are capable of foresight, have a theory of mind, make inferences about their environment, and can adapt their behavior correspondingly. For example, in the most common evolutionary game theory models, individuals from a large population are randomly matched with other population members to play a static game. Those individuals who are more successful (because they employ better strategies) are more likely to be imitated. Imitation-based models are most appropriate when interactions are symmetric in the sense that individuals coincide in their feasible actions and payoffs. However, the paradigm is more challenging to motivate among heterogeneous and diverse actors and when behaviors cannot be observed directly and must be inferred before they are imitated. Moreover, extending this paradigm to account for other forms of cognition that intelligent individuals typically employ when revising their strategies is not straightforward. Finally, the act of imitation itself may be learned and, therefore, subject to cognitive constraints and learning dynamics <span class="citation" data-cites="culturalgeneralintelligenceteam2022learning">(<a href="09_References.html#ref-culturalgeneralintelligenceteam2022learning" role="doc-biblioref">Team et al., 2022</a>)</span>.</p>
<p>There is a clear interest in adapting game theoretic and cultural evolution models to accommodate these nuances <span class="citation" data-cites="HilbeEtAl2018 HauserEtAl2019 WangFu2020 McNamaraEtAl2021">(<a href="09_References.html#ref-HauserEtAl2019" role="doc-biblioref">Hauser et al., 2019</a>; <a href="09_References.html#ref-HilbeEtAl2018" role="doc-biblioref">Hilbe, Šimsa, et al., 2018</a>; <a href="09_References.html#ref-McNamaraEtAl2021" role="doc-biblioref">McNamara et al., 2021</a>; <a href="09_References.html#ref-WangFu2020" role="doc-biblioref">X. Wang &amp; Fu, 2020</a>)</span>. As with the uncertainty reduction and collective computation approaches discussed above, considering how MARL could inform such models has great potential to unleash novel ways of modeling complex systems to tackle the challenges of collective cooperation in more complex settings.</p>
</section>
<section id="multi-agent-reinforcement-learning-marl" class="level2">
<h2 class="anchored" data-anchor-id="multi-agent-reinforcement-learning-marl">Multi-Agent Reinforcement Learning (MARL)</h2>
<p>In a typical MARL setting, each agent observes (part of) the current state of the environment, then takes an action, after which they observe (part of) the new state of the environment and are provided with a reward indicating how desirable the previous “state-action-state” transition was. Over time, the agents update their strategies (a mapping from observation histories to probability distributions over their action space) to increase the long-term amount of reward they receive <span class="citation" data-cites="BusoniuEtAl2008">(<a href="09_References.html#ref-BusoniuEtAl2008" role="doc-biblioref">Busoniu et al., 2008</a>)</span>. In this work, we employ a broad definition of reinforcement learning (RL), including various individual-based update mechanisms. However, we exclude strategy update processes based on social reward comparisons, such as typical evolution models and explicit social learning. Eventually, we are interested in how processes, such as social learning, opinion formation, and collective action, can emerge from individual learning agents.</p>
<p>Modern MARL is inspired by work in several fields, including neuroscience, psychology, economics, and machine learning <span class="citation" data-cites="DayanNiv2008 BushMosteller1951 FudenbergLevine1998 Cross1973 RothErev1995 ErevRoth1998 SuttonBarto2018">(<a href="09_References.html#ref-BushMosteller1951" role="doc-biblioref">Bush &amp; Mosteller, 1951</a>; <a href="09_References.html#ref-Cross1973" role="doc-biblioref">Cross, 1973</a>; <a href="09_References.html#ref-DayanNiv2008" role="doc-biblioref">Dayan &amp; Niv, 2008</a>; <a href="09_References.html#ref-ErevRoth1998" role="doc-biblioref">Erev &amp; Roth, 1998</a>; <a href="09_References.html#ref-FudenbergLevine1998" role="doc-biblioref">Fudenberg &amp; Levine, 1998</a>; <a href="09_References.html#ref-RothErev1995" role="doc-biblioref">Roth &amp; Erev, 1995</a>; <a href="09_References.html#ref-SuttonBarto2018" role="doc-biblioref">Sutton &amp; Barto, 2018</a>)</span>. For example, the commonly used idea of temporal-difference learning is based upon reward-prediction errors, common to humans, other animals, and machines <span class="citation" data-cites="SchultzEtAl1997 BotvinickEtAl2020 Gunawardena2022">(<a href="09_References.html#ref-BotvinickEtAl2020" role="doc-biblioref">Botvinick et al., 2020</a>; <a href="09_References.html#ref-Gunawardena2022" role="doc-biblioref">Gunawardena, 2022</a>; <a href="09_References.html#ref-SchultzEtAl1997" role="doc-biblioref">Schultz et al., 1997</a>)</span>. In recent years, these traditional ideas have been combined with advances in machine learning – in particular, deep learning – to produce spectacular successes in various domains <span class="citation" data-cites="SilverEtAl2016 VinyalsEtAl2019 BernerEtAl2019">(<a href="09_References.html#ref-BernerEtAl2019" role="doc-biblioref">Berner et al., 2019</a>; <a href="09_References.html#ref-SilverEtAl2016" role="doc-biblioref">Silver et al., 2016</a>; <a href="09_References.html#ref-VinyalsEtAl2019" role="doc-biblioref">Vinyals et al., 2019</a>)</span>.</p>
<p>Studies of cooperation in MARL fall under the umbrella of <em>Cooperative AI</em> <span class="citation" data-cites="DafoeEtAl2021">(<a href="09_References.html#ref-DafoeEtAl2021" role="doc-biblioref">Dafoe et al., 2021</a>)</span>. They can be divided based on whether the underlying game is fully cooperative (i.e., where all agents share the same goal) or mixed-sum (as opposed to zero-sum, which describes fully competitive situations). MARL as a field does not have a unique goal <span class="citation" data-cites="ShohamEtAl2007">(<a href="09_References.html#ref-ShohamEtAl2007" role="doc-biblioref">Shoham et al., 2007</a>)</span>. For example, some works aim to obtain game-theoretic equilibria via MARL, while others ask which learning rules are in equilibrium with one another in a specific environment. Despite this variety, the overarching aim of Cooperative AI is to improve the cooperative capabilities of AI systems, increasing joint welfare by prescribing how agents should (learn to) act. Such learning algorithms should ideally generalize to novel situations and scale to high-dimensional environments. A vital advantage of the MARL paradigm is that it can easily accommodate heterogeneous actors. Extending machine learning interpretability techniques to MARL is an ongoing effort to advance the understanding of MARL systems <span class="citation" data-cites="grupen2022conceptbased mcgrath2022acquisition LoveringEtAl2022">(<a href="09_References.html#ref-grupen2022conceptbased" role="doc-biblioref">Grupen et al., 2022</a>; <a href="09_References.html#ref-LoveringEtAl2022" role="doc-biblioref">Lovering et al., 2022</a>; <a href="09_References.html#ref-mcgrath2022acquisition" role="doc-biblioref">McGrath et al., 2022</a>)</span>.</p>
<p>Methodologically, the focus often lies in designing novel algorithmic features to improve the cooperativeness of RL algorithms in large-scale environments. For example, algorithms may be equipped with abilities, such as sending each other messages <span class="citation" data-cites="FoersterEtAl2016">(<a href="09_References.html#ref-FoersterEtAl2016" role="doc-biblioref">Foerster et al., 2016</a>)</span>, making commitments <span class="citation" data-cites="HughesEtAl2020a christoffersen2022get">(<a href="09_References.html#ref-christoffersen2022get" role="doc-biblioref">Christoffersen et al., 2022</a>; <a href="09_References.html#ref-HughesEtAl2020a" role="doc-biblioref">Hughes et al., 2020</a>)</span>, or transferring rewards to others <span class="citation" data-cites="lupu2020gifting WangEtAl2021">(<a href="09_References.html#ref-lupu2020gifting" role="doc-biblioref">Lupu &amp; Precup, 2020</a>; <a href="09_References.html#ref-WangEtAl2021" role="doc-biblioref">W. Z. Wang et al., 2021</a>)</span>. Algorithms are evaluated for their ability to produce agents and multi-agent systems that can generalize, i.e., perform well under conditions they never saw during training, such as situations where they must interact with unfamiliar AI social partners <span class="citation" data-cites="leibo2021scalable stone2010ad">(<a href="09_References.html#ref-leibo2021scalable" role="doc-biblioref">Leibo et al., 2021</a>; <a href="09_References.html#ref-stone2010ad" role="doc-biblioref">Stone et al., 2010</a>)</span> or humans <span class="citation" data-cites="carroll2019utility DBLP:journals/corr/abs-2110-08176 meta2022human">(<a href="09_References.html#ref-carroll2019utility" role="doc-biblioref">Carroll et al., 2019</a>; <a href="09_References.html#ref-meta2022human" role="doc-biblioref">(FAIR)† et al., 2022</a>; <a href="09_References.html#ref-DBLP:journals/corr/abs-2110-08176" role="doc-biblioref">Strouse et al., 2021</a>)</span>. Measuring generalization to a fixed set of test scenarios allows researchers to compare the performance of MARL algorithms to one another despite incompatibilities in their training. In contrast to CSS studies, <em>cooperation</em> is typically not an available action to choose from. Instead, implementing a cooperative strategy must be learned from scratch <span class="citation" data-cites="LeiboEtAl2017">(<a href="09_References.html#ref-LeiboEtAl2017" role="doc-biblioref">Leibo et al., 2017</a>)</span>, and performance is measured by total social welfare.</p>
<p>However, MARL simulation studies on their own are challenging to use to obtain analytically reliable insights into how collective cooperation emerges from complex human and machine behavior in dynamic environments. They often require significant computational resources, while the space to explore suffers from the curse of dimensionality. Moreover, they are typically highly stochastic, and results can be difficult to interpret <span class="citation" data-cites="Hernandez-LealEtAl2019">(<a href="09_References.html#ref-Hernandez-LealEtAl2019" role="doc-biblioref">Hernandez-Leal et al., 2019</a>)</span>. We believe that a unified approach that combines approaches from CSS and MARL could fill this gap.</p>
</section>
<section id="examplary-works-on-the-learning-dynamics-of-cooperation" class="level2">
<h2 class="anchored" data-anchor-id="examplary-works-on-the-learning-dynamics-of-cooperation">Examplary works on the learning dynamics of cooperation</h2>
<p>The study of cooperation has not been at the center of Collective Reinforcement Learning Dynamics (CRLD) studies. Here we list some notable examples from mathematical biology and sociology.</p>
<ul>
<li>L. Panait, K. Tuyls, S. Luke, Theoretical advantages of lenient learners: An evolutionary game theoretic perspective. J. Mach. Learn. Res. 9, 423–457 (2008).</li>
<li>S. S. Izquierdo, L. R. Izquierdo, N. M. Gotts, Reinforcement learning dynamics in social dilemmas. J. Artif. Soc. Soc. Simul. 11, 1 (2008).</li>
<li>M. Wunder, M. L. Littman, M. Babes, Classes of multiagent Q-learning dynamics with epsilon-greedy exploration in ICML (2010).</li>
<li>N. Masuda, M. Nakamura, Numerical analysis of a reinforcement learning model with the dynamic aspiration level in the iterated Prisoner’s dilemma. J. Theor. Biol. 278, 55–62 (2011).</li>
<li>S. Tanabe, N. Masuda, Evolution of cooperation facilitated by reinforcement learning with adaptive aspiration levels. J. Theor. Biol. 293, 151–160 (2012).</li>
<li>T. Ezaki, Y. Horita, M. Takezawa, N. Masuda, Reinforcement learning explains conditional cooperation and its moody cousin. PLoS Comput. Biol. 12, e1005034 (2016).</li>
<li>S. Dridi, E. Akçay, Learning to cooperate: The evolution of social rewards in repeated interactions. Am. Nat. 191, 58–73 (2018).</li>
<li>O. Leimar, J. M. McNamara, Learning leads to bounded rationality and the evolution of cognitive bias in public goods games. Sci. Rep.&nbsp;9, 16319 (2019).</li>
<li>W. Barfuss, J. F. Donges, V. V. Vasconcelos, J. Kurths, S. A. Levin, Caring for the future can turn tragedy into comedy for long-term collective action under risk of collapse. Proc. Natl. Acad. Sci. U.S.A. 117, 12915– 12922 (2020).</li>
<li>W. Z. Wang et al., “Emergent prosociality in multi-agent games through gifting” in Twenty-Ninth International Joint Conference on Artificial Intelligence (2021), vol.&nbsp;1, pp.&nbsp;434–442.</li>
<li>L. Wang et al., Lévy noise promotes cooperation in the Prisoner’s dilemma game with reinforcement learning. Nonlinear Dyn. 108, 1837–1845 (2022).</li>
<li>W. Barfuss, J. M. Meylahn, Intrinsic fluctuations of reinforcement learning promote cooperation. Sci. Rep.&nbsp;13, 1309 (2023).</li>
</ul>
</section>
<section id="sec-note" class="level2">
<h2 class="anchored" data-anchor-id="sec-note">On cooperation and social dilemmas</h2>
<p>In CSS, cooperation is frequently defined mechanistically. A cooperative act might involve colluding with a co-conspirator to remain quiet under interrogation <span class="citation" data-cites="Poundstone2011">(<a href="09_References.html#ref-Poundstone2011" role="doc-biblioref">Poundstone, 2011</a>)</span>, paying a cost to provide a benefit to another (e.g., measured in currency, time, or reproductive success) <span class="citation" data-cites="Sigmund2010">(<a href="09_References.html#ref-Sigmund2010" role="doc-biblioref">Sigmund, 2010</a>)</span>, or provisioning a public good or resource <span class="citation" data-cites="OstromEtAl1992 FehrGachter2000">(<a href="09_References.html#ref-FehrGachter2000" role="doc-biblioref">Fehr &amp; Gächter, 2000</a>; <a href="09_References.html#ref-OstromEtAl1992" role="doc-biblioref">Ostrom et al., 1992</a>)</span>. Game theory allows such a behavior to be modeled using abstract payoffs. <span class="citation" data-cites="Dawes1980">Dawes (<a href="09_References.html#ref-Dawes1980" role="doc-biblioref">1980</a>)</span> summarizes a social dilemma among <span class="math inline">\(N\)</span> agents, each with two atomic actions, <span class="math inline">\(C\)</span> (‘cooperate’) or <span class="math inline">\(D\)</span> (‘defect’), as follows:</p>
<ul>
<li><em>(i)</em> the payoff when all cooperate exceeds that when all defect and</li>
<li><em>(ii)</em> regardless of the composition of the group, a cooperator can always improve their own payoff by switching to defection.</li>
</ul>
<p>A simple example is a prisoner’s dilemma, which takes place in a collective of <span class="math inline">\(N=2\)</span> agents. With payoffs defined by the matrix <span id="eq-payoff-matrix"><span class="math display">\[
\begin{array}{c|cc}
\text{} &amp; \text{C} &amp; \text{D} \\
\hline
\text{C} &amp; R &amp; S \\
\text{D} &amp; T &amp; P \\
\end{array}
\tag{2.1}\]</span></span> a social dilemma requires <span class="math inline">\(T&gt;R&gt;P&gt;S\)</span>, which is the <em>definition</em> of a prisoner’s dilemma <span class="citation" data-cites="Axelrod1984">(<a href="09_References.html#ref-Axelrod1984" role="doc-biblioref">Axelrod, 1984</a>)</span>.</p>
<p>Cooperation becomes a graded quantity when a social dilemma is repeated, although it is still based on (atomic) cooperative actions in each round. As <span class="citation" data-cites="LeiboEtAl2017">Leibo et al. (<a href="09_References.html#ref-LeiboEtAl2017" role="doc-biblioref">2017</a>)</span> note, what constitutes cooperation in spatially and/or temporally extended environments is more complicated and cannot determined using just reduction to a prisoner’s dilemma via empirical game-theoretic analysis (EGTA). EGTA is an approach to game theory that combines expert modeling with empirical data of gameplay. High-dimensional game models are reduced to so-called meta-games via a small set of heuristic strategies. The meta-game, or empirical game, is a simplified model of the high-dimensional game that is used to gain an improved qualitative understanding of the complex multi-agent interaction <span class="citation" data-cites="TuylsEtAl2019">(<a href="09_References.html#ref-TuylsEtAl2019" role="doc-biblioref">Tuyls et al., 2019</a>)</span>.</p>
<p>Avoiding mechanistic considerations altogether, a useful way of thinking about cooperation is in terms of how a collective can jointly achieve higher payoffs, particularly when individual agents cannot force such outcomes. Suppose that the outcome <span class="math inline">\(r^{\ast}\in\mathbb{R}^{N}\)</span> is supported in Nash equilibrium. By the definition of a Nash equilibrium, no agent can improve its payoff through unilateral deviations in its policy. Therefore, if <span class="math inline">\(r\in\mathbb{R}^{N}\)</span> is another outcome for which <span class="math inline">\(r_{i}\geqslant r_{i}^{\ast}\)</span> for all <span class="math inline">\(i=1,\dots ,N\)</span>, with at least one inequality strict, then no agent that would strictly benefit when the collective moves from <span class="math inline">\(r^{\ast}\)</span> to <span class="math inline">\(r\)</span> can force this outcome, even though all agents would be at least as well off in <span class="math inline">\(r\)</span> as in <span class="math inline">\(r^{\ast}\)</span>. Doing so is said to require `cooperation’ <span class="citation" data-cites="Cohen1998">(<a href="09_References.html#ref-Cohen1998" role="doc-biblioref">Cohen, 1998</a>)</span>.</p>
<p>Thinking of cooperation in this way hearkens back the notion of a social dilemma. If <span class="math inline">\(\left(D,D\right)\)</span> is a Nash equilibrium, then <span class="math inline">\(P&gt;S\)</span>. Neither <span class="math inline">\(\left(C,D\right)\)</span> nor <span class="math inline">\(\left(D,C\right)\)</span> can Pareto-dominate <span class="math inline">\(\left(D,D\right)\)</span> due to this inequality, so for ‘cooperation’ to exist it must be the case that <span class="math inline">\(R&gt;P\)</span>. One possibility for the final payoff is that <span class="math inline">\(T\leqslant R\)</span>, in which case <span class="math inline">\(\left(C,C\right)\)</span> is also a Nash equilibrium. Such is the case in the stag hunt game <span class="citation" data-cites="Skyrms2004">(<a href="09_References.html#ref-Skyrms2004" role="doc-biblioref">Skyrms, 2004</a>)</span>. Although this situation describes an interaction in which social welfare can be improved via cooperation, it is not strictly a social dilemma by the definition we used above, because the incentives of the individuals and the pair are not opposing. Rather, it represents an equilibrium selection problem. If, <span class="math inline">\(T&gt;R\)</span> instead, then <span class="math inline">\(T&gt;R&gt;P&gt;S\)</span>, the defining inequalities of a prisoner’s dilemma.</p>
<p>Importantly, the Pareto-dominated outcome (<span class="math inline">\(r^{\ast}\)</span> above) need not be supported in Nash equilibrium in order to define a relevant notion of cooperation. Instead, one might impose the condition that there exists no sequence of unilateral, individually-rational deviations leading the outcome from <span class="math inline">\(r^{\ast}\)</span> to an outcome that Pareto-dominates <span class="math inline">\(r^{\ast}\)</span>. For the game depicted in <a href="#eq-payoff-matrix" class="quarto-xref">Equation&nbsp;<span>2.1</span></a>, this condition allows <span class="math inline">\(S\geqslant P\)</span> as long as <span class="math inline">\(T&gt;R\)</span>. Such is the case in the snowdrift game <span class="citation" data-cites="Sugden2004">(<a href="09_References.html#ref-Sugden2004" role="doc-biblioref">Sugden, 2004</a>)</span>, in which two drivers are stuck on either side of a snowdrift blocking the road and must decide on who clears it. In contrast to the prisoner’s dilemma, a driver is still better off cooperating (clearing the snowdrift) even when the other driver does nothing. One would also prefer to have the co-player do all of the clearing than to collaborate. A simple example in which ‘cooperation’ does not exist–even under this relaxed definition–is the harmony game <span class="citation" data-cites="Hauert2002">(<a href="09_References.html#ref-Hauert2002" role="doc-biblioref">Hauert, 2002</a>)</span>, which satisfies <span class="math inline">\(R&gt;T&gt;S&gt;P\)</span> and possesses the property that the unique Nash equilbrium, <span class="math inline">\(\left(C,C\right)\)</span>, is also Pareto-efficient.</p>
<p>The prisoner’s dilemma, and more generally the definition of <span class="citation" data-cites="Dawes1980">Dawes (<a href="09_References.html#ref-Dawes1980" role="doc-biblioref">1980</a>)</span>, characterize ‘strict’ social dilemmas. There are also ‘weaker’ social dilemmas describing conflicts of interest to lesser degrees. Again using the game in <a href="#eq-payoff-matrix" class="quarto-xref">Equation&nbsp;<span>2.1</span></a>, <span class="citation" data-cites="HauertEtAl2006">Hauert et al. (<a href="09_References.html#ref-HauertEtAl2006" role="doc-biblioref">2006</a>)</span> stipulate that a weak social dilemma should satisfy</p>
<ul>
<li><em>(i)</em> <span class="math inline">\(R&gt;P\)</span>;</li>
<li><em>(ii)</em> <span class="math inline">\(T&gt;S\)</span>; and</li>
<li><em>(iii)</em> <span class="math inline">\(R&gt;S\)</span> and <span class="math inline">\(T&gt;P\)</span>.</li>
</ul>
<p>The intuition behind these conditions is that <em>(i)</em> the payoff for mutual cooperation should exceed that of mutual defection; <em>(ii)</em> in mixed groups, the payoff to defectors should exceed that of cooperators; and <em>(iii)</em> regardless of what action a focal agent takes, they are better off when the co-player cooperates than when the co-player defects. The harmony game satisfies these inequalities, so it is considered a weak social dilemma despite the fact that it has no notion of ‘cooperation’ according to the definition of a ‘strict’ social dilemma. In addition to the prisoner’s dilemma and the harmony game, the remaining two weak social dilemmas are the snowdrift and stag hunt games. As one might expect, the behavior of a weak social dilemma in CSS depends on which of these classes of games it falls under <span class="citation" data-cites="HauertDoebeli2004">(<a href="09_References.html#ref-HauertDoebeli2004" role="doc-biblioref">Hauert &amp; Doebeli, 2004</a>)</span>, rather than just the fact that it’s a weak social dilemma. <!-- This property lies in contrast to strict social dilemmas, which exhibit more robustness owing to their rigidity. --></p>
<p>However, even in strict social dilemmas, we caution that the presence of alternative actions can destabilize conflicts of interest. For example, suppose that in addition to the actions <span class="math inline">\(C\)</span> and <span class="math inline">\(D\)</span> in a prisoner’s dilemma, each player can take action <span class="math inline">\(G\)</span>, which is interpreted as avoiding the prisoner’s dilemma and instead collecting a pot of gold (at no cost). If both players have separate pots of gold available to collect and the value of this gold exceeds all of the prisoner’s dilemma payoffs, then the unique Nash equilibrium of this augmented game is <span class="math inline">\(\left(G,G\right)\)</span>, which is also Pareto-efficient. Like in the harmony game, there is no strict notion of ‘cooperation’ in the sense of Pareto dominance. Most importantly, there is no conflict of interest and thus no strict social dilemma. It is irrelevant that there are options <span class="math inline">\(C\)</span> and <span class="math inline">\(D\)</span> such that <span class="math inline">\(T&gt;R&gt;P&gt;S\)</span>; this ‘embedded’ game is merely a decoy. Only when the action <span class="math inline">\(G\)</span> is unavailable or unknown would the agents view this interaction as a social dilemma. In this sense, social dilemmas need not be preserved upon inclusion into larger games. In this example, one can easily recognize the option <span class="math inline">\(G\)</span> as trivializing the game, but in realistic applications, especially those involving EGTA, it might be entirely unclear whether there are true conflicts of interest. Intriguingly, the augmented game described above could still be considered a sequential social dilemma <span class="citation" data-cites="LeiboEtAl2017">(<a href="09_References.html#ref-LeiboEtAl2017" role="doc-biblioref">Leibo et al., 2017</a>)</span>, owing to the fact that the reference policies representing cooperation and defection can be chosen freely (and thus can represent policies in a smaller, embedded game).</p>
<p>Along these lines, the reduction to matrix games via EGTA could result in too much averaging with respect to social dilemmas. One might instead map a stochastic game not to a matrix game but to a down-sampled stochastic game with a smaller number of ‘salient’ states. A simple example would be when two agents interact in a grid world, with two colors distributed throughout the grid according to some distribution. The two players drift throughout the space via independent, unbiased random walks. When they appear on neighboring tiles, they play one of two matrix games, a prisoner’s dilemma or a harmony game, depending on whether the tiles have the same or different colors. There are then three relevant matrix games: the two played when on neighboring tiles and one ‘null’ game in which rewards are zero when neighbors on non-neighboring tiles. While one may view this game as having a large state space based on the agents’ positions on the grid, this scenario can also be modeled as a three-state game whose transitions are governed by a hidden Markov model (due to the structure of the grid and configuration of its colors). Nonetheless, by averaging appropriately, one might expect to obtain a useful approximation via a stochastic game with just three states. It is an open question whether further reduction to a matrix game would wash out important artifacts of this spatially-extended game.</p>
<p>Regarding cooperation and the goals of CSS, one (rather misleading) aspect of CSS models that could be informed better by the goals of MARL is the fact that social welfare, even in strict social dilemmas, need not be a monotonic function of the level of cooperation. The goal should not always be ‘more cooperation’ in a mechanistic sense. In the prisoner’s dilemma, many studies in CSS make the simplifying assumption that <span class="math inline">\(R&gt;\left(S+T\right) /2\)</span>, which implies that a socially efficient outcome can be attained by full mutual cooperation. However, there are also prisoner’s dilemmas for which <span class="math inline">\(R&lt;\left(S+T\right) /2\)</span>, in which case both agents can do better by agreeing on a strategy of alternation: one agent cooperates in even time steps only, while the other cooperates in odd time steps only. Moving from the mutually cooperative outcome of <span class="math inline">\(\left(R, R\right)\)</span> to the Pareto-dominant outcome of <span class="math inline">\(\left(\left(S+T\right) /2, \left(S+T\right) /2\right)\)</span> requires ‘cooperation’, despite the fact that the latter involves a lower level of the atomic action ‘cooperate’ than the former. Thus, what constitutes a cooperative strategy in a temporally extended social dilemma might be decoupled from what constitutes a cooperative action in the underlying stage game, an observation that has not fully penetrated CSS <span class="citation" data-cites="McAvoyEtAl2022">(<a href="09_References.html#ref-McAvoyEtAl2022" role="doc-biblioref">McAvoy et al., 2022</a>)</span> despite being understood in MARL <span class="citation" data-cites="LeiboEtAl2017">(<a href="09_References.html#ref-LeiboEtAl2017" role="doc-biblioref">Leibo et al., 2017</a>)</span>.</p>
<p>In summary, what constitutes ‘cooperation’ depends on the context. In both CSS and MARL, seemingly isolated systems of agents can involve externalities that affect how an interaction is characterized/understood. If a cooperative social dilemma is actually a zero-sum game among <span class="math inline">\(N\)</span> players and the environment, with the environment getting depleted as the social welfare increases, then the ‘goals’ in such an environment are ambiguous. Agents might also transition between such states and those involving the possibility of true surpluses. Complicating matters further, agents could transition among states involving different numbers of agents, including those with only a single agent and the environment. In turn, an agent can reasonably have many different conceptions of what ‘cooperation’ means, even on short timescales. Rolling such ephemeral interactions into `cooperative strategies’ is only more complicated.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list" style="display: none">
<div id="ref-Arthur1994" class="csl-entry" role="listitem">
Arthur, W. B. (1994). Inductive reasoning and bounded rationality. <em>The American Economic Review</em>, <em>84</em>(2), 406–411.
</div>
<div id="ref-Arthur2014" class="csl-entry" role="listitem">
Arthur, W. B. (2014). <em>Complexity and the <span>Economy</span></em>. <span>Oxford University Press</span>.
</div>
<div id="ref-Axelrod1984" class="csl-entry" role="listitem">
Axelrod, R. (1984). <em>The <span>Evolution Of Cooperation</span></em>. <span>Basic Books</span>.
</div>
<div id="ref-AxelrodHamilton1981" class="csl-entry" role="listitem">
Axelrod, R., &amp; Hamilton, W. D. (1981). The <span>Evolution</span> of <span>Cooperation</span>. <em>Science</em>, <em>211</em>(4489), 1390–1396. <a href="https://doi.org/10.1126/science.7466396">https://doi.org/10.1126/science.7466396</a>
</div>
<div id="ref-BernerEtAl2019" class="csl-entry" role="listitem">
Berner, C., Brockman, G., Chan, B., Cheung, V., Dębiak, P., Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse, C., Józefowicz, R., Gray, S., Olsson, C., Pachocki, J., Petrov, M., Pinto, H. P. d. O., Raiman, J., Salimans, T., Schlatter, J., … Zhang, S. (2019). <em>Dota 2 with <span>Large Scale Deep Reinforcement Learning</span></em> (arXiv:1912.06680). <span>arXiv</span>. <a href="https://doi.org/10.48550/arXiv.1912.06680">https://doi.org/10.48550/arXiv.1912.06680</a>
</div>
<div id="ref-bialek_statistical_2012" class="csl-entry" role="listitem">
Bialek, W., Cavagna, A., Giardina, I., Mora, T., Silvestri, E., Viale, M., &amp; Walczak, A. M. (2012). Statistical mechanics for natural flocks of birds. <em>Proceedings of the National Academy of Sciences</em>, <em>109</em>(13), 4786–4791. <a href="https://doi.org/10.1073/pnas.1118633109">https://doi.org/10.1073/pnas.1118633109</a>
</div>
<div id="ref-BotvinickEtAl2020" class="csl-entry" role="listitem">
Botvinick, M., Wang, J. X., Dabney, W., Miller, K. J., &amp; Kurth-Nelson, Z. (2020). Deep <span>Reinforcement Learning</span> and <span>Its Neuroscientific Implications</span>. <em>Neuron</em>, <em>107</em>(4), 603–616. <a href="https://doi.org/10.1016/j.neuron.2020.06.014">https://doi.org/10.1016/j.neuron.2020.06.014</a>
</div>
<div id="ref-brush_conflicts_2018" class="csl-entry" role="listitem">
Brush, E. R., Krakauer, D. C., &amp; Flack, J. C. (2018). Conflicts of interest improve collective computation of adaptive social structures. <em>Science Advances</em>, <em>4</em>(1), e1603311. <a href="https://doi.org/10.1126/sciadv.1603311">https://doi.org/10.1126/sciadv.1603311</a>
</div>
<div id="ref-buckley_free_2017" class="csl-entry" role="listitem">
Buckley, C. L., Kim, C. S., McGregor, S., &amp; Seth, A. K. (2017). The free energy principle for action and perception: <span>A</span> mathematical review. <em>Journal of Mathematical Psychology</em>, <em>81</em>, 55–79. <a href="https://doi.org/10.1016/j.jmp.2017.09.004">https://doi.org/10.1016/j.jmp.2017.09.004</a>
</div>
<div id="ref-buhl_disorder_2006" class="csl-entry" role="listitem">
Buhl, J., Sumpter, D. J. T., Couzin, I. D., Hale, J. J., Despland, E., Miller, E. R., &amp; Simpson, S. J. (2006). From <span>Disorder</span> to <span>Order</span> in <span>Marching</span> <span>Locusts</span>. <em>Science</em>, <em>312</em>(5778), 1402–1406. <a href="https://doi.org/10.1126/science.1125142">https://doi.org/10.1126/science.1125142</a>
</div>
<div id="ref-BushMosteller1951" class="csl-entry" role="listitem">
Bush, R. R., &amp; Mosteller, F. (1951). A mathematical model for simple learning. <em>Psychological Review</em>, <em>58</em>, 313–323. <a href="https://doi.org/10.1037/h0054388">https://doi.org/10.1037/h0054388</a>
</div>
<div id="ref-BusoniuEtAl2008" class="csl-entry" role="listitem">
Busoniu, L., Babuska, R., &amp; De Schutter, B. (2008). A comprehensive survey of multiagent reinforcement learning. <em>IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)</em>, <em>38</em>(2), 156–172.
</div>
<div id="ref-Camerer2011" class="csl-entry" role="listitem">
Camerer, C. F. (2011). <em>Behavioral game theory: <span>Experiments</span> in strategic interaction</em>. <span>Princeton university press</span>.
</div>
<div id="ref-carroll2019utility" class="csl-entry" role="listitem">
Carroll, M., Shah, R., Ho, M. K., Griffiths, T., Seshia, S., Abbeel, P., &amp; Dragan, A. (2019). On the utility of learning about humans for human-ai coordination. <em>Advances in Neural Information Processing Systems</em>, <em>32</em>.
</div>
<div id="ref-christoffersen2022get" class="csl-entry" role="listitem">
Christoffersen, P. J., Haupt, A. A., &amp; Hadfield-Menell, D. (2022). Get it in writing: Formal contracts mitigate social dilemmas in multi-agent RL. <em>arXiv Preprint arXiv:2208.10469</em>.
</div>
<div id="ref-Cohen1998" class="csl-entry" role="listitem">
Cohen, J. E. (1998). Cooperation and self-interest: <span class="nocase">Pareto-inefficiency</span> of <span>Nash</span> equilibria in finite random games. <em>Proceedings of the National Academy of Sciences</em>, <em>95</em>(17), 9724–9731. <a href="https://doi.org/10.1073/pnas.95.17.9724">https://doi.org/10.1073/pnas.95.17.9724</a>
</div>
<div id="ref-Cross1973" class="csl-entry" role="listitem">
Cross, J. G. (1973). A <span>Stochastic Learning Model</span> of <span>Economic Behavior</span>*. <em>The Quarterly Journal of Economics</em>, <em>87</em>(2), 239–266. <a href="https://doi.org/10.2307/1882186">https://doi.org/10.2307/1882186</a>
</div>
<div id="ref-DafoeEtAl2021" class="csl-entry" role="listitem">
Dafoe, A., Bachrach, Y., Hadfield, G., Horvitz, E., Larson, K., &amp; Graepel, T. (2021). Cooperative <span>AI</span>: Machines must learn to find common ground. <em>Nature</em>, <em>593</em>(7857), 33–36. <a href="https://doi.org/10.1038/d41586-021-01170-0">https://doi.org/10.1038/d41586-021-01170-0</a>
</div>
<div id="ref-daniels_quantifying_2016" class="csl-entry" role="listitem">
Daniels, B. C., Ellison, C. J., Krakauer, D. C., &amp; Flack, J. C. (2016). Quantifying collectivity. <em>Current Opinion in Neurobiology</em>, <em>37</em>, 106–113. <a href="https://doi.org/10.1016/j.conb.2016.01.012">https://doi.org/10.1016/j.conb.2016.01.012</a>
</div>
<div id="ref-daniels_control_2017" class="csl-entry" role="listitem">
Daniels, B. C., Krakauer, D. C., &amp; Flack, J. C. (2017). Control of finite critical behaviour in a small-scale social system. <em>Nature Communications</em>, <em>8</em>(1), 14301. <a href="https://doi.org/10.1038/ncomms14301">https://doi.org/10.1038/ncomms14301</a>
</div>
<div id="ref-daniels_introduction_2021" class="csl-entry" role="listitem">
Daniels, B. C., Laubichler, M. D., &amp; Flack, J. C. (2021). Introduction to the special issue: Quantifying collectivity. <em>Theory in Biosciences</em>, <em>140</em>(4), 321–323. <a href="https://doi.org/10.1007/s12064-021-00358-2">https://doi.org/10.1007/s12064-021-00358-2</a>
</div>
<div id="ref-darriba_predictions_2018" class="csl-entry" role="listitem">
Darriba, Á., &amp; Waszak, F. (2018). Predictions through evidence accumulation over time. <em>Scientific Reports</em>, <em>8</em>(1), 494. <a href="https://doi.org/10.1038/s41598-017-18802-z">https://doi.org/10.1038/s41598-017-18802-z</a>
</div>
<div id="ref-Dawes1980" class="csl-entry" role="listitem">
Dawes, R. M. (1980). Social <span>Dilemmas</span>. <em>Annual Review of Psychology</em>, <em>31</em>(1), 169–193. <a href="https://doi.org/10.1146/annurev.ps.31.020180.001125">https://doi.org/10.1146/annurev.ps.31.020180.001125</a>
</div>
<div id="ref-DayanNiv2008" class="csl-entry" role="listitem">
Dayan, P., &amp; Niv, Y. (2008). Reinforcement learning: <span>The Good</span>, <span>The Bad</span> and <span>The Ugly</span>. <em>Current Opinion in Neurobiology</em>, <em>18</em>(2), 185–196. <a href="https://doi.org/10.1016/j.conb.2008.08.003">https://doi.org/10.1016/j.conb.2008.08.003</a>
</div>
<div id="ref-de_marzo_quantifying_2022" class="csl-entry" role="listitem">
De Marzo, G., Gabrielli, A., Zaccaria, A., &amp; Pietronero, L. (2022). Quantifying the unexpected: <span>A</span> scientific approach to <span>Black</span> <span>Swans</span>. <em>Physical Review Research</em>, <em>4</em>(3), 033079. <a href="https://doi.org/10.1103/PhysRevResearch.4.033079">https://doi.org/10.1103/PhysRevResearch.4.033079</a>
</div>
<div id="ref-dedeo2010inductive" class="csl-entry" role="listitem">
DeDeo, S., Krakauer, D. C., &amp; Flack, J. C. (2010). Inductive game theory and the dynamics of animal conflict. <em>PLoS Computational Biology</em>, <em>6</em>(5), e1000782.
</div>
<div id="ref-epstein_growing_1996" class="csl-entry" role="listitem">
Epstein, J. M., &amp; Axtell, R. L. (1996). <em>Growing <span>Artificial</span> <span>Societies</span>: <span>Social</span> <span>Science</span> <span>From</span> the <span>Bottom</span> <span>Up</span></em> (First Edition). Brookings Institution Press.
</div>
<div id="ref-ErevRoth1998" class="csl-entry" role="listitem">
Erev, I., &amp; Roth, A. E. (1998). Predicting <span>How People Play Games</span>: <span>Reinforcement Learning</span> in <span>Experimental Games</span> with <span>Unique</span>, <span>Mixed Strategy Equilibria</span>. <em>The American Economic Review</em>, <em>88</em>(4), 848–881.
</div>
<div id="ref-meta2022human" class="csl-entry" role="listitem">
(FAIR)†, M. F. A. R. D. T., Bakhtin, A., Brown, N., Dinan, E., Farina, G., Flaherty, C., Fried, D., Goff, A., Gray, J., Hu, H., et al. (2022). Human-level play in the game of diplomacy by combining language models with strategic reasoning. <em>Science</em>, <em>378</em>(6624), 1067–1074.
</div>
<div id="ref-FehrGachter2000" class="csl-entry" role="listitem">
Fehr, E., &amp; Gächter, S. (2000). Cooperation and <span>Punishment</span> in <span>Public Goods Experiments</span>. <em>American Economic Review</em>, <em>90</em>(4), 980–994. <a href="https://doi.org/10.1257/aer.90.4.980">https://doi.org/10.1257/aer.90.4.980</a>
</div>
<div id="ref-flack_coarse_graining_2017" class="csl-entry" role="listitem">
Flack, J. C. (2017). Coarse-graining as a downward causation mechanism. <em>Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences</em>, <em>375</em>(2109), 20160338. <a href="https://doi.org/10.1098/rsta.2016.0338">https://doi.org/10.1098/rsta.2016.0338</a>
</div>
<div id="ref-FoersterEtAl2016" class="csl-entry" role="listitem">
Foerster, J., Assael, I. A., de Freitas, N., &amp; Whiteson, S. (2016). Learning to <span>Communicate</span> with <span>Deep Multi-Agent Reinforcement Learning</span>. <em>Advances in <span>Neural Information Processing Systems</span></em>, <em>29</em>.
</div>
<div id="ref-FranciEtAl2022" class="csl-entry" role="listitem">
Franci, A., Golubitsky, M., Stewart, I., Bizyaeva, A., &amp; Leonard, N. E. (2022). Breaking indecision in multi-agent, multi-option dynamics. <em>arXiv Preprint arXiv:2206.14893</em>.
</div>
<div id="ref-friston_does_2018" class="csl-entry" role="listitem">
Friston, K. (2018). Does predictive coding have a future? <em>Nature Neuroscience</em>, <em>21</em>(8), 1019–1021. <a href="https://doi.org/10.1038/s41593-018-0200-7">https://doi.org/10.1038/s41593-018-0200-7</a>
</div>
<div id="ref-FudenbergLevine1998" class="csl-entry" role="listitem">
Fudenberg, D., &amp; Levine, D. K. (1998). <em>The <span>Theory</span> of <span>Learning</span> in <span>Games</span></em> (K. Binmore, Ed.). <span>MIT Press</span>.
</div>
<div id="ref-grupen2022conceptbased" class="csl-entry" role="listitem">
Grupen, N., Jaques, N., Kim, B., &amp; Omidshafiei, S. (2022). Concept-based understanding of emergent multi-agent behavior. <em>Deep Reinforcement Learning Workshop NeurIPS 2022</em>. <a href="https://openreview.net/forum?id=zt5JpGQ8WhH">https://openreview.net/forum?id=zt5JpGQ8WhH</a>
</div>
<div id="ref-Gunawardena2022" class="csl-entry" role="listitem">
Gunawardena, J. (2022). Learning <span>Outside</span> the <span>Brain</span>: <span>Integrating Cognitive Science</span> and <span>Systems Biology</span>. <em>Proceedings of the IEEE</em>, 1–23. <a href="https://doi.org/10.1109/JPROC.2022.3162791">https://doi.org/10.1109/JPROC.2022.3162791</a>
</div>
<div id="ref-Hauert2002" class="csl-entry" role="listitem">
Hauert, C. (2002). Effects of space in 2 2 games. <em>International Journal of Bifurcation and Chaos</em>, <em>12</em>(07), 1531–1548. <a href="https://doi.org/10.1142/S0218127402005273">https://doi.org/10.1142/S0218127402005273</a>
</div>
<div id="ref-HauertDoebeli2004" class="csl-entry" role="listitem">
Hauert, C., &amp; Doebeli, M. (2004). Spatial structure often inhibits the evolution of cooperation in the snowdrift game. <em>Nature</em>, <em>428</em>(6983), 643–646. <a href="https://doi.org/10.1038/nature02360">https://doi.org/10.1038/nature02360</a>
</div>
<div id="ref-HauertEtAl2006" class="csl-entry" role="listitem">
Hauert, C., Michor, F., Nowak, M. A., &amp; Doebeli, M. (2006). Synergy and discounting of cooperation in social dilemmas. <em>Journal of Theoretical Biology</em>, <em>239</em>(2), 195–202. <a href="https://doi.org/10.1016/j.jtbi.2005.08.040">https://doi.org/10.1016/j.jtbi.2005.08.040</a>
</div>
<div id="ref-HauserEtAl2019" class="csl-entry" role="listitem">
Hauser, O. P., Hilbe, C., Chatterjee, K., &amp; Nowak, M. A. (2019). Social dilemmas among unequals. <em>Nature</em>, <em>572</em>(7770), 524–527.
</div>
<div id="ref-heins_collective_2023" class="csl-entry" role="listitem">
Heins, C., Millidge, B., Costa, L. da, Mann, R., Friston, K., &amp; Couzin, I. (2023). <em>Collective behavior from surprise minimization</em>. arXiv. <a href="http://arxiv.org/abs/2307.14804">http://arxiv.org/abs/2307.14804</a>
</div>
<div id="ref-Hernandez-LealEtAl2019" class="csl-entry" role="listitem">
Hernandez-Leal, P., Kartal, B., &amp; Taylor, M. E. (2019). A survey and critique of multiagent deep reinforcement learning. <em>Autonomous Agents and Multi-Agent Systems</em>, <em>33</em>(6), 750–797. <a href="https://doi.org/10.1007/s10458-019-09421-1">https://doi.org/10.1007/s10458-019-09421-1</a>
</div>
<div id="ref-hilbe:NHB:2018" class="csl-entry" role="listitem">
Hilbe, C., Chatterjee, K., &amp; Nowak, M. A. (2018). <span class="nocase">Partners and rivals in direct reciprocity</span>. <em>Nature Human Behaviour</em>. <a href="https://doi.org/10.1038/s41562-018-0320-9">https://doi.org/10.1038/s41562-018-0320-9</a>
</div>
<div id="ref-HilbeEtAl2018" class="csl-entry" role="listitem">
Hilbe, C., Šimsa, Š., Chatterjee, K., &amp; Nowak, M. A. (2018). Evolution of cooperation in stochastic games. <em>Nature</em>, <em>559</em>(7713), 246–249. <a href="https://doi.org/10.1038/s41586-018-0277-x">https://doi.org/10.1038/s41586-018-0277-x</a>
</div>
<div id="ref-HofbauerSigmund1998" class="csl-entry" role="listitem">
Hofbauer, J., &amp; Sigmund, K. (1998). <em>Evolutionary <span>Games</span> and <span>Population Dynamics</span></em> (First). <span>Cambridge University Press</span>. <a href="https://doi.org/10.1017/CBO9781139173179">https://doi.org/10.1017/CBO9781139173179</a>
</div>
<div id="ref-HollandMiller1991" class="csl-entry" role="listitem">
Holland, J. H., &amp; Miller, J. H. (1991). Artificial <span>Adaptive Agents</span> in <span>Economic Theory</span>. <em>The American Economic Review</em>, <em>81</em>(2), 365–370.
</div>
<div id="ref-HughesEtAl2020a" class="csl-entry" role="listitem">
Hughes, E., Anthony, T. W., Eccles, T., Leibo, J. Z., Balduzzi, D., &amp; Bachrach, Y. (2020). Learning to <span>Resolve Alliance Dilemmas</span> in <span>Many-Player Zero-Sum Games</span>. <em>New <span>Zealand</span></em>, 10.
</div>
<div id="ref-jhawar_noise-induced_2020" class="csl-entry" role="listitem">
Jhawar, J., Morris, R. G., Amith-Kumar, U. R., Danny Raj, M., Rogers, T., Rajendran, H., &amp; Guttal, V. (2020). Noise-induced schooling of fish. <em>Nature Physics</em>, <em>16</em>(4), 488–493. <a href="https://doi.org/10.1038/s41567-020-0787-y">https://doi.org/10.1038/s41567-020-0787-y</a>
</div>
<div id="ref-kempes_thermodynamic_2017" class="csl-entry" role="listitem">
Kempes, C. P., Wolpert, D., Cohen, Z., &amp; Pérez-Mercader, J. (2017). The thermodynamic efficiency of computations made in cells across the range of life. <em>Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences</em>, <em>375</em>(2109), 20160343. <a href="https://doi.org/10.1098/rsta.2016.0343">https://doi.org/10.1098/rsta.2016.0343</a>
</div>
<div id="ref-kleshnina_effect_2023" class="csl-entry" role="listitem">
Kleshnina, M., Hilbe, C., Šimsa, Š., Chatterjee, K., &amp; Nowak, M. A. (2023). The effect of environmental information on evolution of cooperation in stochastic games. <em>Nature Communications</em>, <em>14</em>(1), 4153. <a href="https://doi.org/10.1038/s41467-023-39625-9">https://doi.org/10.1038/s41467-023-39625-9</a>
</div>
<div id="ref-krakauer_intelligent_2010" class="csl-entry" role="listitem">
Krakauer, D. C., Flack, J. C., Dedeo, S., Farmer, D., &amp; Rockmore, D. (2010). Intelligent <span>Data</span> <span>Analysis</span> of <span>Intelligent</span> <span>Systems</span>. In P. R. Cohen, N. M. Adams, &amp; M. R. Berthold (Eds.), <em>Advances in <span>Intelligent</span> <span>Data</span> <span>Analysis</span> <span>IX</span></em> (pp. 8–17). Springer. <a href="https://doi.org/10.1007/978-3-642-13062-5_3">https://doi.org/10.1007/978-3-642-13062-5_3</a>
</div>
<div id="ref-krakauer_information_2020" class="csl-entry" role="listitem">
Krakauer, D., Bertschinger, N., Olbrich, E., Flack, J. C., &amp; Ay, N. (2020). The information theory of individuality. <em>Theory in Biosciences</em>, <em>139</em>(2), 209–223. <a href="https://doi.org/10.1007/s12064-020-00313-7">https://doi.org/10.1007/s12064-020-00313-7</a>
</div>
<div id="ref-leibo2021scalable" class="csl-entry" role="listitem">
Leibo, J. Z., Dueñez-Guzman, E. A., Vezhnevets, A., Agapiou, J. P., Sunehag, P., Koster, R., Matyas, J., Beattie, C., Mordatch, I., &amp; Graepel, T. (2021). Scalable evaluation of multi-agent reinforcement learning with melting pot. <em>International Conference on Machine Learning</em>, 6187–6199.
</div>
<div id="ref-LeiboEtAl2017" class="csl-entry" role="listitem">
Leibo, J. Z., Zambaldi, V., Lanctot, M., Marecki, J., &amp; Graepel, T. (2017). Multi-agent <span>Reinforcement Learning</span> in <span>Sequential Social Dilemmas</span>. <em>Proceedings of the 16th <span>Conference</span> on <span>Autonomous Agents</span> and <span>MultiAgent Systems</span></em>, 464–473.
</div>
<div id="ref-levin_complex_2002" class="csl-entry" role="listitem">
Levin, S. (2002). Complex adaptive systems: <span>Exploring</span> the known, the unknown and the unknowable. <em>Bulletin of the American Mathematical Society</em>, <em>40</em>(1), 3–19. <a href="https://doi.org/10.1090/S0273-0979-02-00965-5">https://doi.org/10.1090/S0273-0979-02-00965-5</a>
</div>
<div id="ref-LoveringEtAl2022" class="csl-entry" role="listitem">
Lovering, C., Forde, J., Konidaris, G., Pavlick, E., &amp; Littman, M. (2022). Evaluation beyond task performance: Analyzing concepts in AlphaZero in hex. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, &amp; A. Oh (Eds.), <em>Advances in neural information processing systems</em> (Vol. 35, pp. 25992–26006). Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/a705747417d32ebf1916169e1a442274-Paper-Conference.pdf">https://proceedings.neurips.cc/paper_files/paper/2022/file/a705747417d32ebf1916169e1a442274-Paper-Conference.pdf</a>
</div>
<div id="ref-lupu2020gifting" class="csl-entry" role="listitem">
Lupu, A., &amp; Precup, D. (2020). Gifting in multi-agent reinforcement learning. <em>Proceedings of the 19th International Conference on Autonomous Agents and Multiagent Systems</em>, 789–797.
</div>
<div id="ref-MardenShamma2018" class="csl-entry" role="listitem">
Marden, J. R., &amp; Shamma, J. S. (2018). Game theory and control. <em>Annual Review of Control, Robotics, and Autonomous Systems</em>, <em>1</em>, 105–134.
</div>
<div id="ref-McAvoyEtAl2022" class="csl-entry" role="listitem">
McAvoy, A., Mori, Y., &amp; Plotkin, J. B. (2022). Selfish optimization and collective learning in populations. <em>Physica D: Nonlinear Phenomena</em>, <em>439</em>, 133426. <a href="https://doi.org/10.1016/j.physd.2022.133426">https://doi.org/10.1016/j.physd.2022.133426</a>
</div>
<div id="ref-mcgrath2022acquisition" class="csl-entry" role="listitem">
McGrath, T., Kapishnikov, A., Tomašev, N., Pearce, A., Wattenberg, M., Hassabis, D., Kim, B., Paquet, U., &amp; Kramnik, V. (2022). Acquisition of chess knowledge in <span>A</span>lpha<span>Z</span>ero. <em>Proceedings of the National Academy of Sciences</em>, <em>119</em>(47), e2206625119.
</div>
<div id="ref-McNamara2013" class="csl-entry" role="listitem">
McNamara, J. M. (2013). Towards a richer evolutionary game theory. <em>Journal of The Royal Society Interface</em>, <em>10</em>(88), 20130544. <a href="https://doi.org/10.1098/rsif.2013.0544">https://doi.org/10.1098/rsif.2013.0544</a>
</div>
<div id="ref-McNamaraEtAl2021" class="csl-entry" role="listitem">
McNamara, J. M., Houston, A. I., &amp; Leimar, O. (2021). Learning, exploitation and bias in games. <em>PLOS ONE</em>, <em>16</em>(2), e0246588. <a href="https://doi.org/10.1371/journal.pone.0246588">https://doi.org/10.1371/journal.pone.0246588</a>
</div>
<div id="ref-mora_are_2011" class="csl-entry" role="listitem">
Mora, T., &amp; Bialek, W. (2011). Are <span>Biological</span> <span>Systems</span> <span>Poised</span> at <span>Criticality</span>? <em>Journal of Statistical Physics</em>, <em>144</em>(2), 268–302. <a href="https://doi.org/10.1007/s10955-011-0229-4">https://doi.org/10.1007/s10955-011-0229-4</a>
</div>
<div id="ref-newman_structure_2003" class="csl-entry" role="listitem">
Newman, M. E. J. (2003). The <span>Structure</span> and <span>Function</span> of <span>Complex</span> <span>Networks</span>. <em>SIAM Review</em>, <em>45</em>(2), 167–256. <a href="https://doi.org/10.1137/S003614450342480">https://doi.org/10.1137/S003614450342480</a>
</div>
<div id="ref-Nowak2006a" class="csl-entry" role="listitem">
Nowak, M. A. (2006). <em>Evolutionary dynamics: Exploring the equations of life</em>. <span>Harvard university press</span>.
</div>
<div id="ref-OstromEtAl1992" class="csl-entry" role="listitem">
Ostrom, E., Walker, J., &amp; Gardner, R. (1992). Covenants with and without a <span>Sword</span>: <span>Self-Governance Is Possible</span>. <em>American Political Science Review</em>, <em>86</em>(2), 404–417. <a href="https://doi.org/10.2307/1964229">https://doi.org/10.2307/1964229</a>
</div>
<div id="ref-ParkEtAl2021" class="csl-entry" role="listitem">
Park, S., Bizyaeva, A., Kawakatsu, M., Franci, A., &amp; Leonard, N. E. (2021). Tuning cooperative behavior in games with nonlinear opinion dynamics. <em>IEEE Control Systems Letters</em>, <em>6</em>, 2030–2035.
</div>
<div id="ref-Poundstone2011" class="csl-entry" role="listitem">
Poundstone, W. (2011). <em>Prisoner’s <span>Dilemma</span></em>. <span>Knopf Doubleday Publishing Group</span>.
</div>
<div id="ref-press:PNAS:2012" class="csl-entry" role="listitem">
Press, W. H., &amp; Dyson, F. J. (2012). Iterated prisoner’s dilemma contains strategies that dominate any evolutionary opponent. <em>Proceedings of the National Academy of Sciences</em>, <em>109</em>(26), 10409–10413. <a href="https://doi.org/10.1073/pnas.1206569109 ">https://doi.org/10.1073/pnas.1206569109 </a>
</div>
<div id="ref-ramos-fernandez_collective_2020" class="csl-entry" role="listitem">
Ramos-Fernandez, G., Smith Aguilar, S. E., Krakauer, D. C., &amp; Flack, J. C. (2020). Collective <span>Computation</span> in <span>Animal</span> <span>Fission</span>-<span>Fusion</span> <span>Dynamics</span>. <em>Frontiers in Robotics and AI</em>, <em>7</em>. <a href="https://www.frontiersin.org/article/10.3389/frobt.2020.00090">https://www.frontiersin.org/article/10.3389/frobt.2020.00090</a>
</div>
<div id="ref-rao_predictive_1999" class="csl-entry" role="listitem">
Rao, R. P. N., &amp; Ballard, D. H. (1999). Predictive coding in the visual cortex: A functional interpretation of some extra-classical receptive-field effects. <em>Nature Neuroscience</em>, <em>2</em>(1), 79–87. <a href="https://doi.org/10.1038/4580">https://doi.org/10.1038/4580</a>
</div>
<div id="ref-rosas_quantifying_2019" class="csl-entry" role="listitem">
Rosas, F. E., Mediano, P. A. M., Gastpar, M., &amp; Jensen, H. J. (2019). Quantifying high-order interdependencies via multivariate extensions of the mutual information. <em>Physical Review E</em>, <em>100</em>(3), 032305. <a href="https://doi.org/10.1103/PhysRevE.100.032305">https://doi.org/10.1103/PhysRevE.100.032305</a>
</div>
<div id="ref-RothErev1995" class="csl-entry" role="listitem">
Roth, A. E., &amp; Erev, I. (1995). Learning in extensive-form games: <span>Experimental</span> data and simple dynamic models in the intermediate term. <em>Games and Economic Behavior</em>, <em>8</em>(1), 164–212. <a href="https://doi.org/10.1016/S0899-8256(05)80020-X">https://doi.org/10.1016/S0899-8256(05)80020-X</a>
</div>
<div id="ref-sarfati_self-organization_2021" class="csl-entry" role="listitem">
Sarfati, R., Hayes, J. C., &amp; Peleg, O. (2021). Self-organization in natural swarms of <span>Photinus</span> carolinus synchronous fireflies. <em>Science Advances</em>, <em>7</em>(28), eabg9259. <a href="https://doi.org/10.1126/sciadv.abg9259">https://doi.org/10.1126/sciadv.abg9259</a>
</div>
<div id="ref-SchultzEtAl1997" class="csl-entry" role="listitem">
Schultz, W., Dayan, P., &amp; Montague, P. R. (1997). A <span>Neural Substrate</span> of <span>Prediction</span> and <span>Reward</span>. <em>Science</em>, <em>275</em>(5306), 1593–1599. <a href="https://doi.org/10.1126/science.275.5306.1593">https://doi.org/10.1126/science.275.5306.1593</a>
</div>
<div id="ref-ShohamEtAl2007" class="csl-entry" role="listitem">
Shoham, Y., Powers, R., &amp; Grenager, T. (2007). If multi-agent learning is the answer, what is the question? <em>Artificial Intelligence</em>, <em>171</em>(7), 365–377. <a href="https://doi.org/10.1016/j.artint.2006.02.006">https://doi.org/10.1016/j.artint.2006.02.006</a>
</div>
<div id="ref-Sigmund2010" class="csl-entry" role="listitem">
Sigmund, K. (2010). The <span>Calculus</span> of <span>Selfishness</span>. In <em>The <span>Calculus</span> of <span>Selfishness</span></em>. <span>Princeton University Press</span>. <a href="https://doi.org/10.1515/9781400832255">https://doi.org/10.1515/9781400832255</a>
</div>
<div id="ref-SilverEtAl2016" class="csl-entry" role="listitem">
Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T., Leach, M., Kavukcuoglu, K., Graepel, T., &amp; Hassabis, D. (2016). Mastering the game of <span>Go</span> with deep neural networks and tree search. <em>Nature</em>, <em>529</em>(7587), 484–489. <a href="https://doi.org/10.1038/nature16961">https://doi.org/10.1038/nature16961</a>
</div>
<div id="ref-Skyrms2004" class="csl-entry" role="listitem">
Skyrms, B. (2004). <em>The <span>Stag Hunt</span> and the <span>Evolution</span> of <span>Social Structure</span></em>. <span>Cambridge University Press</span>.
</div>
<div id="ref-stone2010ad" class="csl-entry" role="listitem">
Stone, P., Kaminka, G., Kraus, S., &amp; Rosenschein, J. (2010). Ad hoc autonomous agent teams: Collaboration without pre-coordination. <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>, <em>24</em>, 1504–1509.
</div>
<div id="ref-DBLP:journals/corr/abs-2110-08176" class="csl-entry" role="listitem">
Strouse, D., McKee, K. R., Botvinick, M. M., Hughes, E., &amp; Everett, R. (2021). Collaborating with humans without human data. <em>CoRR</em>, <em>abs/2110.08176</em>. <a href="https://arxiv.org/abs/2110.08176">https://arxiv.org/abs/2110.08176</a>
</div>
<div id="ref-Sugden2004" class="csl-entry" role="listitem">
Sugden, R. (2004). <em>The <span>Economics</span> of <span>Rights</span>, <span class="nocase">Co-operation</span> and <span>Welfare</span></em>. <span>Springer</span>.
</div>
<div id="ref-SuttonBarto2018" class="csl-entry" role="listitem">
Sutton, R. S., &amp; Barto, A. G. (2018). <em>Reinforcement learning: An introduction</em> (Second edition). <span>The MIT Press</span>.
</div>
<div id="ref-culturalgeneralintelligenceteam2022learning" class="csl-entry" role="listitem">
Team, C. G. I., Bhoopchand, A., Brownfield, B., Collister, A., Lago, A. D., Edwards, A., Everett, R., Frechette, A., Oliveira, Y. G., Hughes, E., Mathewson, K. W., Mendolicchio, P., Pawar, J., Pislar, M., Platonov, A., Senter, E., Singh, S., Zacherl, A., &amp; Zhang, L. M. (2022). <em>Learning robust real-time cultural transmission without human data</em>. <a href="https://arxiv.org/abs/2203.00715">https://arxiv.org/abs/2203.00715</a>
</div>
<div id="ref-tekin_measuring_2017" class="csl-entry" role="listitem">
Tekin, E., Savage, V. M., &amp; Yeh, P. J. (2017). Measuring higher-order drug interactions: <span>A</span> review of recent approaches. <em>Current Opinion in Systems Biology</em>, <em>4</em>, 16–23. <a href="https://doi.org/10.1016/j.coisb.2017.05.015">https://doi.org/10.1016/j.coisb.2017.05.015</a>
</div>
<div id="ref-tekin_general_2018" class="csl-entry" role="listitem">
Tekin, E., Yeh, P. J., &amp; Savage, V. M. (2018). General <span>Form</span> for <span>Interaction</span> <span>Measures</span> and <span>Framework</span> for <span>Deriving</span> <span>Higher</span>-<span>Order</span> <span>Emergent</span> <span>Effects</span>. <em>Frontiers in Ecology and Evolution</em>, <em>6</em>. <a href="https://www.frontiersin.org/articles/10.3389/fevo.2018.00166">https://www.frontiersin.org/articles/10.3389/fevo.2018.00166</a>
</div>
<div id="ref-TuylsEtAl2019" class="csl-entry" role="listitem">
Tuyls, K., Perolat, J., Lanctot, M., Hughes, E., Everett, R., Leibo, J. Z., Szepesvári, C., &amp; Graepel, T. (2019). Bounds and dynamics for empirical game theoretic analysis. <em>Autonomous Agents and Multi-Agent Systems</em>, <em>34</em>(1), 7. <a href="https://doi.org/10.1007/s10458-019-09432-y">https://doi.org/10.1007/s10458-019-09432-y</a>
</div>
<div id="ref-VinyalsEtAl2019" class="csl-entry" role="listitem">
Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds, T., Georgiev, P., Oh, J., Horgan, D., Kroiss, M., Danihelka, I., Huang, A., Sifre, L., Cai, T., Agapiou, J. P., Jaderberg, M., … Silver, D. (2019). Grandmaster level in <span>StarCraft II</span> using multi-agent reinforcement learning. <em>Nature</em>, <em>575</em>(7782), 350–354. <a href="https://doi.org/10.1038/s41586-019-1724-z">https://doi.org/10.1038/s41586-019-1724-z</a>
</div>
<div id="ref-WangEtAl2021" class="csl-entry" role="listitem">
Wang, W. Z., Beliaev, M., Bıyık, E., Lazar, D. A., Pedarsani, R., &amp; Sadigh, D. (2021). Emergent <span>Prosociality</span> in <span>Multi-Agent Games Through Gifting</span>. <em>Twenty-<span>Ninth International Joint Conference</span> on <span>Artificial Intelligence</span></em>, <em>1</em>, 434–442. <a href="https://doi.org/10.24963/ijcai.2021/61">https://doi.org/10.24963/ijcai.2021/61</a>
</div>
<div id="ref-WangFu2020" class="csl-entry" role="listitem">
Wang, X., &amp; Fu, F. (2020). Eco-evolutionary dynamics with environmental feedback: <span>Cooperation</span> in a changing world. <em>Europhysics Letters</em>, <em>132</em>(1), 10001. <a href="https://doi.org/10.1209/0295-5075/132/10001">https://doi.org/10.1209/0295-5075/132/10001</a>
</div>
<div id="ref-wolframcellular1994" class="csl-entry" role="listitem">
Wolfram, S. (1994). <em>Cellular <span>Automata</span> <span>And</span> <span>Complexity</span>: <span>Collected</span> <span>Papers</span></em> (1st edition). Westview Press.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./index.html" class="pagination-link  aria-label=" &lt;span="">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./03_Framework.html" class="pagination-link" aria-label="<span class='chapter-number'>3</span>&nbsp; <span class='chapter-title'>Framework</span>">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Framework</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>