[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Supplementary Information for Collective Cooperative Intelligence",
    "section": "",
    "text": "1 Introduction\nCollective cooperation – in which intelligent actors in complex environments seek ways to improve their joint well-being – is critical for a sustainable future, yet unresolved. Mathematical models are essential for moving forward with this challenge. Our perspective paper argues that building bridges between CSS and MARL offers a more robust understanding of the drivers, mechanisms, and dynamics of collective cooperation from intelligent actors in dynamic environments. Both fields complement each other in their goals, methods, and scope.\nThis supplementary information presents a more detailed background on the literature (Chapter 2). Furthermore, we give all the details regarding the collective reinforcement learning dynamics we employ (Chapter 3) and how we applied it to create all complex phenomena presented in the main text (Chapter 4 - Chapter 7). Chapter 8 contains all required simulation scripts.\n\nReproducibility\nThis supplementary information was created in a fully reproducible writing and computing environment with the help of nbdev and quarto. If you are reading the PDF or web version of this document, you can find the source code in form of Jupyter notebooks at https://github.com/wbarfuss/collective-cooperative-intelligence.\nTo reproduce all simulations, create a new conda environment with the provided pythonenvironment.yml file.\nconda env create -f pythonenvironment.yml\nThis installs also the Collective Reinforcement Learning Dynamics in Python. They are provided by a separate Python package, which is in its early stages of development.\nYou activate the environment with:\nconda activate cocoin\nAfterwards, you should be able to follow along and execute all notebooks.\nIf you have any feedback, questions or problems with code, please do not hesitate to open a Github issue here: https://github.com/wbarfuss/collective-cooperative-intelligence/issues.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02_LiteratureBackground.html",
    "href": "02_LiteratureBackground.html",
    "title": "2  Literature background",
    "section": "",
    "text": "Complex Systems Science (CSS)\nComplex systems are generally out of equilibrium with many interacting components, feedback, and couplings between components and levels (e.g., Levin, 2002). Emergent, collective behavior at the macro level is surprising and hard to predict. An extensive repertoire of strategies and interaction types at the component level makes for multiple emergent patterns and functions at the macroscale–the state of the art to date largely treats each pattern independently. Consequently, there is little understanding of the overall degree of collectivity (Daniels et al., 2021). Power law and heavy-tailed distributions can lead to consequential ‘black swan’ and second and third-order effects (De Marzo et al., 2022). Some complex systems sit near a critical point at which small perturbations can cause a phase transition or reconfiguration because of long-range correlations (Mora & Bialek, 2011), including in finite, relatively small systems like many animal societies and human groups (Daniels et al., 2017).\nCSS methods are diverse. Those most relevant to understanding emergence of cooperative collectives include nonlinear dynamics to study temporal oscillations and couplings in time such as how individuals synchronize their activities (Sarfati et al., 2021), stochastic differential equations to study how, for example, noise influences transitions between disordered and ordered states (Jhawar et al., 2020), approaches from statistical mechanics to study collective behavior in space, such as how swarms (Buhl et al., 2006) and flocks choose trajectories (Bialek et al., 2012), game theory (Hofbauer & Sigmund, 1998; Nowak, 2006), network theory (Newman, 2003) to quantify interaction structure and how for example individuals make decisions under the influence of others in uncertain environments (Kleshnina et al., 2023), cellular automata (Wolfram, 1994) to gain insight from toy models about the relationship between rule complexity and pattern formation, agent based modeling (Epstein & Axtell, 1996), the physics of information to identify the mechanisms supporting information processing and quantify their efficiency with the goal of understanding how energy and information processing interact to shape collective effects (Kempes et al., 2017), and information theory to identify and quantify the contribution of higher order interactions to macroscale effects (Rosas et al., 2019; Tekin et al., 2017, 2018), quantify overall degree of collectivity (Daniels et al., 2016), and to build unifying frameworks leveraging ideas from predictive coding (Darriba & Waszak, 2018; Friston, 2018; Rao & Ballard, 1999), active inference and the free-energy principle (Buckley et al., 2017), and the information theory of individuality (D. Krakauer et al., 2020) for formalizing the role of uncertainty reduction (also called surprise minimization (Heins et al., 2023)) in micro-macro relationships and entity formation and evolution.\nWe expect information theoretic approaches emphasizing uncertainty reduction will be particularly productive for informing the development of a ‘strategic statistical mechanics’ that brings together powerful probabilistic approaches from statistical physics and information theory for deriving micro-macro maps with logical principles from theoretical computer science and the study of inference to capture robust and optimal design of how strategies interact in social circuits to support cooperation at scale. For example, a recent paper on surprise minimization (Heins et al., 2023) makes substantial progress in this direction. The authors develop a modeling framework to capture spatial collective behavior with inference-capable agents. The agents can estimate hidden causes of their sensations and adapt their position in space to minimize surprise (prediction error). The authors then study the relationship between individual inference and the emergence of collective states like cohesion and milling. Next steps include 1) exploring the pros and cons of active inference vs. MARL for encoding cognition into agents, 2) combining this approach with inductive game theory (probabilistic strategies are empirically grounded and extracted from time series) (DeDeo et al., 2010; D. C. Krakauer et al., 2010), and 3) studying how collective states and their transitions are computed from the social circuits (Brush et al., 2018; DeDeo et al., 2010; Ramos-Fernandez et al., 2020) that form as strategies are updated and consolidate into slow variables, which reduce social uncertainty and permit accelerated rates of adaptation (Flack, 2017). With these extensions, it should be possible to begin to deduce the organizational and algorithmic principles underlying the emergence of micro-macro maps in collective information processing systems through feed-forward effects and downward causation. These principles will likely inform the conditions under which collective, cooperative intelligence emerges at scale.\nIt is worth noting that in the models developed in the surprise minimization, inductive game theory, and some of the collective computation work described above, individuals are tracked, making these models, in a sense, agent-based models. However, these approaches distinguish themselves by incorporating agents that model the world in a restricted but cognitively principled manner or by parameterizing the models using probabilistic strategies obtained directly from data and by leveraging the rigor of powerful probabilistic modeling frameworks in statistical physics or dynamical systems traditions. In the more conventional agent-based modeling community, there have also been attempts to develop a more rigorous axiomatic approach, e.g., based on symmetries and bifurcation theory (Franci et al., 2022; Park et al., 2021) and game theory and control (Marden & Shamma, 2018).\nWork within the game theory and cultural evolution CSS sub-communities has made strides in understanding the social and cultural dynamics resulting from interacting boundedly rational agents with a finite computational budget. This work focuses on social and cultural learning mechanisms that allow agents to improve their behavior over time (Arthur, 1994, 2014; Holland & Miller, 1991). Game theoretic models in this tradition aim to explain emerging cooperation from simple yet plausible mechanisms. For example, the famous strategy tit-for-tat, which merely reciprocates what the opponent did in the previous turn, is surprisingly successful against much more complicated strategies (Axelrod & Hamilton, 1981). Its success can be attributed to its ability to control payoffs, ensuring that it receives the same score as the opponent, regardless of the complexity of the opponent’s strategy. The “zero-determinant” strategies later discovered by Press & Dyson (2012) provide a vast generalization of this phenomenon, allowing for extortion and generosity, in addition to more equitable relationships like that of tit-for-tat. These strategies have also encouraged a more geometric view of behavior in CSS (Hilbe, Chatterjee, et al., 2018), moving away from purely mechanistic descriptions.\nAn unsatisfactory facet of many of these game theoretic and cultural and social evolution models is that “cooperation” is based on an atomic action with the property that more cooperation translates to better social welfare. This interchangeability is likely part of the reason for the widespread focus on mechanisms for increasing the level of cooperation within a system. However, even for the most basic model of a conflict of interest, the repeated Prisoner’s Dilemma, it can be the case that high levels of “cooperation” are suboptimal for individuals and the collective, e.g., when agents are better off alternating “cooperation” and “defection” over time (relative to always cooperating) (McAvoy et al., 2022). Along these lines, nonlinearities produce counter-intuitive or hard-to-predict dynamics, meaning that it is essential to consider not only the level of cooperation but also the specific collective states or social outcomes that emerge from alternative strategic configurations and game structures. With their simplifying assumptions, these approaches are suitable for gaining insight into null expectations for baseline conditions but are more limited in utility when tackling cooperation at scale in complex environments composed of cognitively complex, error-prone agents (McNamara, 2013).\nHumans routinely deviate from the behavior predicted by the economic model of Homo economicus (Camerer, 2011). Yet, they are also more sophisticated than assumed in many simple evolutionary game theory models. They are capable of foresight, have a theory of mind, make inferences about their environment, and can adapt their behavior correspondingly. For example, in the most common evolutionary game theory models, individuals from a large population are randomly matched with other population members to play a static game. Those individuals who are more successful (because they employ better strategies) are more likely to be imitated. Imitation-based models are most appropriate when interactions are symmetric in the sense that individuals coincide in their feasible actions and payoffs. However, the paradigm is more challenging to motivate among heterogeneous and diverse actors and when behaviors cannot be observed directly and must be inferred before they are imitated. Moreover, extending this paradigm to account for other forms of cognition that intelligent individuals typically employ when revising their strategies is not straightforward. Finally, the act of imitation itself may be learned and, therefore, subject to cognitive constraints and learning dynamics (Team et al., 2022).\nThere is a clear interest in adapting game theoretic and cultural evolution models to accommodate these nuances (Hauser et al., 2019; Hilbe, Šimsa, et al., 2018; McNamara et al., 2021; X. Wang & Fu, 2020). As with the uncertainty reduction and collective computation approaches discussed above, considering how MARL could inform such models has great potential to unleash novel ways of modeling complex systems to tackle the challenges of collective cooperation in more complex settings.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Literature background</span>"
    ]
  },
  {
    "objectID": "02_LiteratureBackground.html#multi-agent-reinforcement-learning-marl",
    "href": "02_LiteratureBackground.html#multi-agent-reinforcement-learning-marl",
    "title": "2  Literature background",
    "section": "Multi-Agent Reinforcement Learning (MARL)",
    "text": "Multi-Agent Reinforcement Learning (MARL)\nIn a typical MARL setting, each agent observes (part of) the current state of the environment, then takes an action, after which they observe (part of) the new state of the environment and are provided with a reward indicating how desirable the previous “state-action-state” transition was. Over time, the agents update their strategies (a mapping from observation histories to probability distributions over their action space) to increase the long-term amount of reward they receive (Busoniu et al., 2008). In this work, we employ a broad definition of reinforcement learning (RL), including various individual-based update mechanisms. However, we exclude strategy update processes based on social reward comparisons, such as typical evolution models and explicit social learning. Eventually, we are interested in how processes, such as social learning, opinion formation, and collective action, can emerge from individual learning agents.\nModern MARL is inspired by work in several fields, including neuroscience, psychology, economics, and machine learning (Bush & Mosteller, 1951; Cross, 1973; Dayan & Niv, 2008; Erev & Roth, 1998; Fudenberg & Levine, 1998; Roth & Erev, 1995; Sutton & Barto, 2018). For example, the commonly used idea of temporal-difference learning is based upon reward-prediction errors, common to humans, other animals, and machines (Botvinick et al., 2020; Gunawardena, 2022; Schultz et al., 1997). In recent years, these traditional ideas have been combined with advances in machine learning – in particular, deep learning – to produce spectacular successes in various domains (Berner et al., 2019; Silver et al., 2016; Vinyals et al., 2019).\nStudies of cooperation in MARL fall under the umbrella of Cooperative AI (Dafoe et al., 2021). They can be divided based on whether the underlying game is fully cooperative (i.e., where all agents share the same goal) or mixed-sum (as opposed to zero-sum, which describes fully competitive situations). MARL as a field does not have a unique goal (Shoham et al., 2007). For example, some works aim to obtain game-theoretic equilibria via MARL, while others ask which learning rules are in equilibrium with one another in a specific environment. Despite this variety, the overarching aim of Cooperative AI is to improve the cooperative capabilities of AI systems, increasing joint welfare by prescribing how agents should (learn to) act. Such learning algorithms should ideally generalize to novel situations and scale to high-dimensional environments. A vital advantage of the MARL paradigm is that it can easily accommodate heterogeneous actors. Extending machine learning interpretability techniques to MARL is an ongoing effort to advance the understanding of MARL systems (Grupen et al., 2022; Lovering et al., 2022; McGrath et al., 2022).\nMethodologically, the focus often lies in designing novel algorithmic features to improve the cooperativeness of RL algorithms in large-scale environments. For example, algorithms may be equipped with abilities, such as sending each other messages (Foerster et al., 2016), making commitments (Christoffersen et al., 2022; Hughes et al., 2020), or transferring rewards to others (Lupu & Precup, 2020; W. Z. Wang et al., 2021). Algorithms are evaluated for their ability to produce agents and multi-agent systems that can generalize, i.e., perform well under conditions they never saw during training, such as situations where they must interact with unfamiliar AI social partners (Leibo et al., 2021; Stone et al., 2010) or humans (Carroll et al., 2019; (FAIR)† et al., 2022; Strouse et al., 2021). Measuring generalization to a fixed set of test scenarios allows researchers to compare the performance of MARL algorithms to one another despite incompatibilities in their training. In contrast to CSS studies, cooperation is typically not an available action to choose from. Instead, implementing a cooperative strategy must be learned from scratch (Leibo et al., 2017), and performance is measured by total social welfare.\nHowever, MARL simulation studies on their own are challenging to use to obtain analytically reliable insights into how collective cooperation emerges from complex human and machine behavior in dynamic environments. They often require significant computational resources, while the space to explore suffers from the curse of dimensionality. Moreover, they are typically highly stochastic, and results can be difficult to interpret (Hernandez-Leal et al., 2019). We believe that a unified approach that combines approaches from CSS and MARL could fill this gap.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Literature background</span>"
    ]
  },
  {
    "objectID": "02_LiteratureBackground.html#examplary-works-on-the-learning-dynamics-of-cooperation",
    "href": "02_LiteratureBackground.html#examplary-works-on-the-learning-dynamics-of-cooperation",
    "title": "2  Literature background",
    "section": "Examplary works on the learning dynamics of cooperation",
    "text": "Examplary works on the learning dynamics of cooperation\nThe study of cooperation has not been at the center of Collective Reinforcement Learning Dynamics (CRLD) studies. Here we list some notable examples from mathematical biology and sociology.\n\nL. Panait, K. Tuyls, S. Luke, Theoretical advantages of lenient learners: An evolutionary game theoretic perspective. J. Mach. Learn. Res. 9, 423–457 (2008).\nS. S. Izquierdo, L. R. Izquierdo, N. M. Gotts, Reinforcement learning dynamics in social dilemmas. J. Artif. Soc. Soc. Simul. 11, 1 (2008).\nM. Wunder, M. L. Littman, M. Babes, Classes of multiagent Q-learning dynamics with epsilon-greedy exploration in ICML (2010).\nN. Masuda, M. Nakamura, Numerical analysis of a reinforcement learning model with the dynamic aspiration level in the iterated Prisoner’s dilemma. J. Theor. Biol. 278, 55–62 (2011).\nS. Tanabe, N. Masuda, Evolution of cooperation facilitated by reinforcement learning with adaptive aspiration levels. J. Theor. Biol. 293, 151–160 (2012).\nT. Ezaki, Y. Horita, M. Takezawa, N. Masuda, Reinforcement learning explains conditional cooperation and its moody cousin. PLoS Comput. Biol. 12, e1005034 (2016).\nS. Dridi, E. Akçay, Learning to cooperate: The evolution of social rewards in repeated interactions. Am. Nat. 191, 58–73 (2018).\nO. Leimar, J. M. McNamara, Learning leads to bounded rationality and the evolution of cognitive bias in public goods games. Sci. Rep. 9, 16319 (2019).\nW. Barfuss, J. F. Donges, V. V. Vasconcelos, J. Kurths, S. A. Levin, Caring for the future can turn tragedy into comedy for long-term collective action under risk of collapse. Proc. Natl. Acad. Sci. U.S.A. 117, 12915– 12922 (2020).\nW. Z. Wang et al., “Emergent prosociality in multi-agent games through gifting” in Twenty-Ninth International Joint Conference on Artificial Intelligence (2021), vol. 1, pp. 434–442.\nL. Wang et al., Lévy noise promotes cooperation in the Prisoner’s dilemma game with reinforcement learning. Nonlinear Dyn. 108, 1837–1845 (2022).\nW. Barfuss, J. M. Meylahn, Intrinsic fluctuations of reinforcement learning promote cooperation. Sci. Rep. 13, 1309 (2023).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Literature background</span>"
    ]
  },
  {
    "objectID": "02_LiteratureBackground.html#sec-note",
    "href": "02_LiteratureBackground.html#sec-note",
    "title": "2  Literature background",
    "section": "On cooperation and social dilemmas",
    "text": "On cooperation and social dilemmas\nIn CSS, cooperation is frequently defined mechanistically. A cooperative act might involve colluding with a co-conspirator to remain quiet under interrogation (Poundstone, 2011), paying a cost to provide a benefit to another (e.g., measured in currency, time, or reproductive success) (Sigmund, 2010), or provisioning a public good or resource (Fehr & Gächter, 2000; Ostrom et al., 1992). Game theory allows such a behavior to be modeled using abstract payoffs. Dawes (1980) summarizes a social dilemma among \\(N\\) agents, each with two atomic actions, \\(C\\) (‘cooperate’) or \\(D\\) (‘defect’), as follows:\n\n(i) the payoff when all cooperate exceeds that when all defect and\n(ii) regardless of the composition of the group, a cooperator can always improve their own payoff by switching to defection.\n\nA simple example is a prisoner’s dilemma, which takes place in a collective of \\(N=2\\) agents. With payoffs defined by the matrix \\[\n\\begin{array}{c|cc}\n\\text{} & \\text{C} & \\text{D} \\\\\n\\hline\n\\text{C} & R & S \\\\\n\\text{D} & T & P \\\\\n\\end{array}\n\\tag{2.1}\\] a social dilemma requires \\(T&gt;R&gt;P&gt;S\\), which is the definition of a prisoner’s dilemma (Axelrod, 1984).\nCooperation becomes a graded quantity when a social dilemma is repeated, although it is still based on (atomic) cooperative actions in each round. As Leibo et al. (2017) note, what constitutes cooperation in spatially and/or temporally extended environments is more complicated and cannot determined using just reduction to a prisoner’s dilemma via empirical game-theoretic analysis (EGTA). EGTA is an approach to game theory that combines expert modeling with empirical data of gameplay. High-dimensional game models are reduced to so-called meta-games via a small set of heuristic strategies. The meta-game, or empirical game, is a simplified model of the high-dimensional game that is used to gain an improved qualitative understanding of the complex multi-agent interaction (Tuyls et al., 2019).\nAvoiding mechanistic considerations altogether, a useful way of thinking about cooperation is in terms of how a collective can jointly achieve higher payoffs, particularly when individual agents cannot force such outcomes. Suppose that the outcome \\(r^{\\ast}\\in\\mathbb{R}^{N}\\) is supported in Nash equilibrium. By the definition of a Nash equilibrium, no agent can improve its payoff through unilateral deviations in its policy. Therefore, if \\(r\\in\\mathbb{R}^{N}\\) is another outcome for which \\(r_{i}\\geqslant r_{i}^{\\ast}\\) for all \\(i=1,\\dots ,N\\), with at least one inequality strict, then no agent that would strictly benefit when the collective moves from \\(r^{\\ast}\\) to \\(r\\) can force this outcome, even though all agents would be at least as well off in \\(r\\) as in \\(r^{\\ast}\\). Doing so is said to require `cooperation’ (Cohen, 1998).\nThinking of cooperation in this way hearkens back the notion of a social dilemma. If \\(\\left(D,D\\right)\\) is a Nash equilibrium, then \\(P&gt;S\\). Neither \\(\\left(C,D\\right)\\) nor \\(\\left(D,C\\right)\\) can Pareto-dominate \\(\\left(D,D\\right)\\) due to this inequality, so for ‘cooperation’ to exist it must be the case that \\(R&gt;P\\). One possibility for the final payoff is that \\(T\\leqslant R\\), in which case \\(\\left(C,C\\right)\\) is also a Nash equilibrium. Such is the case in the stag hunt game (Skyrms, 2004). Although this situation describes an interaction in which social welfare can be improved via cooperation, it is not strictly a social dilemma by the definition we used above, because the incentives of the individuals and the pair are not opposing. Rather, it represents an equilibrium selection problem. If, \\(T&gt;R\\) instead, then \\(T&gt;R&gt;P&gt;S\\), the defining inequalities of a prisoner’s dilemma.\nImportantly, the Pareto-dominated outcome (\\(r^{\\ast}\\) above) need not be supported in Nash equilibrium in order to define a relevant notion of cooperation. Instead, one might impose the condition that there exists no sequence of unilateral, individually-rational deviations leading the outcome from \\(r^{\\ast}\\) to an outcome that Pareto-dominates \\(r^{\\ast}\\). For the game depicted in Equation 2.1, this condition allows \\(S\\geqslant P\\) as long as \\(T&gt;R\\). Such is the case in the snowdrift game (Sugden, 2004), in which two drivers are stuck on either side of a snowdrift blocking the road and must decide on who clears it. In contrast to the prisoner’s dilemma, a driver is still better off cooperating (clearing the snowdrift) even when the other driver does nothing. One would also prefer to have the co-player do all of the clearing than to collaborate. A simple example in which ‘cooperation’ does not exist–even under this relaxed definition–is the harmony game (Hauert, 2002), which satisfies \\(R&gt;T&gt;S&gt;P\\) and possesses the property that the unique Nash equilbrium, \\(\\left(C,C\\right)\\), is also Pareto-efficient.\nThe prisoner’s dilemma, and more generally the definition of Dawes (1980), characterize ‘strict’ social dilemmas. There are also ‘weaker’ social dilemmas describing conflicts of interest to lesser degrees. Again using the game in Equation 2.1, Hauert et al. (2006) stipulate that a weak social dilemma should satisfy\n\n(i) \\(R&gt;P\\);\n(ii) \\(T&gt;S\\); and\n(iii) \\(R&gt;S\\) and \\(T&gt;P\\).\n\nThe intuition behind these conditions is that (i) the payoff for mutual cooperation should exceed that of mutual defection; (ii) in mixed groups, the payoff to defectors should exceed that of cooperators; and (iii) regardless of what action a focal agent takes, they are better off when the co-player cooperates than when the co-player defects. The harmony game satisfies these inequalities, so it is considered a weak social dilemma despite the fact that it has no notion of ‘cooperation’ according to the definition of a ‘strict’ social dilemma. In addition to the prisoner’s dilemma and the harmony game, the remaining two weak social dilemmas are the snowdrift and stag hunt games. As one might expect, the behavior of a weak social dilemma in CSS depends on which of these classes of games it falls under (Hauert & Doebeli, 2004), rather than just the fact that it’s a weak social dilemma. \nHowever, even in strict social dilemmas, we caution that the presence of alternative actions can destabilize conflicts of interest. For example, suppose that in addition to the actions \\(C\\) and \\(D\\) in a prisoner’s dilemma, each player can take action \\(G\\), which is interpreted as avoiding the prisoner’s dilemma and instead collecting a pot of gold (at no cost). If both players have separate pots of gold available to collect and the value of this gold exceeds all of the prisoner’s dilemma payoffs, then the unique Nash equilibrium of this augmented game is \\(\\left(G,G\\right)\\), which is also Pareto-efficient. Like in the harmony game, there is no strict notion of ‘cooperation’ in the sense of Pareto dominance. Most importantly, there is no conflict of interest and thus no strict social dilemma. It is irrelevant that there are options \\(C\\) and \\(D\\) such that \\(T&gt;R&gt;P&gt;S\\); this ‘embedded’ game is merely a decoy. Only when the action \\(G\\) is unavailable or unknown would the agents view this interaction as a social dilemma. In this sense, social dilemmas need not be preserved upon inclusion into larger games. In this example, one can easily recognize the option \\(G\\) as trivializing the game, but in realistic applications, especially those involving EGTA, it might be entirely unclear whether there are true conflicts of interest. Intriguingly, the augmented game described above could still be considered a sequential social dilemma (Leibo et al., 2017), owing to the fact that the reference policies representing cooperation and defection can be chosen freely (and thus can represent policies in a smaller, embedded game).\nAlong these lines, the reduction to matrix games via EGTA could result in too much averaging with respect to social dilemmas. One might instead map a stochastic game not to a matrix game but to a down-sampled stochastic game with a smaller number of ‘salient’ states. A simple example would be when two agents interact in a grid world, with two colors distributed throughout the grid according to some distribution. The two players drift throughout the space via independent, unbiased random walks. When they appear on neighboring tiles, they play one of two matrix games, a prisoner’s dilemma or a harmony game, depending on whether the tiles have the same or different colors. There are then three relevant matrix games: the two played when on neighboring tiles and one ‘null’ game in which rewards are zero when neighbors on non-neighboring tiles. While one may view this game as having a large state space based on the agents’ positions on the grid, this scenario can also be modeled as a three-state game whose transitions are governed by a hidden Markov model (due to the structure of the grid and configuration of its colors). Nonetheless, by averaging appropriately, one might expect to obtain a useful approximation via a stochastic game with just three states. It is an open question whether further reduction to a matrix game would wash out important artifacts of this spatially-extended game.\nRegarding cooperation and the goals of CSS, one (rather misleading) aspect of CSS models that could be informed better by the goals of MARL is the fact that social welfare, even in strict social dilemmas, need not be a monotonic function of the level of cooperation. The goal should not always be ‘more cooperation’ in a mechanistic sense. In the prisoner’s dilemma, many studies in CSS make the simplifying assumption that \\(R&gt;\\left(S+T\\right) /2\\), which implies that a socially efficient outcome can be attained by full mutual cooperation. However, there are also prisoner’s dilemmas for which \\(R&lt;\\left(S+T\\right) /2\\), in which case both agents can do better by agreeing on a strategy of alternation: one agent cooperates in even time steps only, while the other cooperates in odd time steps only. Moving from the mutually cooperative outcome of \\(\\left(R, R\\right)\\) to the Pareto-dominant outcome of \\(\\left(\\left(S+T\\right) /2, \\left(S+T\\right) /2\\right)\\) requires ‘cooperation’, despite the fact that the latter involves a lower level of the atomic action ‘cooperate’ than the former. Thus, what constitutes a cooperative strategy in a temporally extended social dilemma might be decoupled from what constitutes a cooperative action in the underlying stage game, an observation that has not fully penetrated CSS (McAvoy et al., 2022) despite being understood in MARL (Leibo et al., 2017).\nIn summary, what constitutes ‘cooperation’ depends on the context. In both CSS and MARL, seemingly isolated systems of agents can involve externalities that affect how an interaction is characterized/understood. If a cooperative social dilemma is actually a zero-sum game among \\(N\\) players and the environment, with the environment getting depleted as the social welfare increases, then the ‘goals’ in such an environment are ambiguous. Agents might also transition between such states and those involving the possibility of true surpluses. Complicating matters further, agents could transition among states involving different numbers of agents, including those with only a single agent and the environment. In turn, an agent can reasonably have many different conceptions of what ‘cooperation’ means, even on short timescales. Rolling such ephemeral interactions into `cooperative strategies’ is only more complicated.\n\n\n\n\nArthur, W. B. (1994). Inductive reasoning and bounded rationality. The American Economic Review, 84(2), 406–411.\n\n\nArthur, W. B. (2014). Complexity and the Economy. Oxford University Press.\n\n\nAxelrod, R. (1984). The Evolution Of Cooperation. Basic Books.\n\n\nAxelrod, R., & Hamilton, W. D. (1981). The Evolution of Cooperation. Science, 211(4489), 1390–1396. https://doi.org/10.1126/science.7466396\n\n\nBerner, C., Brockman, G., Chan, B., Cheung, V., Dębiak, P., Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse, C., Józefowicz, R., Gray, S., Olsson, C., Pachocki, J., Petrov, M., Pinto, H. P. d. O., Raiman, J., Salimans, T., Schlatter, J., … Zhang, S. (2019). Dota 2 with Large Scale Deep Reinforcement Learning (arXiv:1912.06680). arXiv. https://doi.org/10.48550/arXiv.1912.06680\n\n\nBialek, W., Cavagna, A., Giardina, I., Mora, T., Silvestri, E., Viale, M., & Walczak, A. M. (2012). Statistical mechanics for natural flocks of birds. Proceedings of the National Academy of Sciences, 109(13), 4786–4791. https://doi.org/10.1073/pnas.1118633109\n\n\nBotvinick, M., Wang, J. X., Dabney, W., Miller, K. J., & Kurth-Nelson, Z. (2020). Deep Reinforcement Learning and Its Neuroscientific Implications. Neuron, 107(4), 603–616. https://doi.org/10.1016/j.neuron.2020.06.014\n\n\nBrush, E. R., Krakauer, D. C., & Flack, J. C. (2018). Conflicts of interest improve collective computation of adaptive social structures. Science Advances, 4(1), e1603311. https://doi.org/10.1126/sciadv.1603311\n\n\nBuckley, C. L., Kim, C. S., McGregor, S., & Seth, A. K. (2017). The free energy principle for action and perception: A mathematical review. Journal of Mathematical Psychology, 81, 55–79. https://doi.org/10.1016/j.jmp.2017.09.004\n\n\nBuhl, J., Sumpter, D. J. T., Couzin, I. D., Hale, J. J., Despland, E., Miller, E. R., & Simpson, S. J. (2006). From Disorder to Order in Marching Locusts. Science, 312(5778), 1402–1406. https://doi.org/10.1126/science.1125142\n\n\nBush, R. R., & Mosteller, F. (1951). A mathematical model for simple learning. Psychological Review, 58, 313–323. https://doi.org/10.1037/h0054388\n\n\nBusoniu, L., Babuska, R., & De Schutter, B. (2008). A comprehensive survey of multiagent reinforcement learning. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), 38(2), 156–172.\n\n\nCamerer, C. F. (2011). Behavioral game theory: Experiments in strategic interaction. Princeton university press.\n\n\nCarroll, M., Shah, R., Ho, M. K., Griffiths, T., Seshia, S., Abbeel, P., & Dragan, A. (2019). On the utility of learning about humans for human-ai coordination. Advances in Neural Information Processing Systems, 32.\n\n\nChristoffersen, P. J., Haupt, A. A., & Hadfield-Menell, D. (2022). Get it in writing: Formal contracts mitigate social dilemmas in multi-agent RL. arXiv Preprint arXiv:2208.10469.\n\n\nCohen, J. E. (1998). Cooperation and self-interest: Pareto-inefficiency of Nash equilibria in finite random games. Proceedings of the National Academy of Sciences, 95(17), 9724–9731. https://doi.org/10.1073/pnas.95.17.9724\n\n\nCross, J. G. (1973). A Stochastic Learning Model of Economic Behavior*. The Quarterly Journal of Economics, 87(2), 239–266. https://doi.org/10.2307/1882186\n\n\nDafoe, A., Bachrach, Y., Hadfield, G., Horvitz, E., Larson, K., & Graepel, T. (2021). Cooperative AI: Machines must learn to find common ground. Nature, 593(7857), 33–36. https://doi.org/10.1038/d41586-021-01170-0\n\n\nDaniels, B. C., Ellison, C. J., Krakauer, D. C., & Flack, J. C. (2016). Quantifying collectivity. Current Opinion in Neurobiology, 37, 106–113. https://doi.org/10.1016/j.conb.2016.01.012\n\n\nDaniels, B. C., Krakauer, D. C., & Flack, J. C. (2017). Control of finite critical behaviour in a small-scale social system. Nature Communications, 8(1), 14301. https://doi.org/10.1038/ncomms14301\n\n\nDaniels, B. C., Laubichler, M. D., & Flack, J. C. (2021). Introduction to the special issue: Quantifying collectivity. Theory in Biosciences, 140(4), 321–323. https://doi.org/10.1007/s12064-021-00358-2\n\n\nDarriba, Á., & Waszak, F. (2018). Predictions through evidence accumulation over time. Scientific Reports, 8(1), 494. https://doi.org/10.1038/s41598-017-18802-z\n\n\nDawes, R. M. (1980). Social Dilemmas. Annual Review of Psychology, 31(1), 169–193. https://doi.org/10.1146/annurev.ps.31.020180.001125\n\n\nDayan, P., & Niv, Y. (2008). Reinforcement learning: The Good, The Bad and The Ugly. Current Opinion in Neurobiology, 18(2), 185–196. https://doi.org/10.1016/j.conb.2008.08.003\n\n\nDe Marzo, G., Gabrielli, A., Zaccaria, A., & Pietronero, L. (2022). Quantifying the unexpected: A scientific approach to Black Swans. Physical Review Research, 4(3), 033079. https://doi.org/10.1103/PhysRevResearch.4.033079\n\n\nDeDeo, S., Krakauer, D. C., & Flack, J. C. (2010). Inductive game theory and the dynamics of animal conflict. PLoS Computational Biology, 6(5), e1000782.\n\n\nEpstein, J. M., & Axtell, R. L. (1996). Growing Artificial Societies: Social Science From the Bottom Up (First Edition). Brookings Institution Press.\n\n\nErev, I., & Roth, A. E. (1998). Predicting How People Play Games: Reinforcement Learning in Experimental Games with Unique, Mixed Strategy Equilibria. The American Economic Review, 88(4), 848–881.\n\n\n(FAIR)†, M. F. A. R. D. T., Bakhtin, A., Brown, N., Dinan, E., Farina, G., Flaherty, C., Fried, D., Goff, A., Gray, J., Hu, H., et al. (2022). Human-level play in the game of diplomacy by combining language models with strategic reasoning. Science, 378(6624), 1067–1074.\n\n\nFehr, E., & Gächter, S. (2000). Cooperation and Punishment in Public Goods Experiments. American Economic Review, 90(4), 980–994. https://doi.org/10.1257/aer.90.4.980\n\n\nFlack, J. C. (2017). Coarse-graining as a downward causation mechanism. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 375(2109), 20160338. https://doi.org/10.1098/rsta.2016.0338\n\n\nFoerster, J., Assael, I. A., de Freitas, N., & Whiteson, S. (2016). Learning to Communicate with Deep Multi-Agent Reinforcement Learning. Advances in Neural Information Processing Systems, 29.\n\n\nFranci, A., Golubitsky, M., Stewart, I., Bizyaeva, A., & Leonard, N. E. (2022). Breaking indecision in multi-agent, multi-option dynamics. arXiv Preprint arXiv:2206.14893.\n\n\nFriston, K. (2018). Does predictive coding have a future? Nature Neuroscience, 21(8), 1019–1021. https://doi.org/10.1038/s41593-018-0200-7\n\n\nFudenberg, D., & Levine, D. K. (1998). The Theory of Learning in Games (K. Binmore, Ed.). MIT Press.\n\n\nGrupen, N., Jaques, N., Kim, B., & Omidshafiei, S. (2022). Concept-based understanding of emergent multi-agent behavior. Deep Reinforcement Learning Workshop NeurIPS 2022. https://openreview.net/forum?id=zt5JpGQ8WhH\n\n\nGunawardena, J. (2022). Learning Outside the Brain: Integrating Cognitive Science and Systems Biology. Proceedings of the IEEE, 1–23. https://doi.org/10.1109/JPROC.2022.3162791\n\n\nHauert, C. (2002). Effects of space in 2 2 games. International Journal of Bifurcation and Chaos, 12(07), 1531–1548. https://doi.org/10.1142/S0218127402005273\n\n\nHauert, C., & Doebeli, M. (2004). Spatial structure often inhibits the evolution of cooperation in the snowdrift game. Nature, 428(6983), 643–646. https://doi.org/10.1038/nature02360\n\n\nHauert, C., Michor, F., Nowak, M. A., & Doebeli, M. (2006). Synergy and discounting of cooperation in social dilemmas. Journal of Theoretical Biology, 239(2), 195–202. https://doi.org/10.1016/j.jtbi.2005.08.040\n\n\nHauser, O. P., Hilbe, C., Chatterjee, K., & Nowak, M. A. (2019). Social dilemmas among unequals. Nature, 572(7770), 524–527.\n\n\nHeins, C., Millidge, B., Costa, L. da, Mann, R., Friston, K., & Couzin, I. (2023). Collective behavior from surprise minimization. arXiv. http://arxiv.org/abs/2307.14804\n\n\nHernandez-Leal, P., Kartal, B., & Taylor, M. E. (2019). A survey and critique of multiagent deep reinforcement learning. Autonomous Agents and Multi-Agent Systems, 33(6), 750–797. https://doi.org/10.1007/s10458-019-09421-1\n\n\nHilbe, C., Chatterjee, K., & Nowak, M. A. (2018). Partners and rivals in direct reciprocity. Nature Human Behaviour. https://doi.org/10.1038/s41562-018-0320-9\n\n\nHilbe, C., Šimsa, Š., Chatterjee, K., & Nowak, M. A. (2018). Evolution of cooperation in stochastic games. Nature, 559(7713), 246–249. https://doi.org/10.1038/s41586-018-0277-x\n\n\nHofbauer, J., & Sigmund, K. (1998). Evolutionary Games and Population Dynamics (First). Cambridge University Press. https://doi.org/10.1017/CBO9781139173179\n\n\nHolland, J. H., & Miller, J. H. (1991). Artificial Adaptive Agents in Economic Theory. The American Economic Review, 81(2), 365–370.\n\n\nHughes, E., Anthony, T. W., Eccles, T., Leibo, J. Z., Balduzzi, D., & Bachrach, Y. (2020). Learning to Resolve Alliance Dilemmas in Many-Player Zero-Sum Games. New Zealand, 10.\n\n\nJhawar, J., Morris, R. G., Amith-Kumar, U. R., Danny Raj, M., Rogers, T., Rajendran, H., & Guttal, V. (2020). Noise-induced schooling of fish. Nature Physics, 16(4), 488–493. https://doi.org/10.1038/s41567-020-0787-y\n\n\nKempes, C. P., Wolpert, D., Cohen, Z., & Pérez-Mercader, J. (2017). The thermodynamic efficiency of computations made in cells across the range of life. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 375(2109), 20160343. https://doi.org/10.1098/rsta.2016.0343\n\n\nKleshnina, M., Hilbe, C., Šimsa, Š., Chatterjee, K., & Nowak, M. A. (2023). The effect of environmental information on evolution of cooperation in stochastic games. Nature Communications, 14(1), 4153. https://doi.org/10.1038/s41467-023-39625-9\n\n\nKrakauer, D. C., Flack, J. C., Dedeo, S., Farmer, D., & Rockmore, D. (2010). Intelligent Data Analysis of Intelligent Systems. In P. R. Cohen, N. M. Adams, & M. R. Berthold (Eds.), Advances in Intelligent Data Analysis IX (pp. 8–17). Springer. https://doi.org/10.1007/978-3-642-13062-5_3\n\n\nKrakauer, D., Bertschinger, N., Olbrich, E., Flack, J. C., & Ay, N. (2020). The information theory of individuality. Theory in Biosciences, 139(2), 209–223. https://doi.org/10.1007/s12064-020-00313-7\n\n\nLeibo, J. Z., Dueñez-Guzman, E. A., Vezhnevets, A., Agapiou, J. P., Sunehag, P., Koster, R., Matyas, J., Beattie, C., Mordatch, I., & Graepel, T. (2021). Scalable evaluation of multi-agent reinforcement learning with melting pot. International Conference on Machine Learning, 6187–6199.\n\n\nLeibo, J. Z., Zambaldi, V., Lanctot, M., Marecki, J., & Graepel, T. (2017). Multi-agent Reinforcement Learning in Sequential Social Dilemmas. Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems, 464–473.\n\n\nLevin, S. (2002). Complex adaptive systems: Exploring the known, the unknown and the unknowable. Bulletin of the American Mathematical Society, 40(1), 3–19. https://doi.org/10.1090/S0273-0979-02-00965-5\n\n\nLovering, C., Forde, J., Konidaris, G., Pavlick, E., & Littman, M. (2022). Evaluation beyond task performance: Analyzing concepts in AlphaZero in hex. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, & A. Oh (Eds.), Advances in neural information processing systems (Vol. 35, pp. 25992–26006). Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2022/file/a705747417d32ebf1916169e1a442274-Paper-Conference.pdf\n\n\nLupu, A., & Precup, D. (2020). Gifting in multi-agent reinforcement learning. Proceedings of the 19th International Conference on Autonomous Agents and Multiagent Systems, 789–797.\n\n\nMarden, J. R., & Shamma, J. S. (2018). Game theory and control. Annual Review of Control, Robotics, and Autonomous Systems, 1, 105–134.\n\n\nMcAvoy, A., Mori, Y., & Plotkin, J. B. (2022). Selfish optimization and collective learning in populations. Physica D: Nonlinear Phenomena, 439, 133426. https://doi.org/10.1016/j.physd.2022.133426\n\n\nMcGrath, T., Kapishnikov, A., Tomašev, N., Pearce, A., Wattenberg, M., Hassabis, D., Kim, B., Paquet, U., & Kramnik, V. (2022). Acquisition of chess knowledge in AlphaZero. Proceedings of the National Academy of Sciences, 119(47), e2206625119.\n\n\nMcNamara, J. M. (2013). Towards a richer evolutionary game theory. Journal of The Royal Society Interface, 10(88), 20130544. https://doi.org/10.1098/rsif.2013.0544\n\n\nMcNamara, J. M., Houston, A. I., & Leimar, O. (2021). Learning, exploitation and bias in games. PLOS ONE, 16(2), e0246588. https://doi.org/10.1371/journal.pone.0246588\n\n\nMora, T., & Bialek, W. (2011). Are Biological Systems Poised at Criticality? Journal of Statistical Physics, 144(2), 268–302. https://doi.org/10.1007/s10955-011-0229-4\n\n\nNewman, M. E. J. (2003). The Structure and Function of Complex Networks. SIAM Review, 45(2), 167–256. https://doi.org/10.1137/S003614450342480\n\n\nNowak, M. A. (2006). Evolutionary dynamics: Exploring the equations of life. Harvard university press.\n\n\nOstrom, E., Walker, J., & Gardner, R. (1992). Covenants with and without a Sword: Self-Governance Is Possible. American Political Science Review, 86(2), 404–417. https://doi.org/10.2307/1964229\n\n\nPark, S., Bizyaeva, A., Kawakatsu, M., Franci, A., & Leonard, N. E. (2021). Tuning cooperative behavior in games with nonlinear opinion dynamics. IEEE Control Systems Letters, 6, 2030–2035.\n\n\nPoundstone, W. (2011). Prisoner’s Dilemma. Knopf Doubleday Publishing Group.\n\n\nPress, W. H., & Dyson, F. J. (2012). Iterated prisoner’s dilemma contains strategies that dominate any evolutionary opponent. Proceedings of the National Academy of Sciences, 109(26), 10409–10413. https://doi.org/10.1073/pnas.1206569109 \n\n\nRamos-Fernandez, G., Smith Aguilar, S. E., Krakauer, D. C., & Flack, J. C. (2020). Collective Computation in Animal Fission-Fusion Dynamics. Frontiers in Robotics and AI, 7. https://www.frontiersin.org/article/10.3389/frobt.2020.00090\n\n\nRao, R. P. N., & Ballard, D. H. (1999). Predictive coding in the visual cortex: A functional interpretation of some extra-classical receptive-field effects. Nature Neuroscience, 2(1), 79–87. https://doi.org/10.1038/4580\n\n\nRosas, F. E., Mediano, P. A. M., Gastpar, M., & Jensen, H. J. (2019). Quantifying high-order interdependencies via multivariate extensions of the mutual information. Physical Review E, 100(3), 032305. https://doi.org/10.1103/PhysRevE.100.032305\n\n\nRoth, A. E., & Erev, I. (1995). Learning in extensive-form games: Experimental data and simple dynamic models in the intermediate term. Games and Economic Behavior, 8(1), 164–212. https://doi.org/10.1016/S0899-8256(05)80020-X\n\n\nSarfati, R., Hayes, J. C., & Peleg, O. (2021). Self-organization in natural swarms of Photinus carolinus synchronous fireflies. Science Advances, 7(28), eabg9259. https://doi.org/10.1126/sciadv.abg9259\n\n\nSchultz, W., Dayan, P., & Montague, P. R. (1997). A Neural Substrate of Prediction and Reward. Science, 275(5306), 1593–1599. https://doi.org/10.1126/science.275.5306.1593\n\n\nShoham, Y., Powers, R., & Grenager, T. (2007). If multi-agent learning is the answer, what is the question? Artificial Intelligence, 171(7), 365–377. https://doi.org/10.1016/j.artint.2006.02.006\n\n\nSigmund, K. (2010). The Calculus of Selfishness. In The Calculus of Selfishness. Princeton University Press. https://doi.org/10.1515/9781400832255\n\n\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T., Leach, M., Kavukcuoglu, K., Graepel, T., & Hassabis, D. (2016). Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587), 484–489. https://doi.org/10.1038/nature16961\n\n\nSkyrms, B. (2004). The Stag Hunt and the Evolution of Social Structure. Cambridge University Press.\n\n\nStone, P., Kaminka, G., Kraus, S., & Rosenschein, J. (2010). Ad hoc autonomous agent teams: Collaboration without pre-coordination. Proceedings of the AAAI Conference on Artificial Intelligence, 24, 1504–1509.\n\n\nStrouse, D., McKee, K. R., Botvinick, M. M., Hughes, E., & Everett, R. (2021). Collaborating with humans without human data. CoRR, abs/2110.08176. https://arxiv.org/abs/2110.08176\n\n\nSugden, R. (2004). The Economics of Rights, Co-operation and Welfare. Springer.\n\n\nSutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction (Second edition). The MIT Press.\n\n\nTeam, C. G. I., Bhoopchand, A., Brownfield, B., Collister, A., Lago, A. D., Edwards, A., Everett, R., Frechette, A., Oliveira, Y. G., Hughes, E., Mathewson, K. W., Mendolicchio, P., Pawar, J., Pislar, M., Platonov, A., Senter, E., Singh, S., Zacherl, A., & Zhang, L. M. (2022). Learning robust real-time cultural transmission without human data. https://arxiv.org/abs/2203.00715\n\n\nTekin, E., Savage, V. M., & Yeh, P. J. (2017). Measuring higher-order drug interactions: A review of recent approaches. Current Opinion in Systems Biology, 4, 16–23. https://doi.org/10.1016/j.coisb.2017.05.015\n\n\nTekin, E., Yeh, P. J., & Savage, V. M. (2018). General Form for Interaction Measures and Framework for Deriving Higher-Order Emergent Effects. Frontiers in Ecology and Evolution, 6. https://www.frontiersin.org/articles/10.3389/fevo.2018.00166\n\n\nTuyls, K., Perolat, J., Lanctot, M., Hughes, E., Everett, R., Leibo, J. Z., Szepesvári, C., & Graepel, T. (2019). Bounds and dynamics for empirical game theoretic analysis. Autonomous Agents and Multi-Agent Systems, 34(1), 7. https://doi.org/10.1007/s10458-019-09432-y\n\n\nVinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds, T., Georgiev, P., Oh, J., Horgan, D., Kroiss, M., Danihelka, I., Huang, A., Sifre, L., Cai, T., Agapiou, J. P., Jaderberg, M., … Silver, D. (2019). Grandmaster level in StarCraft II using multi-agent reinforcement learning. Nature, 575(7782), 350–354. https://doi.org/10.1038/s41586-019-1724-z\n\n\nWang, W. Z., Beliaev, M., Bıyık, E., Lazar, D. A., Pedarsani, R., & Sadigh, D. (2021). Emergent Prosociality in Multi-Agent Games Through Gifting. Twenty-Ninth International Joint Conference on Artificial Intelligence, 1, 434–442. https://doi.org/10.24963/ijcai.2021/61\n\n\nWang, X., & Fu, F. (2020). Eco-evolutionary dynamics with environmental feedback: Cooperation in a changing world. Europhysics Letters, 132(1), 10001. https://doi.org/10.1209/0295-5075/132/10001\n\n\nWolfram, S. (1994). Cellular Automata And Complexity: Collected Papers (1st edition). Westview Press.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Literature background</span>"
    ]
  },
  {
    "objectID": "03_Framework.html",
    "href": "03_Framework.html",
    "title": "3  Framework",
    "section": "",
    "text": "Multi-agent environment interface (MAEi)\nThe basis for the learning dynamics is the multi-agent environment interface (MAEi) (Figure 3.1), which itself is based in its most basic form on the formal framework of stochastic games, also known as Markov games (Littman, 1994), which consist of the elements \\(\\langle N, \\mathcal S, \\boldsymbol{\\mathcal A}, T, \\boldsymbol{R} \\rangle\\).\nIn an MAEi, \\(N \\in \\mathbb N\\) agents reside in an environment of \\(Z \\in \\mathbb N\\) states \\(\\mathcal S=(S_1, \\dots, S_Z)\\). In each state \\(s\\), each agent \\(i \\in \\{1,\\dots, N\\}\\) has a maximum of \\(M \\in \\mathbb N\\) available actions \\(\\mathcal A^i = (A^i_1,\\dots,A^i_M)\\) to choose from. \\(\\boldsymbol{\\mathcal A} = \\bigotimes_i \\mathcal A^i\\) is the joint-action set where \\(\\bigotimes_i\\) denotes the cartesian product over the sets indexed by \\(i\\). Agents choose their actions simultaneously. A joint action is denoted by \\(\\boldsymbol a = (a^1, \\dots, a^N) \\in \\boldsymbol{\\mathcal A}\\). With \\(\\boldsymbol a^{-i} = (a^1,\\dots,\\) \\(a^{i-1},\\) \\(a^{i+1},\\) \\(\\dots,\\) \\(a^N)\\) we denote the joint action except agent \\(i\\)’s, and we write the joint action in which agent \\(i\\) chooses \\(a^i\\) and all other agents choose \\(\\boldsymbol a^{-i}\\) as \\(a^i\\boldsymbol a^{-i}\\). We chose an equal number of actions for all states and agents out of notational convenience.\nThe transition function \\(T: \\mathcal S \\times \\boldsymbol{\\mathcal A} \\times \\mathcal S \\rightarrow [0, 1]\\) determines the probabilistic state change. \\(T(s, \\boldsymbol a, s')\\) is the transition probability from current state \\(s\\) to next state \\(s'\\) under joint action \\(\\boldsymbol a\\). Throughout this work, we restrict ourselves to ergodic environments without absorbing states.\nThe reward function \\(\\boldsymbol R: \\mathcal S \\times \\boldsymbol{\\mathcal A} \\times \\mathcal S \\rightarrow \\mathbb{R}^N\\) maps the triple of current state \\(s\\), joint action \\(\\boldsymbol a\\) and next state \\(s'\\) to an immediate reward scalar for each agent. \\(R^i(s,\\boldsymbol a,s')\\) is the reward agent \\(i\\) receives. Note that the reward function is often defined as depending only on the current state and joint action, \\(R^i(s, \\boldsymbol a)\\). Our formulation maps onto this variant by averaging out the transition probabilities towards the next state according to \\(R^i(s, \\boldsymbol a) = \\sum_{s'} T(s, \\boldsymbol a, s') R^i(s, \\boldsymbol a, s')\\).\nIn principle, agents could condition their probabilities of choosing action on the entire history of past play. However, doing so is not only cognitively demanding. It also requires that agents observe all other agents’ actions. Therefore, we focus our analysis on simple, so-called Markov strategies, with which agents choose their actions based only on the current state: \\(X^i : \\mathcal S^i \\times \\mathcal A^i \\rightarrow [0,1]\\). \\(X^i(s, a^i)\\) is the probability that agent \\(i\\) chooses action \\(a^i\\) given the environment is in state \\(s\\). We denote the joint strategy by \\(\\boldsymbol X = \\boldsymbol X(s, \\boldsymbol a) = \\bigotimes_i X^i(s, a^i) : {\\mathcal S} \\times \\mathbf{\\mathcal A} \\rightarrow [0,1]^N\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Framework</span>"
    ]
  },
  {
    "objectID": "03_Framework.html#multi-agent-environment-interface-maei",
    "href": "03_Framework.html#multi-agent-environment-interface-maei",
    "title": "3  Framework",
    "section": "",
    "text": "Figure 3.1: Multi-Agent Environment Interface (MAEi)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Framework</span>"
    ]
  },
  {
    "objectID": "03_Framework.html#ecological-tipping-environment",
    "href": "03_Framework.html#ecological-tipping-environment",
    "title": "3  Framework",
    "section": "Ecological Tipping Environment",
    "text": "Ecological Tipping Environment\nWe illustrate an application of the multi-agent environment interface by specifying a concrete environment that allows studying the prospects of collective action under environmental tipping elements (Barfuss et al., 2020).\n\n\n\n\n\n\nFigure 3.2: Ecological Tipping Environment\n\n\n\nIt is available in the Python package via:\n\nfrom pyCRLD.Environments.EcologicalPublicGood import EcologicalPublicGood as EPG\nenv = EPG(N=2, f=1.2, c=5, m=-5, qc=0.2, qr=0.01)\n\nThe environmental state set consists of two states, a prosperous and a degraded one, \\(\\mathcal S = \\{\\mathsf g, \\mathsf p\\}\\).\n\nenv.Sset\n\n['g', 'p']\n\n\nIn each state \\(s \\in \\mathcal S\\), each agent \\(i \\in \\{1, \\dots, N\\}\\) can choose from their action set between either cooperation or defection, \\(\\mathcal A^i=\\{\\mathsf c, \\mathsf d\\}\\).\n\nenv.Aset\n\n[['c', 'd'], ['c', 'd']]\n\n\nWe denote the number of cooperating (defecting) agents by \\(N_{\\mathsf c}\\) (\\(N_{\\mathsf d} = N - N_{\\mathsf c}\\)).\nA collapse from the prosperous state to the degraded state occurs with transition probability, \\[\nT(\\mathsf p, \\boldsymbol a, \\mathsf g) = \\frac{N_{\\mathsf d}}{N} q_{c},\n\\] with \\(q_c \\in [0, 1]\\) being the collapse leverage parameter, indicating how much impact a defecting agent exerts on the environment. Thus, the environment remains within the prosperous state with probability, \\(T(\\mathsf p, \\boldsymbol a, \\mathsf p) = 1 - T(\\mathsf p, \\boldsymbol a, \\mathsf g)\\).\nIn the degraded state, we set the recovery to occur with probability, \\[\nT(\\mathsf g, \\boldsymbol a, \\mathsf p) = q_{r},\n\\] independent of the agents’ actions. The parameter \\(q_r\\) sets the recovery probability, and the probability that the environment remains degraded is, thus, \\(T(\\mathsf g, \\boldsymbol a, \\mathsf g) = 1 - T(\\mathsf g, \\boldsymbol a, \\mathsf p)\\).\n\nenv.T.round(4)\n\narray([[[[0.99, 0.01],\n         [0.99, 0.01]],\n\n        [[0.99, 0.01],\n         [0.99, 0.01]]],\n\n\n       [[[0.  , 1.  ],\n         [0.1 , 0.9 ]],\n\n        [[0.1 , 0.9 ],\n         [0.2 , 0.8 ]]]])\n\n\nRewards in the prosperous state follow the standard public good game, \\[\nR^i(\\mathsf p, a^i{\\boldsymbol a}^{-i}, \\mathsf p) =\n\\left\\{\n\\begin{array}{ll}\nfc {N_c \\over N} - c& \\text{if } a^i = \\mathsf c \\\\\nfc {N_c \\over N} & \\text{if } a^i = \\mathsf d\n\\end{array}\n\\right.\n\\] where \\(c\\) denotes the cost of cooperation and \\(f\\), the cooperation synergy factor.\n\nenv.R[0, 1, :, :, 1]\n\narray([[ 1., -2.],\n       [ 3.,  0.]])\n\n\n\nenv.R[1, 1, :, :, 1]\n\narray([[ 1.,  3.],\n       [-2.,  0.]])\n\n\nHowever, when a state transition involves the degraded state, \\(\\mathsf g\\), the agents receive an environmental collapse impact, \\(m &lt; 0\\), \\[\nR^i(\\mathsf p, {\\boldsymbol a}, \\mathsf g) = R^i(\\mathsf g, {\\boldsymbol a}, \\mathsf p) = R^i(\\mathsf g, {\\boldsymbol a}, \\mathsf g) = m, \\quad \\text{for all } \\boldsymbol a, i.\n\\]\nFor illustration purposes, we set the model’s parameters as \\(N=2, f=1.2, c=5, m=-5, q_c=0.2\\), and \\(qr=0.01\\):\n\nenv = EPG(N=2, f=1.2, c=5, m=-5, qc=0.2, qr=0.01)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Framework</span>"
    ]
  },
  {
    "objectID": "03_Framework.html#sec-RL",
    "href": "03_Framework.html#sec-RL",
    "title": "3  Framework",
    "section": "Reinforcement learning",
    "text": "Reinforcement learning\nLearning helps agents adjust their behavior to changes in their environment, both from other agents and external factors. This is essential when the future is unpredictable, unknown, and complex, and thus, detailed pre-planning is doomed to failure.\nIn particular, reinforcement learning is a trial-and-error method of mapping situations to actions to maximize a numerical reward signal (Sutton & Barto, 2018). When rewards are a delayed consequence of current actions, so-called temporal-difference or reward-prediction learning has been particularly influential (Sutton, 1988). This type of learning summarizes the difference between value estimates from past and present experiences into a reward-prediction error, which is then used to adapt the current behavior to gain more rewards over time. There also exist remarkable similarities between computational reinforcement learning and the results of neuroscientific experiments (Dayan & Niv, 2008). Dopamine conveys reward-prediction errors to brain structures where learning and decision-making occur (Schultz et al., 1997). This dopamine reward-prediction error signal constitutes a potential neuronal substrate for the essential economic decision quantity of utility (Schultz et al., 2017).\nIn the following, we present the essential elements of the reinforcement learning update.\n\nGain\nWe assume that at each time step \\(t\\), each agent \\(i\\) strives to maximize its exponentially discounted sum of future rewards,\n\\[\nG^i_t = \\mathsf N^i \\sum_{k=0}^\\infty (\\gamma^i)^k r^i_{t+k},\n\\tag{3.1}\\]\nwhere \\(r^i(t+k)\\) is the reward agent \\(i\\) receives at time step \\(t+k\\), and \\(\\gamma^i \\in [0,1)\\) is the discount factor of agent \\(i.\\) The discount factor regulates how much an agent cares for future rewards, where \\(\\gamma^i\\) close to \\(1\\) means that it cares for the future almost as much for the present and \\(\\gamma^i\\) close to \\(0\\) means that it cares almost only for immediate rewards. \\(\\mathsf N^i\\) denotes a normalization constant. It is either \\(1\\), or \\((1-\\gamma^i).\\) While machine learning researchers often use \\(\\mathsf N^i=1\\), the pre-factor \\(N^i=(1-\\gamma^i)\\) has the advantage of normalizing the gains, \\(G^i(t)\\), to be on the same numerical scale as the rewards.\n\n\nValue functions\nGiven a joint strategy \\(\\boldsymbol X\\), we define the state values, \\(V^i_{\\boldsymbol X}(s)\\), as the expected gain, \\(G^i(t)\\), when starting in state \\(s\\) and then following the joint strategy, \\(\\boldsymbol X\\),\n\\[\nV^i_{\\boldsymbol X}(s) = \\mathbb E_{\\boldsymbol X} \\big[G^i_t | s_t = s \\big].\n\\tag{3.2}\\]\nAnalogously, we define the state-action values, \\(Q^i_{\\boldsymbol X}(s, a)\\), as the expected gain, \\(G^i(t)\\), when starting in state \\(s\\), executing action \\(a\\), and then following the joint strategy, \\(\\boldsymbol X\\),\n\\[\nQ^i_{\\boldsymbol X}(s, a) = \\mathbb E_{\\boldsymbol X} \\big[G^i_t | s_t = s, a^i_t=a \\big].\n\\tag{3.3}\\]\nFrom Equation 3.1 and Equation 3.2, we can obtain the famous Bellman equation as follows, denoting the next state as \\(s'\\),\n\\[\\begin{align}\nV^i_{\\boldsymbol X}(s) &= \\mathbb E_{\\boldsymbol X} \\big[G^i_t | s_t = s \\big] \\\\\n&= \\mathbb E_{\\boldsymbol X} \\Big[\\mathsf N^i \\sum_{k=0}^\\infty (\\gamma^i)^k r^i_{t+k} | s_t = s \\Big] \\\\\n&= \\mathbb E_{\\boldsymbol X} \\Big[\\mathsf N^i r^i_t + \\mathsf N^i \\gamma^i \\sum_{k=0}^\\infty (\\gamma^i)^{k} r^i_{t+1+k} | s_t = s \\Big] \\\\\n&= \\mathbb E_{\\boldsymbol X} \\Big[\\mathsf N^i r^i_t + \\gamma^i V^i_{\\boldsymbol X}(s') | s_t = s\\Big] \\\\\n&= \\mathbb E_{\\boldsymbol X} \\Big[\\mathsf N^i R^i(s,\\boldsymbol a, s') + \\gamma^i V^i_{\\boldsymbol X}(s') | s_t = s\\Big].\n\\end{align}\\]\nAnalogously, we can write for the state-action values, \\[\nQ^i_{\\boldsymbol X}(s, a) = \\mathbb E_{\\boldsymbol X} \\Big[\\mathsf N^i R^i(s,\\boldsymbol a, s') + \\gamma^i V^i_{\\boldsymbol X}(s') | s_t = s, a^i_t=a\\Big].\n\\tag{3.4}\\]\nThus, the value function can be expressed via a recursive relationship. The value of a state equals the discounted value of the next state (\\(\\gamma^i V^i_{\\boldsymbol X}(s')\\)) plus the reward the agent receives along the way, properly normalized (\\(N^i R^i(s,\\boldsymbol a, s')\\)). This recursion will come in useful for learning (see Section 3.3.4).\n\n\nStrategy function\nIn general, reinforcement learning agents do not know the true state and state-action values, \\(V^i_{\\boldsymbol X}(s)\\), and \\(Q^i_{\\boldsymbol X}(s, a)\\). Instead, they hold variable beliefs about the quality of each available action in each state \\(Q^i_t(s, a)\\). The higher an agent believes an action brings value, the more likely it will choose it. We parameterize the agents’ behavior according to the soft-max strategy function,\n\\[\nX^i_t(s, a) = \\frac{e^{\\beta^i Q^i_t(s,a)}}{\\sum_b e^{\\beta^i Q^i_t(s,b)}},\n\\tag{3.5}\\]\nwhere the intensity-of-choice parameters, \\(\\beta^i \\in \\mathbb R^+\\), regulate the exploration-exploitation trade-off. For high \\(\\beta^i\\), agents exploit their learned knowledge about the environment, leaning toward actions with high estimated state-action values. For low \\(\\beta^i\\), agents are more likely to deviate from these high-value actions to explore the environment further with the chance of finding actions that eventually lead to even higher values. This soft-max strategy function can be motivated by the maximum-entropy principle (Jaynes & Bretthorst, 2003), stating that the current strategy of an agent should follow a distribution that maximizes entropy subject to current beliefs about the qualities \\(Q_t^i(s, a)\\) (Wolpert, 2006; Wolpert et al., 2012).\n\n\nLearning\nLearning means updating the quality estimates, \\(Q^i_t(s,a)\\), with the current reward-prediciton error, \\(\\delta^i_t(s, a)\\), after selection action \\(a_t\\) in state \\(s_t\\) according to\n\\[\nQ^i_{t+1}(s_t, a_t) = Q^i_{t}(s_t, a_t) + \\alpha^i \\delta^i_t(s_t, a_t),\n\\tag{3.6}\\]\nwhere \\(\\alpha^i \\in (0,1)\\) is the learning rate of agent \\(i\\), which regulates how much new information the agent uses for the update. The reward-prediction error, \\(\\delta^i_t(s_t, a_t)\\), equals the difference of the new quality estimate, \\(\\mathsf N^i r^i_t + \\gamma^i \\mathcal Q_n^i(s_{t+1})\\), and the current quality estimate, \\(\\mathcal Q_c^i(s_{t})\\),\n\\[\n\\delta^i_t(s_t, a_t) = \\mathsf N^i r^i_t + \\gamma^i \\mathcal{Q}^i_n(s_{t+1}, a_{t+1}) - \\mathcal Q^i_c(s_{t}, a_{t}),\n\\tag{3.7}\\]\nwhere the \\(\\mathcal{Q}_n^i\\) represents the quality estimate of the next state and \\(\\mathcal{Q}_c^i\\) represents the quality estimate of the current state. Depending on how we choose, \\(\\mathcal{Q}_n^i\\), and \\(\\mathcal{Q}_c^i\\), we recover various well-known temporal-difference reinforcement learning update schemes (Barfuss et al., 2019).\n\nVariants\nFor example, if \\(\\mathcal{Q}_n^i = \\mathcal{Q}_c^i = Q^i_t\\), we obtain the so called SARSA update,\n\\[\\delta^i_t(s_t, a_t) = \\mathsf N^i r^i_t + \\gamma^i Q^i_t(s_{t+1}, a_{t+1}) - Q^i_t(s_{t}, a_{t}).\\]\nIf \\(\\mathcal{Q}_n^i = \\max_b Q^i_t(s_{t+1}, b)\\), and \\(\\mathcal{Q}_c^i = Q^i_t\\), we obtain the famous Q-learning update,\n\\[\\delta^i_t(s_t, a_t) = \\mathsf N^i r^i_t + \\gamma^i \\max_b Q^i_t(s_{t+1}, b) - Q^i_t(s_{t}, a_{t}).\\]\nAnd if \\(\\mathcal{Q}_n^i = \\mathcal{Q}_c^i = V^i_t\\) is a separate state-value estimate, we obtain an actor-critic update,\n\\[\\delta^i_t(s_t, a_t) = \\mathsf N^i r^i_t + \\gamma^i V^i_t(s_{t+1}) - V^i_t(s_{t}).\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Framework</span>"
    ]
  },
  {
    "objectID": "03_Framework.html#collective-reinforcement-learning-dynamics-crld",
    "href": "03_Framework.html#collective-reinforcement-learning-dynamics-crld",
    "title": "3  Framework",
    "section": "Collective Reinforcement Learning Dynamics (CRLD)",
    "text": "Collective Reinforcement Learning Dynamics (CRLD)\n\nMotivation\nIn Section 3.3, we saw how to derive temporal-difference reward-prediction reinforcement learning from first principles. Agents strive to improve their discounted sum of future rewards (Equation 3.1) while acting according to the maximum entropy principle (Equation 3.5). However, using these standard reinforcement algorithms directly for modeling comes also with some challenges:\n\nFirst of all, the learning is highly stochastic, since, in general, all agents strategies \\(X^i(s,a)\\), and the environments transition function \\(T(s, \\boldsymbol a, s')\\) are probability distributions.\nThis stochasticity can make it sometimes hard to explain, why a phenomenon occurred in a simulation.\nReinforcement learning is also very sample-inefficient, meaning it can take the agents a long time to learn something.\nThus, learning simulations are computationally intense, since one requires many simulations to make sense of the stochasticity, of which each takes a long time to address the sample inefficiency.\n\nHow can we address these challenges? In Section 3.3.4, we saw that we could express different reward-prediction learning variants by formulating different reward-prediction errors, \\(\\boldsymbol \\delta\\). The essential idea of the collective reinforcement learning dynamics approach is to replace the individual sample realizations of the reward-prediction error with its strategy average plus a small error term,\n\\[\\boldsymbol \\delta \\leftarrow \\bar{\\boldsymbol\\delta} + \\boldsymbol\\epsilon.\\]\nThus, collective reinforcement learning dynamics describe how agents with access to (a good approximation of) the strategy-average reward-prediction error would learn. There are at least three interpretations to motivate how the agents can obtain the strategy averages:\n\nThe agents are batch learners. They store experiences (state observations, rewards, actions, next state observations) inside a memory batch and replay these experiences to make the learning more stable. In the limit of an infinite memory batch, the error term vanishes, \\(\\boldsymbol\\epsilon \\rightarrow 0\\) (Barfuss, 2020).\nThe agents learn on two different time scales. On one time scale, the agents interact with the environment, collecting experiences and integrating them to improve their quality estimates while keeping their strategies fixed. On the other time scale, they use the accumulated experiences to adapt their strategy. In the limit of a complete time scale separation, having infinite experiences between two strategy updates, the error term vanishes, \\(\\boldsymbol\\epsilon \\rightarrow 0\\) (Barfuss, 2022).\nThe agents have a model of how the environment works, including how the other agents behave currently, but not how the other agents learn. This model can be used to stabilize learning. In the limit of a perfect model (and sufficient cognitive resources), the error term vanishes, \\(\\boldsymbol\\epsilon \\rightarrow 0\\).\n\nIn the following, we focus on the idealized case of a vanishing error term, \\(\\boldsymbol\\epsilon \\rightarrow 0\\).\n\n\nDerivation\nWe start by combining Equation 3.5 and Equation 3.6 to obtain the joint strategy update,\n\\[\nX^i_{t+1}(s, a) = \\frac{X^i_t(s, a) \\exp \\left({\\alpha^i\\beta^i \\bar\\delta^i(s, a)} \\right)}{\\sum_b X^i_t(s, b) \\exp \\left({\\alpha^i\\beta^i \\bar\\delta^i(s, b)} \\right)},\n\\tag{3.8}\\]\nwhere we have also replaced the sample reward-prediction error, \\(\\delta^i_t(s, a)\\), with its strategy average, \\(\\bar\\delta^i(s, a)\\). Thus, in the remainder, we can focus on obtaining the strategy-average reward-prediction error, \\(\\bar\\delta^i(s, a)=\\delta^i_{\\boldsymbol{X}_t}(s, a)\\). We equip a symbol with a straight bar on top to denote the averaging with the current joint policy \\(\\boldsymbol{X}_t\\). From Equation 3.7, we see that we need to construct the strategy-average reward, the strategy-average value of the next state, and the strategy-average value of the current state.\nEquation 3.8 suggests summarizing the product of the learning rate \\(\\alpha^i\\) and the intensity-of-choice \\(\\beta^i\\) into an effective learning rate \\(\\eta^i\\). If we restate the denominator by \\(\\bar{\\mathfrak Z}^i(s) = \\sum_b X^i_t(s, b) \\exp \\left({\\alpha^i\\beta^i \\bar\\delta^i(s, b)} \\right)\\), we recover exactly the form used in the main text,\n\\[\nX^i_{t+1}(s, a) = \\frac{1}{\\bar{\\mathfrak{Z}}^i(s)} X^i_t(s, a) \\exp\\big(\\eta^i \\cdot \\bar \\delta^i(s, a) \\big).\n\\]\n\nRewards\nThe strategy-average version of the current reward is obtained by considering each agent \\(i\\) taking action \\(a\\) in state \\(s\\) when all other agents \\(j\\) act according to their strategy \\(X^j(s, a^j)\\), causing the environment to transition to the next state \\(s'\\) with probability \\(T(s, \\boldsymbol a, s')\\), during which agent \\(i\\) receives reward \\(R^i(s, \\boldsymbol a, s')\\). Mathematically, we write,\n\\[\n\\bar R^i(s, a) = \\sum_{a^j} \\sum_{s'}  \\prod_{j\\neq i} X^j(s, a^j) T(s, \\boldsymbol a, s') R^i(s, \\mathbf a, s').\n\\]\n\n\nNext values\nThe strategy average of the following state value is likewise computed by averaging over all actions of the other agents and following states.\nWe start with the simplest learning variant, actor-critic learning. For each agent \\(i\\), state \\(s\\), and action \\(a\\), all other agents \\(j\\neq i\\) choose their action \\(a^j\\) with probability \\(X^j(s, a^j)\\). Consequently, the environment transitions to the next state \\(s'\\) with probability \\(T(s, \\boldsymbol a, s')\\). At \\(s'\\), the agent estimates the quality of the next state to be of \\(\\bar V^i(s')\\). Mathematically, we write,\n\\[\n{}^{n}\\!{\\bar Q}^i(s, a) = \\sum_{a^j} \\sum_{s'} \\prod_{j \\neq i} X^j(s, a^j) T(s, \\boldsymbol a, s') \\bar V^i(s').\n\\]\nWe obtain the strategy-average value estimate of the following state precisely as the state values of the following state, \\(\\bar V^i(s') = V^i_{\\boldsymbol X}(s')\\), as defined in Equation 3.2. We compute them by writing the Bellman equation, \\[\n{\\bar V}^i(s) = \\mathsf N^i {\\bar R^i}(s) + \\gamma^i {\\bar T}(s, s') {\\bar V}^i(s'),\n\\] in matrix form, \\[\n\\boldsymbol{\\bar V}^i = \\mathsf N^i \\boldsymbol{\\bar R^i} + \\gamma^i \\boldsymbol{\\bar T} \\boldsymbol{\\bar V}^i,\n\\] which allows us to bring all state value variables on one site through a matrix inversion, \\[\n\\boldsymbol{\\bar V}^i = N^i \\left( \\mathbb{1}_{Z} - \\gamma^i \\boldsymbol{\\bar T} \\right)^{-1} \\boldsymbol{\\bar R^i}.\n\\]\nHere, \\(\\bar R^i(s)\\) is the strategy-average reward value agent \\(i\\) receives in state \\(s\\). They are computed by averaging over all agents’ strategies, \\(X^j(s, a^j)\\), and the state transition \\(T(s, \\boldsymbol a, s')\\), \\[\n\\bar R^i(s) = \\sum_{a^j} \\sum_{s'} \\prod_{j} X^j(s, a^j) T(s, \\boldsymbol a, s') R^i(s, \\boldsymbol a, s').\n\\]\nAnd \\(\\bar T(s, s')\\) are the strategy-average transition probabilities. They are computed by averaging over all agents’ strategies, \\(X^j(s, a^j)\\), \\[\n\\bar T(s, s') = \\sum_{a^j} \\prod_{j} X^j(s, a^j) T(s, \\boldsymbol a, s').\n\\]\nLast, \\(\\mathbb{1}_Z\\), is the \\(Z\\)-by-\\(Z\\) identity matrix.\nFor SARSA learning, the strategy average of the following state value reads,\n\\[\n{}^{n}\\!{\\bar Q}^i(s, a) = \\sum_{a^j} \\sum_{s'} \\prod_{j \\neq i} X^j(s, a^j) T(s, \\boldsymbol a, s') \\sum_{a^i} X^i(s', a^i) \\bar Q^i(s', a^i),\n\\]\nwhere we replace \\(Q^i_t(s_{t+1}, a_{t+1})\\) by the strategy-average next-state next-action value \\(\\sum_{a^i} X^i(s', a^i) \\bar Q^i(s', a^i)\\).\nHere, the strategy-average state-action values, \\(\\bar Q^i(s, a) = Q^i_{\\boldsymbol X}(s, a)\\), are exaclty the state-action values defined in Equation 3.3. We compute them exactly as Equation 3.3 prescribes,\n\\[\n\\bar Q^i(s, a) = \\mathsf N^i \\bar R^i(s, a) + \\gamma^i \\sum_{s'} \\bar T^i(s, a, s') \\bar V^i(s'),\n\\]\nwhere \\(\\bar T^i(s, a, s')\\) is the strategy-average transition model from the perspective of agent \\(i\\). It can be computed by averaging out all other agents’ strategies from the transition tensor, \\[\n\\bar T^i(s, a, s') = \\sum_{a^j} \\prod_{j \\neq i} X^j(s, a^j) T(s, \\mathbf a, s').\n\\]\nHowever, it is easy to show that \\(\\sum_{a^i} X^i(s', a^i) \\bar Q^i(s', a^i) = \\bar V^i(s')\\), and thus, the strategy-average next-state values of SARSA and actor-critic learning are indeed identical.\n\n\nCurrent values\nThe strategy-average of the current state value in the reward-prediction error of actor-critic learning, \\(\\bar V^i(s)\\), is - for each agent \\(i\\) and state \\(s\\) - a constant in actions. Thus, they do not affect the joint strategy update (Equation 3.8).\nThe state-action value of the current state, \\(Q^i_t(s_t, a_t)\\), in SARSA learning becomes, \\(\\frac{1}{\\beta^i} \\ln X^i(s, a)\\), in the strategy-average reward-prediction error and can be seen as a regularization term. We can derive it by inverting Equation 3.5, \\[\nQ^i_t(s, a) = \\frac{1}{\\beta^i} \\ln X^i_t(s, a) + \\frac{1}{\\beta^i} \\ln\\Big(\\sum_b e^{\\beta^i Q^i_t(s, b)} \\Big),\n\\] and realizing that the dynamics induced by Equation 3.8 are invariant under additive transformations, which are constant in actions.\n\n\nReward-prediction error\nTogether, the strategy-average reward-prediction error for actor-critic learning reads, \\[\n\\bar \\delta^i(s, a) = \\mathsf N^i \\bar R^i(s, a) + \\gamma^i \\cdot {}^n\\! \\bar Q^i(s, a) = \\bar Q^i(s, a),\n\\] and the strategy-average actor-critic learning dynamics, thus, \\[\nX^i_{t+1}(s, a) = \\frac{X^i_t(s, a) \\exp\\big( \\alpha^i\\beta^i \\bar Q^i(s, a)\\big)}{\\sum_b X^i_t(s, b) \\exp\\big( \\alpha^i\\beta^i \\bar Q^i(s, b)\\big)}.\n\\] With \\(\\alpha^i\\beta^i \\bar Q^i(s, a)\\) being the fitness of agent \\(i\\)’s action \\(a\\) in state \\(s\\), these dynamics are exactly equivalent to the alternative replicator dynamics in discrete time (Hofbauer & Sigmund, 2003).\nFor SARSA learning, the strategy-average reward-prediction error reads, \\[\n\\bar \\delta^i(s, a) = \\mathsf N^i \\bar R^i(s, a) + \\gamma^i \\cdot {}^n\\! \\bar Q^i(s, a) - \\frac{1}{\\beta^i} \\ln X^i(s, a) = \\bar Q^i(s, a) - \\frac{1}{\\beta^i} \\ln X^i(s, a) ,\n\\] and the strategy-average SARSA learning dynamics, thus, \\[\nX^i_{t+1}(s, a) = \\frac{X^i_t(s, a) \\exp\\big( \\alpha^i \\big( \\beta^i \\bar Q^i(s, a) - \\ln X^i(s, a) \\big)\\big)}{\\sum_b X^i_t(s, b) \\exp\\big( \\alpha^i \\big(\\beta^i \\bar Q^i(s, b) - \\ln X^i(s, b) \\big)\\big)}.\n\\]\n\n\n\n\nBarfuss, W. (2020). Reinforcement Learning Dynamics in the Infinite Memory Limit. Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems, 1768–1770.\n\n\nBarfuss, W. (2022). Dynamical systems as a level of cognitive analysis of multi-agent learning. Neural Computing and Applications, 34(3), 1653–1671. https://doi.org/10.1007/s00521-021-06117-0\n\n\nBarfuss, W., Donges, J. F., & Kurths, J. (2019). Deterministic limit of temporal difference reinforcement learning for stochastic games. Physical Review E, 99(4), 043305. https://doi.org/10.1103/PhysRevE.99.043305\n\n\nBarfuss, W., Donges, J. F., Vasconcelos, V. V., Kurths, J., & Levin, S. A. (2020). Caring for the future can turn tragedy into comedy for long-term collective action under risk of collapse. Proceedings of the National Academy of Sciences, 117(23), 12915–12922. https://doi.org/10.1073/pnas.1916545117\n\n\nDayan, P., & Niv, Y. (2008). Reinforcement learning: The Good, The Bad and The Ugly. Current Opinion in Neurobiology, 18(2), 185–196. https://doi.org/10.1016/j.conb.2008.08.003\n\n\nHofbauer, J., & Sigmund, K. (2003). Evolutionary game dynamics. Bulletin of the American Mathematical Society, 40(4), 479–519. https://doi.org/10.1090/S0273-0979-03-00988-1\n\n\nJaynes, E. T., & Bretthorst, G. L. (2003). Probability theory: The logic of science. Cambridge University Press. http://www5.unitn.it/Biblioteca/it/Web/LibriElettroniciDettaglio/50847\n\n\nLittman, M. L. (1994). Markov games as a framework for multi-agent reinforcement learning. In W. W. Cohen & H. Hirsh (Eds.), Machine Learning Proceedings 1994 (pp. 157–163). Morgan Kaufmann. https://doi.org/10.1016/B978-1-55860-335-6.50027-1\n\n\nSchultz, W., Dayan, P., & Montague, P. R. (1997). A Neural Substrate of Prediction and Reward. Science, 275(5306), 1593–1599. https://doi.org/10.1126/science.275.5306.1593\n\n\nSchultz, W., Stauffer, W. R., & Lak, A. (2017). The phasic dopamine signal maturing: From reward via behavioural activation to formal economic utility. Current Opinion in Neurobiology, 43, 139–148. https://doi.org/10.1016/j.conb.2017.03.013\n\n\nSutton, R. S. (1988). Learning to predict by the methods of temporal differences. Machine Learning, 3(1), 9–44. https://doi.org/10.1007/BF00115009\n\n\nSutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction (Second edition). The MIT Press.\n\n\nWolpert, D. H. (2006). Information Theory - The Bridge Connecting Bounded Rational Game Theory and Statistical Physics. In D. Braha, A. A. Minai, & Y. Bar-Yam (Eds.), Complex Engineered Systems: Science Meets Technology (pp. 262–290). Springer. https://doi.org/10.1007/3-540-32834-3_12\n\n\nWolpert, D. H., Harré, M., Olbrich, E., Bertschinger, N., & Jost, J. (2012). Hysteresis effects of changing the parameters of noncooperative games. Physical Review E, 85(3), 036102. https://doi.org/10.1103/PhysRevE.85.036102",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Framework</span>"
    ]
  },
  {
    "objectID": "04_MultiStability.html",
    "href": "04_MultiStability.html",
    "title": "4  Multi-stability",
    "section": "",
    "text": "Phase space plot\nWe start by plotting the flow of CRLD in the strategy phase space projection of the prosperous state. For that, we define a function, to help us compile as initial strategies.\ndef compile_strategy(p0c:float,  # cooperation probability of agent zero\n                     p1c:float):  # cooperation probability of agent one\n    Pi = np.array([0.95, p0c])  # coop. prob. in degraded state set to 0.95\n    Pj = np.array([0.95, p1c])\n    xi = np.array([Pi, 1-Pi]).T\n    xj = np.array([Pj, 1-Pj]).T\n    return np.array([xi, xj])\nFor example,\ncompile_strategy(0.2, 0.95)\n\narray([[[0.95, 0.05],\n        [0.2 , 0.8 ]],\n\n       [[0.95, 0.05],\n        [0.95, 0.05]]])\nThe arrows indicate the strategy-average reward-prediction errors. Their colors additionally indicate their length.\n# Inititalize the ecological public good environment\nenv = EPG(N=2, f=1.2, c=5, m=-5, qc=0.2, qr=0.01, degraded_choice=False)\n\n# Create multi-agent environment interface\nMAEi = stratAC(env=env, learning_rates=0.1, discount_factors=0.75)\n\n# Strategy flow plot\n# ------------------\nx = ([0], [1], [0])  # which (agent, observation, action) to plot on x axis\ny = ([1], [1], [0])  # which (agent, observation, action) to plot on y axis\neps=10e-3; action_probability_points = np.linspace(0+eps, 1.0-eps, 9)\nax = fp.plot_strategy_flow(MAEi, x, y, action_probability_points, NrRandom=64)\n\n# Trajectories\n# ------------------\nxtrajs = []  # storing strategy trajectories \nfprs = []    # and whether a fixed point is reached\nfor pc in [0.15, 0.175, 0.2]:  # cooperation probability of agent 2\n    X = compile_strategy(pc, 0.95)\n    xtraj, fixedpointreached = MAEi.trajectory(X, Tmax=2000, tolerance=10**-5)\n    xtrajs.append(xtraj); fprs.append(fixedpointreached)\n    print(\"Trajectory length:\",len(xtraj))\n    \n# Add trajectories to flow plot\nfp.plot_trajectories(xtrajs, x=x, y=y, fprs=fprs,\n                     cols=['red','blue','blue'], lws=[2], msss=[2],\n                     lss=['-'], alphas=[0.75],\n                     axes=ax)\n\n# Add separatrix\no = [0.619, 0.6191]; o1 = compile_strategy(*o); o2 = compile_strategy(*o[::-1])\nsep1=[]; sep2=[]\nfor _ in range(1000): o1, _ = MAEi.reverse_step(o1); sep1.append(o1)\nfor _ in range(1000): o2, _ = MAEi.reverse_step(o2); sep2.append(o2)\nfp.plot_trajectories([sep1, sep2], x=x, y=y, cols=['purple'], lws=[1],\n                     lss=['--'], alphas=[0.95], plot_startmarker=False, axes=ax)\n\n# Add saddle node\n# by reversing the dynamics from two agents with identical strategies\no = [0.5, 0.5]; o = compile_strategy(*o)\nfor _ in range(1000): o, _ = MAEi.reverse_step(o)\nax[0].scatter(*o[:,1,0], c='purple', marker='P', s=50)\n\n# Make labels nice\nax[0].set_ylabel(f\"$X^2(s=Prosp.,a=Coop.)$\")\nax[0].set_xlabel(f\"$X^1(s=Prosp.,a=Coop.)$\")\n\n# # Save plot\nplt.gcf().set_facecolor('white') # for dark mode on web\nplt.tight_layout() \nplt.savefig('_figs/fig_01PhaseSpace.png', dpi=150)\n\nTrajectory length: 296\nTrajectory length: 298\nTrajectory length: 253\n\n\n\n\n\nPhase space projection of the prosperous state of the ecological public goods environment",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multi-stability</span>"
    ]
  },
  {
    "objectID": "04_MultiStability.html#sample-trajectories",
    "href": "04_MultiStability.html#sample-trajectories",
    "title": "4  Multi-stability",
    "section": "Sample trajectories",
    "text": "Sample trajectories\nNext, we create a more fine-grained bundle of learning trajectories.\n\n# Cooperation probability of agent 2\npcs = np.concatenate((np.linspace(0.05, 0.95, 51),\n                       np.linspace(0.15, 0.18, 101),\n                       np.linspace(0.1646, 0.1649, 101),\n                       np.linspace(0.16475, 0.164765, 51)))\npcs = np.sort(np.unique(pcs))\n\nxtrajs = []  # storing strategy trajectories \nfprs = []    # and whether a fixed point is reached\nfor pc in pcs:\n    # Compile initial joint strategy\n    Pi = np.array([0.95, pc])\n    Pj = np.array([0.95, 0.95])\n    xi = np.array([Pi, 1-Pi]).T\n    xj = np.array([Pj, 1-Pj]).T\n    X = np.array([xi, xj])\n\n    # Compute trajectory\n    xtraj, fixedpointreached = MAEi.trajectory(X, Tmax=2000, tolerance=10**-5)\n    xtrajs.append(xtraj)\n    fprs.append(fixedpointreached)\n\nWe obtain the critical point in this bundle of learning trajectories where the two agents switch or tip from complete defection to complete cooperation.\n\n# assuming, that all trajectories convergend\nassert np.all(fprs)\n\n# obtaining the cooperation probability at convergences\nconverged_pcs = [xtraj[-1][:, 1, 0] for xtraj in xtrajs]\n\n# showing the biomodal distribution of full defection and full cooperation\nnp.histogram(np.array(converged_pcs).mean(-1), range=(0,1))[0]\n\narray([138,   0,   0,   0,   0,   0,   0,   0,   0, 162])\n\n\nThus, the critical point lies at the index\n\ncp = np.histogram(np.array(converged_pcs).mean(-1), range=(0,1))[0][0]\ncp\n\n138\n\n\nand has an approximate value between\n\nprint(pcs[cp-1], 'and', pcs[cp], '.')\n\n0.1647584 and 0.1647587 .",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multi-stability</span>"
    ]
  },
  {
    "objectID": "04_MultiStability.html#critical-slowing-down",
    "href": "04_MultiStability.html#critical-slowing-down",
    "title": "4  Multi-stability",
    "section": "Critical slowing down",
    "text": "Critical slowing down\nWe use this more fine-grained bundle of learning trajectories to visualize the phenomenon of a critical slowing down by plotting the time steps required to reach convergence.\n\n# Create the canves\nfsf = 0.7 # figure size factor \nplt.figure(figsize=(fsf*4, fsf*2.5))\n\n# Plot the time steps required to convergence, i.e. the trajectory lengths \nplt.plot(pcs[:cp], [len(xtraj) for xtraj in xtrajs[:cp]],\n         '-', color='red', lw=2, alpha=0.8)  # defectors in red\nplt.plot(pcs[cp:], [len(xtraj) for xtraj in xtrajs[cp:]], \n         '-', color='blue', lw=2, alpha=0.6) # cooperators in blue\n\n# Make labels and axis nice\nplt.xlabel(f\"$X^1(s=Prosp.,a=Coop.)$\")\nplt.ylabel('Timesteps to\\nconvergence')\nplt.xlim(0,1)\nplt.ylim(0, 800)\nplt.gca().spines.right.set_visible(False)\nplt.gca().spines.top.set_visible(False)\n\n# Save plot\nplt.gcf().set_facecolor('white') # for dark mode on web\nplt.subplots_adjust(top=0.95, bottom=0.3, left=0.28, right=0.94)\nplt.savefig('_figs/fig_01SlowingDown.png', dpi=150)\n\n\n\n\nTime steps required to convergence show a critical slowing down around the tipping point.\n\n\n\n\nAt the critical point \\(X^1_0(s=\\text{Prosp.}, a=\\text{Coop.}) \\approx 0.16475855\\), the collective learning takes about an order of magnitude longer to converge than close to full cooperation \\(X^1_0(s=\\text{Prosp.}, a=\\text{Coop.}) \\approx 1.0\\), and about four times as much than close to full defection \\(X^1_0(s=\\text{Prosp.}, a=\\text{Coop.}) \\approx 0.0\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multi-stability</span>"
    ]
  },
  {
    "objectID": "04_MultiStability.html#detailed-phase-space",
    "href": "04_MultiStability.html#detailed-phase-space",
    "title": "4  Multi-stability",
    "section": "Detailed phase space",
    "text": "Detailed phase space\nWe plot a detailed phase space where we zoom in on the area around the critical saddle point on the separatrix.\n\n# Create the canves\nfsf =  0.65  # figure size factor\n_, ax = plt.subplots(1,1, figsize=(fsf*4, fsf*3.5))\n\n# Plot the reward-prediction error flow\naction_probability_points = np.linspace(0.612, 0.619, 17)\nax = fp.plot_strategy_flow(MAEi, x, y, action_probability_points, NrRandom=64,\n                           axes=[ax])\n\n# Plot the defecting learning trajectories in red \nfp.plot_trajectories(xtrajs[:cp], x=x, y=y, fprs=fprs, axes=ax, cols=['red'],\n                     lws=[2], msss=[2], mss=['.'], lss=['-'], alphas=[0.15])\n\n# Plot the cooperating learning trajectories in blue\nfp.plot_trajectories(xtrajs[cp:], x=x, y=y, fprs=fprs, axes=ax, cols=['blue'],\n                     lws=[2], msss=[2], mss=['.'], lss=['-'], alphas=[0.15])\n\n# Make labels and axis nice\nax[0].set_ylabel(f\"$X^2(s=Prosp.,a=Coop.)$\")\nax[0].set_xlabel(f\"$X^1(s=Prosp.,a=Coop.)$\")\n\nax[0].set_ylim(0.613, 0.619)\nax[0].set_xlim(0.6125, 0.6155)\n\n# Save plot\nplt.gcf().set_facecolor('white') # for dark mode on web\nplt.tight_layout()\nplt.savefig('_figs/fig_01PhaseSpaceDetail.png', dpi=150)\n\n\n\n\nStrategy phase space at the critical bifurcation point.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multi-stability</span>"
    ]
  },
  {
    "objectID": "04_MultiStability.html#time-scale-separation",
    "href": "04_MultiStability.html#time-scale-separation",
    "title": "4  Multi-stability",
    "section": "Time scale separation",
    "text": "Time scale separation\nLast, we visulize the emergent time scale separation at the critical point by plotting the level of cooperation over time for the two initial strategies around the critical point.\n\n# Create the canves\nfsf = 0.5  # figure size factor\nplt.figure(figsize=(fsf*6, fsf*4))\n\n# Plot the defecting learners in red\n#   agent 1 with dots\nplt.plot(xtrajs[cp-1][:, 0, 1, 0], color='red', lw=5, ls=':') \n#   agent 2 with dashes\nplt.plot(xtrajs[cp-1][:, 1, 1, 0], color='red', lw=4, ls=\"--\", alpha=0.4)\n\n# Plot the cooperating learners in blue\n#   agent 1 with dots\nplt.plot(xtrajs[cp][:, 0, 1, 0], color='blue', lw=3, ls=':')\n#   agent 2 with dashes\nplt.plot(xtrajs[cp][:, 1, 1, 0], color='blue', lw=2, ls=\"--\", alpha=0.4)\n\n# Create a nice legend\ncustom_lines = [Line2D([0], [0], color='black', ls=':', lw=2),\n                Line2D([0], [0], color='gray', ls='--', lw=2)]\nplt.legend(custom_lines, ['Agent 1', 'Agent 2'], ncol=1)\n\n# Make labels and axis nice\nplt.gca().spines.right.set_visible(False)\nplt.gca().spines.top.set_visible(False)\nplt.xlabel(\"Timesteps\")\nplt.ylabel(\"Cooperation\")\n\n# Save plot\nplt.gcf().set_facecolor('white') # for dark mode on web\nplt.subplots_adjust(top=0.98, bottom=0.22, left=0.22, right=0.98)\nplt.savefig('_figs/fig_01PhaseSpaceTrajectory.png', dpi=150)\n\n\n\n\nEmergent time scale seperation at the critical point.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multi-stability</span>"
    ]
  },
  {
    "objectID": "05_AbruptTransitions.html",
    "href": "05_AbruptTransitions.html",
    "title": "5  Abrupt transitions",
    "section": "",
    "text": "Compute data\nWe start by computing the CRLD trajectories from 250 random initial strategies along a varying discount factor.\n# Set data trajectory for storing results, e.g.,\nddir = '/Users/wolf/Downloads/CoCoIn_data'\n\n# Initialize first environment and multi-agent environment interface (MAEi)\nenv = EPG(N=2, f=1.2, c=5, m=-5, qc=0.2, qr=0.01, degraded_choice=False)\nMAEi = stratAC(env=env, learning_rates=0.1, discount_factors=0.99, use_prefactor=True)\n\n# Create random initial strategies from simulation scripts 'sim'\nXs = sim.initial_strategies(MAEi, 250)\n\n# Create discount factors to loop through\ndiscountfacts = np.sort(np.unique(list(np.linspace(0.1, 1.0, 10)[:-1].round(2))\n                        + list(np.arange(0.5, 1.0, 0.05).round(2))\n                        + list(np.arange(0.65, 0.9, 0.0125).round(4)) \n                        + [0.01, 0.99]))\nprint(\"Discount factors:\")\nprint(discountfacts)\n\nDiscount factors:\n[0.01   0.1    0.2    0.3    0.4    0.5    0.55   0.6    0.65   0.6625\n 0.675  0.6875 0.7    0.7125 0.725  0.7375 0.75   0.7625 0.775  0.7875\n 0.8    0.8125 0.825  0.8375 0.85   0.8625 0.875  0.8875 0.9    0.95\n 0.99  ]\n# compute or load the data from disk (if they exist)\nconvtimes = []; rewss = []; coops = []\nfor dcf in discountfacts:\n    print(f\" = = = = {dcf} = = = =\")\n    MAEi = stratAC(env=env, learning_rates=0.1, discount_factors=dcf,\n                   use_prefactor=False)\n    \n    trjs, fprs = sim.obtain_trajectories(MAEi, Xs, 25000, ddir=ddir)\n\n    # convergence times\n    convtimes.append([len(trj) for trj in trjs])\n    \n    # final rewards\n    rewss.append(sim.final_rewards(MAEi, trjs))\n    \n    # cooperative acts\n    coops.append([trj[-1].astype(float)[:,1,0] for trj in trjs])",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Abrupt transitions</span>"
    ]
  },
  {
    "objectID": "05_AbruptTransitions.html#sec-plotfunc",
    "href": "05_AbruptTransitions.html#sec-plotfunc",
    "title": "5  Abrupt transitions",
    "section": "Plotting function",
    "text": "Plotting function\nNext, we create a function to plot the data along the varying parameter.\n\ndef plot_valuehistograms_vs_parameters(parameters, values, bins, rnge,\n                                       marker='.', alpha=1.0, color='black',\n                                       cmap='viridis', ax=None):\n    \"\"\"\n    Plot a histogram for each parameter next to each other.\n    \n    Parameters\n    ----------\n    parameters: iterable\n        of float-valued parameters\n    values: iterable\n        of iterable of values for each parameter\n    bins: int\n        The number of bins for the histograms\n    rnge: tupe\n        Range of the histogram as (min, max)   \n    \"\"\"\n    # Figure\n    if ax is None:\n        _, ax = plt.subplots()\n\n    # Create iterable of histograms for values\n    valhist=[]\n    for conv in values:\n        h = np.histogram(conv, bins=bins, range=rnge)[0]\n        valhist.append(h)\n\n    # Adjust spacing\n    params = np.array(parameters)\n    delta = params[1:] - params[:-1]\n    paramedges = np.concatenate(([parameters[0]-0.5*delta[0]],\n                                parameters[:-1] + 0.5*delta,\n                                [parameters[-1]+0.5*delta[-1]]))\n    valedges = np.linspace(rnge[0], rnge[1], bins+1)\n\n    # Plot histograms with colormap\n    X, Y = np.meshgrid(paramedges, valedges)\n    ax.pcolormesh(X, Y, np.array(valhist).T, cmap=cmap, alpha=alpha*0.75)\n\n    # Plot median, quantiles and mean\n    quartile1, medians, quartile3 = np.percentile(values, [25, 50, 75], axis=1)\n    ax.fill_between(params, quartile1, quartile3, color=color, alpha=alpha*0.2)\n    ax.plot(params, medians, marker=marker, markersize=4, linestyle='-', \n            color=color, alpha=0.5*alpha)\n    ax.plot(params, np.mean(values, axis=1), marker=marker, linestyle='',\n            color=color, alpha=alpha)\n    \n    # Adjust the visible y range\n    ax.set_ylim(rnge[0], rnge[1])",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Abrupt transitions</span>"
    ]
  },
  {
    "objectID": "05_AbruptTransitions.html#abrupt-transition",
    "href": "05_AbruptTransitions.html#abrupt-transition",
    "title": "5  Abrupt transitions",
    "section": "Abrupt transition",
    "text": "Abrupt transition\nWe use the created plotting function (Section 5.2) to visualize the phenomenon of an abrupt transition from complete defection to complete cooperation.\nWe show the abrupt transition in the level of cooperation at convergence.\n\n# Create the canves    \nfsf = 0.7  # figure size factor\nfig, ax = plt.subplots(figsize=(fsf*6, fsf*3))    \n\n# Plot the cooperation probabilities versus the discount factors\nplot_valuehistograms_vs_parameters(parameters=discountfacts,\n                                   values=np.array(coops).mean(-1), ax=ax,\n                                   bins=21, rnge=(-0.1, 1.1), cmap='Blues')\n\n# Make labels and axis nice\nplt.ylabel('Cooperation')\nplt.xlabel('Discount factor')\n\n# Save plot\nplt.subplots_adjust(left=0.15, right=0.98, top=0.98, bottom=0.2)\n\n\n\n\nAbrupt transition in final cooperation likelihood\n\n\n\n\nWe also show the abrupt transition in the level of final rewards obtained by the agents.\n\n# Create the canves    \nfsf = 0.7  # figure size factor\nfig, ax = plt.subplots(figsize=(fsf*6, fsf*3))    \n\n# Plot the reward levels versus the discount factors\nplot_valuehistograms_vs_parameters(parameters=discountfacts,\n                                   values=np.array(rewss).mean(-1), ax=ax,\n                                   bins=21, rnge=(-5.25, 1.25), cmap='Reds')\n\n# Make labels and axis nice\nplt.ylabel('Reward')\nplt.xlabel('Discount factor')\nplt.subplots_adjust(left=0.15, right=0.98, top=0.98, bottom=0.2)\n\n\n\n\nAbrupt transition in final rewards\n\n\n\n\nSince the transition from complete defection to complete cooperation and from low reward and high reward appear similar, we can also try to plot them together into one plot, with cooperation on the left y-axis and the reward on the right y-axis.\n\n# Create the canves    \nfsf = 0.7  # figure size factor\nfig, ax1 = plt.subplots(figsize=(fsf*6, fsf*2.8))    \nax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n\n# Plot the cooperation probabilities versus the discount factors\nplot_valuehistograms_vs_parameters(parameters=discountfacts,\n                                   values=np.array(coops).mean(-1), ax=ax1,\n                                   bins=21, rnge=(-0.1, 1.1), cmap='Blues',\n                                   marker='x', color='blue')\n\n# Plot the reward levels versus the discount factors\nplot_valuehistograms_vs_parameters(parameters=discountfacts,\n                                   values=np.array(rewss).mean(-1), ax=ax2,\n                                   bins=21, rnge=(-5.35, 1.55), cmap='Reds',\n                                   marker='.', alpha=0.5, color='red')\n\n# Make labels and axis nice\nax1.set_xlabel('Discount factor')\nax1.set_ylabel('Cooperation (X)', color='Blue')\nax1.tick_params(axis='y', labelcolor='Blue')\nax2.set_ylabel('Reward (⚫️)', color='Red')\nax2.tick_params(axis='y', labelcolor='Red')\nax2.set_yticks([-5, 0, 1]);\nplt.subplots_adjust(left=0.15, right=0.88, top=0.96, bottom=0.22)\nplt.savefig('_figs/fig_02AbruptTransitionCooperationReward.png', dpi=150)\n\n\n\n\nAbrupt transition in final cooperation likelihood and in final rewards",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Abrupt transitions</span>"
    ]
  },
  {
    "objectID": "05_AbruptTransitions.html#critical-slowing-down",
    "href": "05_AbruptTransitions.html#critical-slowing-down",
    "title": "5  Abrupt transitions",
    "section": "Critical slowing down",
    "text": "Critical slowing down\nWe use the created plotting function (Section 5.2) to visualize the phenomenon of a critical slowing down of the learning speed around the tipping point.\n\n# Create the canves    \nfsf = 0.7 # figure size factor\nfig, ax = plt.subplots(figsize=(fsf*6, fsf*3.5))    \n\n# Plot the convergence times versus the discount factors\nplot_valuehistograms_vs_parameters(parameters=discountfacts, values=convtimes,\n                                   bins=21, rnge=(0, 150), cmap='Greys', ax=ax)\n\n# Make labels and axis nice\nplt.ylabel('Timesteps to convergence')\nplt.xlabel('Discount factor')\n\n# Save plot\nplt.subplots_adjust(left=0.15, right=0.98, top=0.96, bottom=0.18)\nplt.savefig('_figs/fig_02AbruptTransitionSpeed.png', dpi=150)\n\n\n\n\nCritical slowing down in parameter space",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Abrupt transitions</span>"
    ]
  },
  {
    "objectID": "06_Hysteresis.html",
    "href": "06_Hysteresis.html",
    "title": "6  Hysteresis",
    "section": "",
    "text": "Compute data\nFirst, we compute the data for the hysteresis curve.\n# Set up the ecological public goods environment\nenv = EPG(N=2, f=1.2, c=5, m=-5, qc=0.2, qr=0.01, degraded_choice=False)\n\n# Compile the list of discount factors \ndcfs = list(np.arange(0.6, 0.9, 0.005))\n# Hysteresis curve parameters first increase and then decrease again\nhystcurve = dcfs + dcfs[::-1]\n\ncoops = []  # for storing the cooperation probabilities\nfor i, dcf in enumerate(hystcurve):\n    # Adjust multi-agent environment interface with discount factor\n    MAEi = stratS(env=env, discount_factors=dcf, use_prefactor=True,\n                learning_rates=0.01, choice_intensities=60)\n    if i==0: # Choose random intial policy \n        X = MAEi.random_softmax_strategy()\n        \n    # Compute trajectory\n    trj, fpr = MAEi.trajectory(X, Tmax=2500, tolerance=10e-12)\n    print('\\r ', dcf, fpr, end=' ')\n    X = trj[-1] # select last strategy\n    coops.append(X[:, 1, 0]) # append to storage container",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Hysteresis</span>"
    ]
  },
  {
    "objectID": "06_Hysteresis.html#plot-curve",
    "href": "06_Hysteresis.html#plot-curve",
    "title": "6  Hysteresis",
    "section": "Plot curve",
    "text": "Plot curve\nNow, we plot the computed data. We use the points’ size and color to indicate the time dimensions of the discount factor changes. The time flows from big to small data points and from dark to light ones.\n\n# Create the canves\nfsf = 0.75  # figure size factor\nplt.figure(figsize=(fsf*6, fsf*3))    \n\n# Plot background line\nplt.plot(hystcurve, np.array(coops).mean(-1),'-',alpha=0.5,color='k',zorder=-1)\n# Plot data points with size and color indicating the time dimension\nplt.scatter(hystcurve, np.array(coops).mean(-1), alpha=0.9,\n            s=np.arange(len(hystcurve))[::-1]+1, c=np.arange(len(hystcurve)))\n\n# Make labels and axis nice\nplt.ylabel('Cooperation')\nplt.xlabel('Discount Factor')\nplt.gca().spines.right.set_visible(False)\nplt.gca().spines.top.set_visible(False)\n\n# Legend\nax = plt.gcf().add_axes([0.85, 0.22, 0.12, 0.6])\n# ax = plt.gcf().add_axes([0.135, 0.38, 0.12, 0.6])\nax.scatter(np.ones_like(hystcurve)[::4], np.arange(len(hystcurve))[::4], alpha=0.9, \n           s=0.75*np.arange(len(hystcurve))[::-1][::4]+1, c=np.arange(len(hystcurve))[::4])\n# ax.annotate('Time', xy=(0.5, 1.07), xycoords='axes fraction', va='center', ha='center', fontsize=9)\nax.annotate('Start', xy=(1.6, 0), xycoords='data', va='center', ha='left', fontsize=8)\nax.annotate('End', xy=(1.6, len(hystcurve)-5), xycoords='data', va='center', ha='left', fontsize=8)\nax.set_ylim(-10,); ax.set_xlim(0,4)\nax.set_yticks([]); ax.set_xticks([])\nfor spine in ax.spines.values(): spine.set_edgecolor('grey')\n\n# Save plot\nplt.subplots_adjust(left=0.125, right=0.98, top=0.98, bottom=0.2)\nplt.savefig(\"_figs/fig_03Hysteresis.png\", dpi=150)\n\n\n\n\nHysteresis curve\n\n\n\n\nAs one can see, when the discount factor starts to increase, the learners remain close to defection up to the critical point of about 0.83 when they suddenly switch to complete cooperation. However, when the discount factor starts to decrease again, they remain at almost full cooperation until a much smaller value of approx. 0.71. Only then do the agents suddenly become complete defectors again.\n\n\n\n\nBarfuss, W., Donges, J. F., & Kurths, J. (2019). Deterministic limit of temporal difference reinforcement learning for stochastic games. Physical Review E, 99(4), 043305. https://doi.org/10.1103/PhysRevE.99.043305",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Hysteresis</span>"
    ]
  },
  {
    "objectID": "07_DynamicRegmies.html",
    "href": "07_DynamicRegmies.html",
    "title": "7  Dynamic regimes",
    "section": "",
    "text": "Memory-one prisoners’ dilemma environment\nBy trial and error, we saw that the ecological public goods environment is not prone to exhibit dynamic regimes other than convergence to fixed points. Thus, we here use the memory-one Prisoner’s Dilemma. We start creating the memory-one Prisoner’s Dilemma by initializing a standard normal-form Prisoner’s Dilemma.\npd = PD(R=1.0, T=1.25, S=-0.25, P=0)\nThe rewards of agent 1 and agent 2, respectively, are:\npd.R[0, 0, :, :, 0]\n\narray([[ 1.  , -0.25],\n       [ 1.25,  0.  ]])\nand\npd.R[1, 0, :, :, 0]\n\narray([[ 1.  ,  1.25],\n       [-0.25,  0.  ]])\nwith the first action cooperation, and the second defection. The environment state set consists only of a void dummy state,\npd.Sset\n\n['.']\nTo transform the normal-form Prisoner’s Dilemma (PD) into a memory-one PD, we can use the history-embedding class he.\n::: {#cell-15 .cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’ execution_count=10}\n:::\nshow_doc(he)\n\n\nsource\n\nHistoryEmbedded\n\n HistoryEmbedded (env, h)\n\nAbstract Environment wrapper to embed a given environment into a larger history space\nh must be an iterable of length 1+N (where N=Nr. of Agents) The first element of history specifies the length of the state-history. Subsequent elements specify the length of the respective action-history\n\n\n\n\nDetails\n\n\n\n\nenv\nAn environment\n\n\nh\nHistory specification\nThus,\n# Inititalize the memory-1 Prisoner's Dilemma:\nenv = he(pd, (1,1,1))\nwhich automatically results in the following four environmental states,\nenv.Sset\n\n['c,c,.|', 'c,d,.|', 'd,c,.|', 'd,d,.|']\nFor example, the first state, 'c,c,.|, indicates that both agents chose cooperation c in the previous round. The state, 'c,d,.|, means that the first agents chose cooperation c and the second defection d, and so on.\nWith the memory-one PD environment ready, we can finally create the multi-agent environment interface\nMAEi = stratAC(env=env, learning_rates=0.1, discount_factors=0.99)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dynamic regimes</span>"
    ]
  },
  {
    "objectID": "07_DynamicRegmies.html#memory-one-prisoners-dilemma-environment",
    "href": "07_DynamicRegmies.html#memory-one-prisoners-dilemma-environment",
    "title": "7  Dynamic regimes",
    "section": "",
    "text": "from nbdev.showdoc import *",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dynamic regimes</span>"
    ]
  },
  {
    "objectID": "07_DynamicRegmies.html#compute-data",
    "href": "07_DynamicRegmies.html#compute-data",
    "title": "7  Dynamic regimes",
    "section": "Compute data",
    "text": "Compute data\nThe initial strategy is vital to showcase different dynamic regimes in the memory-one PD environment. By trial and error (see Section 7.4), we obtained an interesting initial strategy which we hardcoded below. We compare this initial strategy to two other strategies. One is close by, the other further apart.\n\n# Initial strategies\n# ------------------\n# Initial strategy 1\nX1 = np.array([[[0.60862106, 0.39137894],\n                [0.65139908, 0.34860092],\n                [0.72655916, 0.27344087],\n                [0.52245504, 0.47754502]],\n               [[0.26495466, 0.73504543],\n                [0.88308924, 0.1169107 ],\n                [0.37133005, 0.62866992],\n                [0.53166837, 0.46833161]]])\n# Initial strategy 2\nX2 = np.array([[[0.60, 0.4],\n                [0.6, 0.4],\n                [0.7, 0.3],\n                [0.5, 0.5]],\n               [[0.3, 0.7],\n                [0.8, 0.2 ],\n                [0.3, 0.7],\n                [0.5, 0.5]]])\n# Initial strategy 3\nPi = np.array([0.98, 0.05, 0.85, 0.99])\nPj = np.array([0.2, 0.8, 0.05, 0.95])\nxi = np.array([Pi, 1-Pi]).T\nxj = np.array([Pj, 1-Pj]).T\nX3 = np.array([xi, xj])\n# Initial strategies\nXs = [X1, X2, X3]\n\n# Trajectories\n# ------------\nxtrajs = []  # storing strategy trajectories \nfprs = []    # and whether a fixed point is reached\nfor i, X in enumerate(Xs):\n    xtraj, fpr = MAEi.trajectory(X, Tmax=2500, tolerance=10**-5)\n    xtrajs.append(xtraj)\n    fprs.append(fpr)\n    \n# Compute reward trajectories   \nrtrajs = [np.array([MAEi.Ri(x) for x in xtraj]) for xtraj in xtrajs]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dynamic regimes</span>"
    ]
  },
  {
    "objectID": "07_DynamicRegmies.html#plot-data",
    "href": "07_DynamicRegmies.html#plot-data",
    "title": "7  Dynamic regimes",
    "section": "Plot data",
    "text": "Plot data\nWe plot the computed data with the strategy trajectories in phase space and reward trajectories over time.\n\n# Create canvas\nfsf = 0.7  # figure size factor\nfig = plt.figure(figsize=(fsf*4.5, fsf*6))\ngs = GridSpec(2, 1, height_ratios=[1, 4], \n              hspace=0.35, left=0.18, right=0.98, top=0.92, bottom=0.12)\nax1 = fig.add_subplot(gs[0])\nax2 = fig.add_subplot(gs[1])\n\n# Strategy flow plot\n# ------------------\nx = ([0], [0], [0])  # which (agent, observation, action) to plot on x axis\ny = ([1], [0], [0])  # which (agent, observation, action) to plot on y axis\neps=10e-3; action_probability_points = np.linspace(0+eps, 1.0-eps, 9)\nfp.plot_strategy_flow(MAEi, x, y, action_probability_points, NrRandom=64,\n                      cmap='Greys', sf=0.20, axes=[ax2])\n\n# Trajectories\n# ------------\n# in phase space\nfp.plot_trajectories(xtrajs, x=x, y=y, fprs=fprs, axes=[ax2], alphas=[0.5],\n                     cols=['red','orange','blue'], lws=[2], lss=['-'])\n# and over time      \nax1.plot(rtrajs[0][:, 0], c='red', ls='--')\nax1.plot(rtrajs[0][:, 1], c='red', ls=':')\nax1.plot(rtrajs[1][:, 0], c='orange', ls='--')\nax1.plot(rtrajs[1][:, 1], c='orange', ls=':')\nax1.plot(rtrajs[2][:, 0], c='blue', ls='--')\nax1.plot(rtrajs[2][:, 1], c='blue', ls=':')\n\n# Decorations\n# -----------\n# Make labels nice\nax1.set_ylim(-0.25, 1.25)\nax1.set_ylabel('Reward')\nax1.set_xlabel('Time steps')\nax2.set_ylabel(f\"$X^2(s=CC,a=C)$\")\nax2.set_xlabel(f\"$X^1(s=CC,a=C)$\")\n\n# Create legend\ncustom_lines = [Line2D([0], [0], color='gray', ls='--', lw=1),\n                Line2D([0], [0], color='gray', ls=':', lw=1)]\nax1.legend(custom_lines, ['Agent 1', 'Agent 2'], ncol=2, bbox_to_anchor=(1,1),\n           loc='lower right')\n\n# Save plot\nplt.savefig('_figs/fig_04DynamicRegimes.png', dpi=150)\n\n\n\n\n\n\n\nFigure 7.1: Different dynamic regimes in CRLD applied to repeated Prisoner’s Dilemma with agents adopting memory-one strategies (i.e., their next action depends on the joint action of the previous round). Such a system is equivalent to a stochastic game with four states (CC, CD, DC, DD), which encode the agents’ previous behavior. The graphs show the dynamics for three different initial memory-1 strategies.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dynamic regimes</span>"
    ]
  },
  {
    "objectID": "07_DynamicRegmies.html#sec-findstrategy",
    "href": "07_DynamicRegmies.html#sec-findstrategy",
    "title": "7  Dynamic regimes",
    "section": "Find initial strategy",
    "text": "Find initial strategy\nLast, we show how to find an initial strategy that leads to a dynamic regime other than the convergence to a fixed point.\n\n# If necessary, create  multi-agent environment interface from scratch\nenv = he(PD(R=1.0, T=1.25, S=-0.25, P=0), (1,1,1))\nMAEi = stratAC(env=env, learning_rates=0.1, discount_factors=0.99)\n\n# Compute trajectories for some random initial strategies\n# and see whether they did not reach a fixed point.\nprint(\"No fixed point reached for trajectories:\")\nxtrajs = []\nTmax = 2500 # Convergence-time threshold \nfor i in range(500):\n    X = MAEi.random_softmax_strategy()\n    xtraj, fpr = MAEi.trajectory(X, Tmax=Tmax, tolerance=10**-5)\n    xtrajs.append(xtraj)\n        \n    if not fpr:\n        print(i, end=' ')\n\nNo fixed point reached for trajectories:\n142 238 487 \n\n\nWe check that the center of the distribution of convergence times is well below the convergence-time threshold.\n\nplt.figure(figsize=(3,2))\nhist = plt.hist([len(xt) for xt in xtrajs], bins=25)\nplt.plot([Tmax, Tmax], [0, 1.1*max(hist[0])], c='red')\nplt.ylim(0, 1.1*max(hist[0]))\nplt.xlabel(\"Convergence timesteps\")   \nplt.ylabel(\"Count\");\n\n\n\n\nHistogram of convergence times.\n\n\n\n\nLast, we examine the non-convergent trajectories in the strategy phase space of all environmental states of the memory-one Prisoner’s Dilemma.\n\nxt, fpr = MAEi.trajectory(xtrajs[238][0], Tmax=2500, tolerance=10**-5)\n\nx = ([0], [0,1,2,3], [0])  # which (agent, observation, action) to plot on x ax\ny = ([1], [0,1,2,3], [0])  # which (agent, observation, action) to plot on y ax\neps=10e-3; action_probability_points = np.linspace(0+eps, 1.0-eps, 9)\nax = fp.plot_strategy_flow(MAEi, x, y, action_probability_points, NrRandom=32,\n                           conds=MAEi.env.Sset)\nfp.plot_trajectories([xt], x=x, y=y, \n                     cols=['red'], lws=[2], lss=['-'], alphas=[0.5], axes=ax);\n\n\n\n\n\n\n\n\n\n\n\n\nLeonardos, S., & Piliouras, G. (2021). Exploration-Exploitation in Multi-Agent Learning: Catastrophe Theory Meets Game Theory. Proceedings of the AAAI Conference on Artificial Intelligence, 35(13), 11263–11271. https://doi.org/10.1609/aaai.v35i13.17343\n\n\nZinkevich, M., Greenwald, A., & Littman, M. (2005). Cyclic Equilibria in Markov Games. Advances in Neural Information Processing Systems, 18. https://proceedings.neurips.cc/paper/2005/hash/9752d873fa71c19dc602bf2a0696f9b5-Abstract.html",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dynamic regimes</span>"
    ]
  },
  {
    "objectID": "08_SimulationScripts.html",
    "href": "08_SimulationScripts.html",
    "title": "8  Simulation Scripts",
    "section": "",
    "text": "Create initial strategies\n#| exports\ndef initial_strategies(MAEi,  # Multi-agent environment interface\n                       number:int,  # Number of strategies to create\n                       iterations:int=1000  # Latin hyper cube sampling parameter\n                       )-&gt;np.ndarray:  # Array of initial strategies\n    \"\"\"\n    Create a set of inital strategies using latin hyper cube sampling\n    \"\"\"\n    assert MAEi.M == 2, 'Sampling for M&gt;2 not straightforward'\n    # https://www.egr.msu.edu/~kdeb/papers/c2018010.pdf\n    # https://www.cs.cmu.edu/~nasmith/papers/smith+tromble.tr04.pdf\n    \n    eps = 10**(-6)\n    space = Space(MAEi.N * MAEi.Q * (MAEi.M-1)*[(0.0+eps, 1.0-eps)])\n    \n    # generate latin hyper cubes\n    lhs = Lhs(criterion=\"maximin\", iterations=iterations)\n    x = lhs.generate(space.dimensions, number, random_state=42)\n    x = np.array(x).reshape(number, MAEi.N, MAEi.Q, MAEi.M-1)\n    \n    # complete and normalize\n    inits = np.zeros((number, MAEi.N, MAEi.Q, MAEi.M))\n    inits[..., 0] = x[...,0]\n    inits[..., 1] = 1 - x[...,0]\n    \n    return inits\nFor example,\nclass mae: N=3; Q=4; M=2 # dummy MAEi for demonstration only\nXs = initial_strategies(mae, 7)\nXs.shape\n\n(7, 3, 4, 2)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Simulation Scripts</span>"
    ]
  },
  {
    "objectID": "08_SimulationScripts.html#compute-trajectories",
    "href": "08_SimulationScripts.html#compute-trajectories",
    "title": "8  Simulation Scripts",
    "section": "Compute trajectories",
    "text": "Compute trajectories\n\n #| exports\ndef compute_trajectories(MAEi,  # Multi-agent environment interface\n                         inits,  # Iterable of inital conditions\n                         Tmax=1000,  # Number of maximum iteration steps for each run\n                         tol=10e-5  # Tolerance to classify a trajectory as converged\n                         )-&gt;tuple:  # (iterables of trajectories, fixed-point-reacheds)\n    \"\"\"\n    Compute learning trajectories from a set of inital strategies.\n    \"\"\"\n    trjs = []; fprs = []\n    leni = len(inits)\n    \n    for xi, x0 in enumerate(inits):\n        print(\"\\r \", np.round(xi/leni, 4), end='')\n        x = x0.copy()\n        \n        trj, fpr = MAEi.trajectory(x, Tmax=Tmax, tolerance=tol)\n        \n        trjs.append(trj)\n        fprs.append(fpr)\n        \n    print()\n    print('Computed', leni, 'trajectories')\n    \n    return np.array(trjs, dtype=object), np.array(fprs)\n\nAfter computing the trajectories, we can check whether or not all converged and look at the histograms of their lengths:\n\n #| exports\ndef check_run(trjs,  # Iterable of learning trajectories\n              fprs=None):  # Iterable of bools whether a fixed point was reached\n    \"\"\"\n    Perform some checks for an iterable of learning trajectories\n    \"\"\"\n    if fprs is not None:\n        print('Unique fixed points reached:', np.unique(fprs))\n    plt.hist([len(traj) for traj in trjs], bins=20);\n    plt.title('Histrogram of trajectories lengths')",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Simulation Scripts</span>"
    ]
  },
  {
    "objectID": "08_SimulationScripts.html#saving-reloading",
    "href": "08_SimulationScripts.html#saving-reloading",
    "title": "8  Simulation Scripts",
    "section": "Saving & reloading",
    "text": "Saving & reloading\nTo not recompute everything from scratch, we save runs to disk and retrieve them more efficiently and faster when needed.\n\n #| exports\ndef _transform_tensor_into_hash(tens):\n    \"\"\"Transform `tens` into a string for filename saving\"\"\"\n    r = int(hashlib.sha512(str(tens).encode('utf-8')).hexdigest()[:16], 16)\n    return r\n\n\n #| exports\ndef obtain_trajectories(MAEi,  # Multi-agent environment interface\n                        inits,  # Iterable of inital conditions\n                        Tmax=1000,  # Number of maximum iteration steps for each run\n                        tol=10e-5,  # Tolerance to classify a trajectory as converged\n                        ddir='data',  # Path to data directory to store the results\n                        verbose=1  # Verbosity level\n                        )-&gt;tuple:  # (iterables of trajectories, fixed-point-reacheds)\n    \"\"\"\n    Obtain learning trajectories from a set of inital strategies.\n    Check wether you can load them from disk. If yes, do so. If not, compute.\n    \"\"\"\n    fn = ddir + '/' + MAEi.id() + '_' + str(_transform_tensor_into_hash(inits))\n    fn += \".npz\"\n    \n    try:\n        dat = np.load(fn, allow_pickle=True)\n        ddic = dict(zip((k for k in dat), (dat[k] for k in dat)))\n        print(\"Loading \", fn) if verbose else None\n    \n    except:\n        print(\"Computing \", fn) if verbose else None\n        trjs, fprs = compute_trajectories(MAEi, inits, Tmax=Tmax, tol=tol)\n        check_run(trjs, fprs)\n        # rtrajs = obtain_rewards(AEi, πtrajs)\n        \n        ddic = dict(trjs=trjs, fprs=fprs)\n        np.savez_compressed(fn, **ddic)\n        dat = np.load(fn, allow_pickle=True)\n        ddic = dict(zip((k for k in dat), (dat[k] for k in dat)))\n    \n    return ddic['trjs'], ddic['fprs']",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Simulation Scripts</span>"
    ]
  },
  {
    "objectID": "08_SimulationScripts.html#final-rewards",
    "href": "08_SimulationScripts.html#final-rewards",
    "title": "8  Simulation Scripts",
    "section": "Final rewards",
    "text": "Final rewards\n\n #| exports\ndef final_rewards(MAEi, # Multi-agent environment interface\n                  trjs  # Iterable of learning trajectories\n                  )-&gt;np.ndarray:  # Array of final rewards\n    \"\"\"\n    Compute final rewards from a set of learning trajectories.\n    \"\"\"\n    rews = []\n    for trj in trjs:\n        x = trj[-1].astype(float)\n        rs = np.einsum(MAEi.Ps(x), [0], MAEi.Ris(x), [1,0], [1])        \n        # rs = MAEi.Ri(x)\n        rews.append(rs)\n        \n    return np.array(rews)\n\n\n #| hide\nimport nbdev\nnbdev.export.nb_export(\"08_SimulationScripts.ipynb\", \"_code\")",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Simulation Scripts</span>"
    ]
  },
  {
    "objectID": "09_References.html",
    "href": "09_References.html",
    "title": "9  References",
    "section": "",
    "text": "Arthur, W. B. (1994). Inductive reasoning and bounded rationality.\nThe American Economic Review, 84(2), 406–411.\n\n\nArthur, W. B. (2014). Complexity and the Economy.\nOxford University Press.\n\n\nAxelrod, R. (1984). The Evolution Of Cooperation.\nBasic Books.\n\n\nAxelrod, R., & Hamilton, W. D. (1981). The Evolution of\nCooperation. Science, 211(4489),\n1390–1396. https://doi.org/10.1126/science.7466396\n\n\nBarfuss, W. (2020). Reinforcement Learning Dynamics in the\nInfinite Memory Limit. Proceedings of the 19th\nInternational Conference on Autonomous Agents\nand MultiAgent Systems, 1768–1770.\n\n\nBarfuss, W. (2022). Dynamical systems as a level of cognitive analysis\nof multi-agent learning. Neural Computing and Applications,\n34(3), 1653–1671. https://doi.org/10.1007/s00521-021-06117-0\n\n\nBarfuss, W., Donges, J. F., & Kurths, J. (2019). Deterministic limit\nof temporal difference reinforcement learning for stochastic games.\nPhysical Review E, 99(4), 043305. https://doi.org/10.1103/PhysRevE.99.043305\n\n\nBarfuss, W., Donges, J. F., Vasconcelos, V. V., Kurths, J., & Levin,\nS. A. (2020). Caring for the future can turn tragedy into comedy for\nlong-term collective action under risk of collapse. Proceedings of\nthe National Academy of Sciences, 117(23), 12915–12922. https://doi.org/10.1073/pnas.1916545117\n\n\nBerner, C., Brockman, G., Chan, B., Cheung, V., Dębiak, P., Dennison,\nC., Farhi, D., Fischer, Q., Hashme, S., Hesse, C., Józefowicz, R., Gray,\nS., Olsson, C., Pachocki, J., Petrov, M., Pinto, H. P. d. O., Raiman,\nJ., Salimans, T., Schlatter, J., … Zhang, S. (2019). Dota 2 with\nLarge Scale Deep Reinforcement Learning\n(arXiv:1912.06680). arXiv. https://doi.org/10.48550/arXiv.1912.06680\n\n\nBialek, W., Cavagna, A., Giardina, I., Mora, T., Silvestri, E., Viale,\nM., & Walczak, A. M. (2012). Statistical mechanics for natural\nflocks of birds. Proceedings of the National Academy of\nSciences, 109(13), 4786–4791. https://doi.org/10.1073/pnas.1118633109\n\n\nBotvinick, M., Wang, J. X., Dabney, W., Miller, K. J., &\nKurth-Nelson, Z. (2020). Deep Reinforcement Learning and\nIts Neuroscientific Implications. Neuron,\n107(4), 603–616. https://doi.org/10.1016/j.neuron.2020.06.014\n\n\nBrush, E. R., Krakauer, D. C., & Flack, J. C. (2018). Conflicts of\ninterest improve collective computation of adaptive social structures.\nScience Advances, 4(1), e1603311. https://doi.org/10.1126/sciadv.1603311\n\n\nBuckley, C. L., Kim, C. S., McGregor, S., & Seth, A. K. (2017). The\nfree energy principle for action and perception: A\nmathematical review. Journal of Mathematical Psychology,\n81, 55–79. https://doi.org/10.1016/j.jmp.2017.09.004\n\n\nBuhl, J., Sumpter, D. J. T., Couzin, I. D., Hale, J. J., Despland, E.,\nMiller, E. R., & Simpson, S. J. (2006). From Disorder\nto Order in Marching Locusts.\nScience, 312(5778), 1402–1406. https://doi.org/10.1126/science.1125142\n\n\nBush, R. R., & Mosteller, F. (1951). A mathematical model for simple\nlearning. Psychological Review, 58, 313–323. https://doi.org/10.1037/h0054388\n\n\nBusoniu, L., Babuska, R., & De Schutter, B. (2008). A comprehensive\nsurvey of multiagent reinforcement learning. IEEE Transactions on\nSystems, Man, and Cybernetics, Part C (Applications and Reviews),\n38(2), 156–172.\n\n\nCamerer, C. F. (2011). Behavioral game theory:\nExperiments in strategic interaction. Princeton\nuniversity press.\n\n\nCarroll, M., Shah, R., Ho, M. K., Griffiths, T., Seshia, S., Abbeel, P.,\n& Dragan, A. (2019). On the utility of learning about humans for\nhuman-ai coordination. Advances in Neural Information Processing\nSystems, 32.\n\n\nChristoffersen, P. J., Haupt, A. A., & Hadfield-Menell, D. (2022).\nGet it in writing: Formal contracts mitigate social dilemmas in\nmulti-agent RL. arXiv Preprint arXiv:2208.10469.\n\n\nCohen, J. E. (1998). Cooperation and self-interest: Pareto-inefficiency of Nash\nequilibria in finite random games. Proceedings of the National\nAcademy of Sciences, 95(17), 9724–9731. https://doi.org/10.1073/pnas.95.17.9724\n\n\nCross, J. G. (1973). A Stochastic Learning Model of\nEconomic Behavior*. The Quarterly Journal of\nEconomics, 87(2), 239–266. https://doi.org/10.2307/1882186\n\n\nDafoe, A., Bachrach, Y., Hadfield, G., Horvitz, E., Larson, K., &\nGraepel, T. (2021). Cooperative AI: Machines must learn to\nfind common ground. Nature, 593(7857), 33–36. https://doi.org/10.1038/d41586-021-01170-0\n\n\nDaniels, B. C., Ellison, C. J., Krakauer, D. C., & Flack, J. C.\n(2016). Quantifying collectivity. Current Opinion in\nNeurobiology, 37, 106–113. https://doi.org/10.1016/j.conb.2016.01.012\n\n\nDaniels, B. C., Krakauer, D. C., & Flack, J. C. (2017). Control of\nfinite critical behaviour in a small-scale social system. Nature\nCommunications, 8(1), 14301. https://doi.org/10.1038/ncomms14301\n\n\nDaniels, B. C., Laubichler, M. D., & Flack, J. C. (2021).\nIntroduction to the special issue: Quantifying collectivity. Theory\nin Biosciences, 140(4), 321–323. https://doi.org/10.1007/s12064-021-00358-2\n\n\nDarriba, Á., & Waszak, F. (2018). Predictions through evidence\naccumulation over time. Scientific Reports, 8(1), 494.\nhttps://doi.org/10.1038/s41598-017-18802-z\n\n\nDawes, R. M. (1980). Social Dilemmas. Annual Review of\nPsychology, 31(1), 169–193. https://doi.org/10.1146/annurev.ps.31.020180.001125\n\n\nDayan, P., & Niv, Y. (2008). Reinforcement learning: The\nGood, The Bad and The Ugly. Current\nOpinion in Neurobiology, 18(2), 185–196. https://doi.org/10.1016/j.conb.2008.08.003\n\n\nDe Marzo, G., Gabrielli, A., Zaccaria, A., & Pietronero, L. (2022).\nQuantifying the unexpected: A scientific approach to\nBlack Swans. Physical Review\nResearch, 4(3), 033079. https://doi.org/10.1103/PhysRevResearch.4.033079\n\n\nDeDeo, S., Krakauer, D. C., & Flack, J. C. (2010). Inductive game\ntheory and the dynamics of animal conflict. PLoS Computational\nBiology, 6(5), e1000782.\n\n\nEpstein, J. M., & Axtell, R. L. (1996). Growing\nArtificial Societies: Social\nScience From the Bottom\nUp (First Edition). Brookings Institution Press.\n\n\nErev, I., & Roth, A. E. (1998). Predicting How People Play\nGames: Reinforcement Learning in Experimental\nGames with Unique, Mixed Strategy\nEquilibria. The American Economic Review,\n88(4), 848–881.\n\n\n(FAIR)†, M. F. A. R. D. T., Bakhtin, A., Brown, N., Dinan, E., Farina,\nG., Flaherty, C., Fried, D., Goff, A., Gray, J., Hu, H., et al. (2022).\nHuman-level play in the game of diplomacy by combining language models\nwith strategic reasoning. Science, 378(6624),\n1067–1074.\n\n\nFehr, E., & Gächter, S. (2000). Cooperation and\nPunishment in Public Goods Experiments.\nAmerican Economic Review, 90(4), 980–994. https://doi.org/10.1257/aer.90.4.980\n\n\nFlack, J. C. (2017). Coarse-graining as a downward causation mechanism.\nPhilosophical Transactions of the Royal Society A: Mathematical,\nPhysical and Engineering Sciences, 375(2109), 20160338. https://doi.org/10.1098/rsta.2016.0338\n\n\nFoerster, J., Assael, I. A., de Freitas, N., & Whiteson, S. (2016).\nLearning to Communicate with Deep Multi-Agent\nReinforcement Learning. Advances in Neural Information\nProcessing Systems, 29.\n\n\nFranci, A., Golubitsky, M., Stewart, I., Bizyaeva, A., & Leonard, N.\nE. (2022). Breaking indecision in multi-agent, multi-option dynamics.\narXiv Preprint arXiv:2206.14893.\n\n\nFriston, K. (2018). Does predictive coding have a future? Nature\nNeuroscience, 21(8), 1019–1021. https://doi.org/10.1038/s41593-018-0200-7\n\n\nFudenberg, D., & Levine, D. K. (1998). The Theory\nof Learning in Games (K. Binmore, Ed.).\nMIT Press.\n\n\nGrupen, N., Jaques, N., Kim, B., & Omidshafiei, S. (2022).\nConcept-based understanding of emergent multi-agent behavior. Deep\nReinforcement Learning Workshop NeurIPS 2022. https://openreview.net/forum?id=zt5JpGQ8WhH\n\n\nGunawardena, J. (2022). Learning Outside the\nBrain: Integrating Cognitive Science and\nSystems Biology. Proceedings of the IEEE, 1–23. https://doi.org/10.1109/JPROC.2022.3162791\n\n\nHauert, C. (2002). Effects of space in 2 2 games. International\nJournal of Bifurcation and Chaos, 12(07), 1531–1548. https://doi.org/10.1142/S0218127402005273\n\n\nHauert, C., & Doebeli, M. (2004). Spatial structure often inhibits\nthe evolution of cooperation in the snowdrift game. Nature,\n428(6983), 643–646. https://doi.org/10.1038/nature02360\n\n\nHauert, C., Michor, F., Nowak, M. A., & Doebeli, M. (2006). Synergy\nand discounting of cooperation in social dilemmas. Journal of\nTheoretical Biology, 239(2), 195–202. https://doi.org/10.1016/j.jtbi.2005.08.040\n\n\nHauser, O. P., Hilbe, C., Chatterjee, K., & Nowak, M. A. (2019).\nSocial dilemmas among unequals. Nature, 572(7770),\n524–527.\n\n\nHeins, C., Millidge, B., Costa, L. da, Mann, R., Friston, K., &\nCouzin, I. (2023). Collective behavior from surprise\nminimization. arXiv. http://arxiv.org/abs/2307.14804\n\n\nHernandez-Leal, P., Kartal, B., & Taylor, M. E. (2019). A survey and\ncritique of multiagent deep reinforcement learning. Autonomous\nAgents and Multi-Agent Systems, 33(6), 750–797. https://doi.org/10.1007/s10458-019-09421-1\n\n\nHilbe, C., Chatterjee, K., & Nowak, M. A. (2018). Partners and rivals in direct reciprocity.\nNature Human Behaviour. https://doi.org/10.1038/s41562-018-0320-9\n\n\nHilbe, C., Šimsa, Š., Chatterjee, K., & Nowak, M. A. (2018).\nEvolution of cooperation in stochastic games. Nature,\n559(7713), 246–249. https://doi.org/10.1038/s41586-018-0277-x\n\n\nHofbauer, J., & Sigmund, K. (1998). Evolutionary\nGames and Population Dynamics (First).\nCambridge University Press. https://doi.org/10.1017/CBO9781139173179\n\n\nHofbauer, J., & Sigmund, K. (2003). Evolutionary game dynamics.\nBulletin of the American Mathematical Society, 40(4),\n479–519. https://doi.org/10.1090/S0273-0979-03-00988-1\n\n\nHolland, J. H., & Miller, J. H. (1991). Artificial Adaptive\nAgents in Economic Theory. The American Economic\nReview, 81(2), 365–370.\n\n\nHughes, E., Anthony, T. W., Eccles, T., Leibo, J. Z., Balduzzi, D.,\n& Bachrach, Y. (2020). Learning to Resolve Alliance\nDilemmas in Many-Player Zero-Sum Games. New\nZealand, 10.\n\n\nJaynes, E. T., & Bretthorst, G. L. (2003). Probability theory:\nThe logic of science. Cambridge University Press. http://www5.unitn.it/Biblioteca/it/Web/LibriElettroniciDettaglio/50847\n\n\nJhawar, J., Morris, R. G., Amith-Kumar, U. R., Danny Raj, M., Rogers,\nT., Rajendran, H., & Guttal, V. (2020). Noise-induced schooling of\nfish. Nature Physics, 16(4), 488–493. https://doi.org/10.1038/s41567-020-0787-y\n\n\nKempes, C. P., Wolpert, D., Cohen, Z., & Pérez-Mercader, J. (2017).\nThe thermodynamic efficiency of computations made in cells across the\nrange of life. Philosophical Transactions of the Royal Society A:\nMathematical, Physical and Engineering Sciences,\n375(2109), 20160343. https://doi.org/10.1098/rsta.2016.0343\n\n\nKleshnina, M., Hilbe, C., Šimsa, Š., Chatterjee, K., & Nowak, M. A.\n(2023). The effect of environmental information on evolution of\ncooperation in stochastic games. Nature Communications,\n14(1), 4153. https://doi.org/10.1038/s41467-023-39625-9\n\n\nKrakauer, D. C., Flack, J. C., Dedeo, S., Farmer, D., & Rockmore, D.\n(2010). Intelligent Data Analysis of\nIntelligent Systems. In P. R. Cohen, N. M.\nAdams, & M. R. Berthold (Eds.), Advances in\nIntelligent Data Analysis\nIX (pp. 8–17). Springer. https://doi.org/10.1007/978-3-642-13062-5_3\n\n\nKrakauer, D., Bertschinger, N., Olbrich, E., Flack, J. C., & Ay, N.\n(2020). The information theory of individuality. Theory in\nBiosciences, 139(2), 209–223. https://doi.org/10.1007/s12064-020-00313-7\n\n\nLeibo, J. Z., Dueñez-Guzman, E. A., Vezhnevets, A., Agapiou, J. P.,\nSunehag, P., Koster, R., Matyas, J., Beattie, C., Mordatch, I., &\nGraepel, T. (2021). Scalable evaluation of multi-agent reinforcement\nlearning with melting pot. International Conference on Machine\nLearning, 6187–6199.\n\n\nLeibo, J. Z., Zambaldi, V., Lanctot, M., Marecki, J., & Graepel, T.\n(2017). Multi-agent Reinforcement Learning in\nSequential Social Dilemmas. Proceedings of the 16th\nConference on Autonomous Agents and\nMultiAgent Systems, 464–473.\n\n\nLeonardos, S., & Piliouras, G. (2021).\nExploration-Exploitation in\nMulti-Agent Learning:\nCatastrophe Theory Meets\nGame Theory. Proceedings of the AAAI\nConference on Artificial Intelligence, 35(13),\n11263–11271. https://doi.org/10.1609/aaai.v35i13.17343\n\n\nLevin, S. (2002). Complex adaptive systems: Exploring the\nknown, the unknown and the unknowable. Bulletin of the American\nMathematical Society, 40(1), 3–19. https://doi.org/10.1090/S0273-0979-02-00965-5\n\n\nLittman, M. L. (1994). Markov games as a framework for multi-agent\nreinforcement learning. In W. W. Cohen & H. Hirsh (Eds.),\nMachine Learning Proceedings 1994 (pp. 157–163).\nMorgan Kaufmann. https://doi.org/10.1016/B978-1-55860-335-6.50027-1\n\n\nLovering, C., Forde, J., Konidaris, G., Pavlick, E., & Littman, M.\n(2022). Evaluation beyond task performance: Analyzing concepts in\nAlphaZero in hex. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K.\nCho, & A. Oh (Eds.), Advances in neural information processing\nsystems (Vol. 35, pp. 25992–26006). Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2022/file/a705747417d32ebf1916169e1a442274-Paper-Conference.pdf\n\n\nLupu, A., & Precup, D. (2020). Gifting in multi-agent reinforcement\nlearning. Proceedings of the 19th International Conference on\nAutonomous Agents and Multiagent Systems, 789–797.\n\n\nMarden, J. R., & Shamma, J. S. (2018). Game theory and control.\nAnnual Review of Control, Robotics, and Autonomous Systems,\n1, 105–134.\n\n\nMcAvoy, A., Mori, Y., & Plotkin, J. B. (2022). Selfish optimization\nand collective learning in populations. Physica D: Nonlinear\nPhenomena, 439, 133426. https://doi.org/10.1016/j.physd.2022.133426\n\n\nMcGrath, T., Kapishnikov, A., Tomašev, N., Pearce, A., Wattenberg, M.,\nHassabis, D., Kim, B., Paquet, U., & Kramnik, V. (2022). Acquisition\nof chess knowledge in AlphaZero.\nProceedings of the National Academy of Sciences,\n119(47), e2206625119.\n\n\nMcNamara, J. M. (2013). Towards a richer evolutionary game theory.\nJournal of The Royal Society Interface, 10(88),\n20130544. https://doi.org/10.1098/rsif.2013.0544\n\n\nMcNamara, J. M., Houston, A. I., & Leimar, O. (2021). Learning,\nexploitation and bias in games. PLOS ONE, 16(2),\ne0246588. https://doi.org/10.1371/journal.pone.0246588\n\n\nMora, T., & Bialek, W. (2011). Are Biological\nSystems Poised at Criticality?\nJournal of Statistical Physics, 144(2), 268–302. https://doi.org/10.1007/s10955-011-0229-4\n\n\nNewman, M. E. J. (2003). The Structure and\nFunction of Complex Networks.\nSIAM Review, 45(2), 167–256. https://doi.org/10.1137/S003614450342480\n\n\nNowak, M. A. (2006). Evolutionary dynamics: Exploring the equations\nof life. Harvard university press.\n\n\nOstrom, E., Walker, J., & Gardner, R. (1992). Covenants with and\nwithout a Sword: Self-Governance Is Possible.\nAmerican Political Science Review, 86(2), 404–417. https://doi.org/10.2307/1964229\n\n\nPark, S., Bizyaeva, A., Kawakatsu, M., Franci, A., & Leonard, N. E.\n(2021). Tuning cooperative behavior in games with nonlinear opinion\ndynamics. IEEE Control Systems Letters, 6, 2030–2035.\n\n\nPoundstone, W. (2011). Prisoner’s Dilemma.\nKnopf Doubleday Publishing Group.\n\n\nPress, W. H., & Dyson, F. J. (2012). Iterated prisoner’s dilemma\ncontains strategies that dominate any evolutionary opponent.\nProceedings of the National Academy of Sciences,\n109(26), 10409–10413. https://doi.org/10.1073/pnas.1206569109\n\n\n\nRamos-Fernandez, G., Smith Aguilar, S. E., Krakauer, D. C., & Flack,\nJ. C. (2020). Collective Computation in Animal\nFission-Fusion Dynamics.\nFrontiers in Robotics and AI, 7. https://www.frontiersin.org/article/10.3389/frobt.2020.00090\n\n\nRao, R. P. N., & Ballard, D. H. (1999). Predictive coding in the\nvisual cortex: A functional interpretation of some extra-classical\nreceptive-field effects. Nature Neuroscience, 2(1),\n79–87. https://doi.org/10.1038/4580\n\n\nRosas, F. E., Mediano, P. A. M., Gastpar, M., & Jensen, H. J.\n(2019). Quantifying high-order interdependencies via multivariate\nextensions of the mutual information. Physical Review E,\n100(3), 032305. https://doi.org/10.1103/PhysRevE.100.032305\n\n\nRoth, A. E., & Erev, I. (1995). Learning in extensive-form games:\nExperimental data and simple dynamic models in the\nintermediate term. Games and Economic Behavior, 8(1),\n164–212. https://doi.org/10.1016/S0899-8256(05)80020-X\n\n\nSarfati, R., Hayes, J. C., & Peleg, O. (2021). Self-organization in\nnatural swarms of Photinus carolinus synchronous fireflies.\nScience Advances, 7(28), eabg9259. https://doi.org/10.1126/sciadv.abg9259\n\n\nSchultz, W., Dayan, P., & Montague, P. R. (1997). A Neural\nSubstrate of Prediction and Reward.\nScience, 275(5306), 1593–1599. https://doi.org/10.1126/science.275.5306.1593\n\n\nSchultz, W., Stauffer, W. R., & Lak, A. (2017). The phasic dopamine\nsignal maturing: From reward via behavioural activation to formal\neconomic utility. Current Opinion in Neurobiology, 43,\n139–148. https://doi.org/10.1016/j.conb.2017.03.013\n\n\nShoham, Y., Powers, R., & Grenager, T. (2007). If multi-agent\nlearning is the answer, what is the question? Artificial\nIntelligence, 171(7), 365–377. https://doi.org/10.1016/j.artint.2006.02.006\n\n\nSigmund, K. (2010). The Calculus of\nSelfishness. In The Calculus of\nSelfishness. Princeton University Press.\nhttps://doi.org/10.1515/9781400832255\n\n\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den\nDriessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V.,\nLanctot, M., Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N.,\nSutskever, I., Lillicrap, T., Leach, M., Kavukcuoglu, K., Graepel, T.,\n& Hassabis, D. (2016). Mastering the game of Go with\ndeep neural networks and tree search. Nature,\n529(7587), 484–489. https://doi.org/10.1038/nature16961\n\n\nSkyrms, B. (2004). The Stag Hunt and the\nEvolution of Social Structure.\nCambridge University Press.\n\n\nStone, P., Kaminka, G., Kraus, S., & Rosenschein, J. (2010). Ad hoc\nautonomous agent teams: Collaboration without pre-coordination.\nProceedings of the AAAI Conference on Artificial Intelligence,\n24, 1504–1509.\n\n\nStrouse, D., McKee, K. R., Botvinick, M. M., Hughes, E., & Everett,\nR. (2021). Collaborating with humans without human data. CoRR,\nabs/2110.08176. https://arxiv.org/abs/2110.08176\n\n\nSugden, R. (2004). The Economics of\nRights, Co-operation and\nWelfare. Springer.\n\n\nSutton, R. S. (1988). Learning to predict by the methods of temporal\ndifferences. Machine Learning, 3(1), 9–44. https://doi.org/10.1007/BF00115009\n\n\nSutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An\nintroduction (Second edition). The MIT Press.\n\n\nTeam, C. G. I., Bhoopchand, A., Brownfield, B., Collister, A., Lago, A.\nD., Edwards, A., Everett, R., Frechette, A., Oliveira, Y. G., Hughes,\nE., Mathewson, K. W., Mendolicchio, P., Pawar, J., Pislar, M., Platonov,\nA., Senter, E., Singh, S., Zacherl, A., & Zhang, L. M. (2022).\nLearning robust real-time cultural transmission without human\ndata. https://arxiv.org/abs/2203.00715\n\n\nTekin, E., Savage, V. M., & Yeh, P. J. (2017). Measuring\nhigher-order drug interactions: A review of recent\napproaches. Current Opinion in Systems Biology, 4,\n16–23. https://doi.org/10.1016/j.coisb.2017.05.015\n\n\nTekin, E., Yeh, P. J., & Savage, V. M. (2018). General\nForm for Interaction Measures and\nFramework for Deriving\nHigher-Order Emergent\nEffects. Frontiers in Ecology and Evolution,\n6. https://www.frontiersin.org/articles/10.3389/fevo.2018.00166\n\n\nTuyls, K., Perolat, J., Lanctot, M., Hughes, E., Everett, R., Leibo, J.\nZ., Szepesvári, C., & Graepel, T. (2019). Bounds and dynamics for\nempirical game theoretic analysis. Autonomous Agents and Multi-Agent\nSystems, 34(1), 7. https://doi.org/10.1007/s10458-019-09432-y\n\n\nVinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A.,\nChung, J., Choi, D. H., Powell, R., Ewalds, T., Georgiev, P., Oh, J.,\nHorgan, D., Kroiss, M., Danihelka, I., Huang, A., Sifre, L., Cai, T.,\nAgapiou, J. P., Jaderberg, M., … Silver, D. (2019). Grandmaster level in\nStarCraft II using multi-agent reinforcement learning.\nNature, 575(7782), 350–354. https://doi.org/10.1038/s41586-019-1724-z\n\n\nWang, W. Z., Beliaev, M., Bıyık, E., Lazar, D. A., Pedarsani, R., &\nSadigh, D. (2021). Emergent Prosociality in\nMulti-Agent Games Through Gifting. Twenty-Ninth\nInternational Joint Conference on Artificial\nIntelligence, 1, 434–442. https://doi.org/10.24963/ijcai.2021/61\n\n\nWang, X., & Fu, F. (2020). Eco-evolutionary dynamics with\nenvironmental feedback: Cooperation in a changing world.\nEurophysics Letters, 132(1), 10001. https://doi.org/10.1209/0295-5075/132/10001\n\n\nWolfram, S. (1994). Cellular Automata And\nComplexity: Collected Papers\n(1st edition). Westview Press.\n\n\nWolpert, D. H. (2006). Information Theory - The\nBridge Connecting Bounded Rational Game Theory and\nStatistical Physics. In D. Braha, A. A. Minai, & Y.\nBar-Yam (Eds.), Complex Engineered Systems:\nScience Meets Technology (pp. 262–290).\nSpringer. https://doi.org/10.1007/3-540-32834-3_12\n\n\nWolpert, D. H., Harré, M., Olbrich, E., Bertschinger, N., & Jost, J.\n(2012). Hysteresis effects of changing the parameters of noncooperative\ngames. Physical Review E, 85(3), 036102. https://doi.org/10.1103/PhysRevE.85.036102\n\n\nZinkevich, M., Greenwald, A., & Littman, M. (2005). Cyclic\nEquilibria in Markov Games.\nAdvances in Neural Information\nProcessing Systems, 18. https://proceedings.neurips.cc/paper/2005/hash/9752d873fa71c19dc602bf2a0696f9b5-Abstract.html",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>References</span>"
    ]
  }
]